<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>计算机视觉 | がんばろう</title><meta name="author" content="今天睡够了吗"><meta name="copyright" content="今天睡够了吗"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="图像增广在卷积神经网络提到大型数据集是成功应用深度神经网络的先决条件 图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模 应用图像增广的原因是，随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力，例如可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖；还可以调整亮度、颜色等因素来降低模型对颜色的敏感">
<meta property="og:type" content="article">
<meta property="og:title" content="计算机视觉">
<meta property="og:url" content="http://yhblogs.cn/posts/62963.html">
<meta property="og:site_name" content="がんばろう">
<meta property="og:description" content="图像增广在卷积神经网络提到大型数据集是成功应用深度神经网络的先决条件 图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模 应用图像增广的原因是，随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力，例如可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖；还可以调整亮度、颜色等因素来降低模型对颜色的敏感">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-po97l3_1280x720.webp">
<meta property="article:published_time" content="2025-11-01T20:53:57.000Z">
<meta property="article:modified_time" content="2026-01-31T12:00:30.728Z">
<meta property="article:author" content="今天睡够了吗">
<meta property="article:tag" content="⌨️python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-po97l3_1280x720.webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yhblogs.cn/posts/62963.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '计算机视觉',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-01-31 12:00:30'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_3319458_ks437t3n4r.css"><link rel="stylesheet" href="/css/modify.css"><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/b_2a1aef95f351a5f7ef72eb81e6838fd6.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouye"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw iconfont icon-rili"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw iconfont icon-biaoqian"></i><span> 标签</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="がんばろう"><img class="site-icon" src="/img/favicon.png"><span class="site-name">がんばろう</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouye"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw iconfont icon-rili"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw iconfont icon-biaoqian"></i><span> 标签</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">计算机视觉</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-11-01T20:53:57.000Z" title="发表于 2025-11-01 20:53:57">2025-11-01</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">24.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>95分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="计算机视觉"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h2 id="图像增广"><a href="#图像增广" class="headerlink" title="图像增广"></a>图像增广</h2><p>在卷积神经网络提到大型数据集是成功应用深度神经网络的先决条件</p>
<p>图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模</p>
<p>应用图像增广的原因是，随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力，例如可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖；还可以调整亮度、颜色等因素来降低模型对颜色的敏感度</p>
<p><strong>图像增广技术对于AlexNet的成功是必不可少的</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="comment"># 图像数据 + 图像模型 + 图像处理</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="comment"># 图像数据预处理与增强</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets <span class="comment"># 数据集</span></span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data  <span class="comment"># 数据加载接口</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br></pre></td></tr></tbody></table></figure>

<h3 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h3><p>在对常用图像增广方法的探索时，将使用下面这个尺寸为400×500的图像作为示例</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">'imgs/cat1.jpg'</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/cat1.webp" alt="cat1" style="zoom: 50%;">

<p>大多数图像增广方法都具有一定的随机性</p>
<p>定义辅助函数<code>apply</code>，此函数在输入图像<code>img</code>上多次运行图像增广方法<code>aug</code>并显示所有结果</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""Plot a list of images."""</span></span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()  <span class="comment"># 把子图对象展开成一维列表方便索引</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(img):  <span class="comment"># 如果是张量，就转为numpy</span></span><br><span class="line">            img = img.detach().numpy()</span><br><span class="line">        ax.imshow(img)</span><br><span class="line">        ax.axis(<span class="string">'off'</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply</span>(<span class="params">img, aug, num_rows=<span class="number">2</span>, num_cols=<span class="number">4</span>, scale=<span class="number">1.5</span></span>):</span><br><span class="line">    <span class="comment"># aug: 增强操作对象(transform)</span></span><br><span class="line">    Y = [aug(img) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_rows * num_cols)]</span><br><span class="line">    show_images(Y, num_rows, num_cols, scale=scale)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="翻转和裁剪"><a href="#翻转和裁剪" class="headerlink" title="翻转和裁剪"></a>翻转和裁剪</h4><p>左右翻转图像通常不会改变对象的类别，这是最早且最广泛使用的图像增广方法之一</p>
<p>使用<code>transforms</code>模块来创建<code>RandomFlipLeftRight</code>实例，这样就各有50%的几率使图像向左或向右翻转</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apply(img, transforms.RandomHorizontalFlip())</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511012052.webp" alt="202511012052" style="zoom:80%;">

<p>上下翻转图像不如左右图像翻转那样常用，至少对于这个图像，上下翻转不会妨碍识别</p>
<p>创建一个<code>RandomFlipTopBottom</code>实例，使图像各有50%的几率向上或向下翻转</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apply(img, transforms.RandomVerticalFlip())</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511012056.webp" alt="202511012056" style="zoom:80%;">

<p>示例图片中猫位于图片的中间，但并非所有图片都是这样，汇聚层可以降低卷积层对目标位置的敏感性，另外也可以通过对图像进行随机裁剪，使物体以不同的比例出现在图像的不同位置，也可以降低模型对目标位置的敏感性</p>
<p>随机裁剪的区域面积占原始面积的10%到100%，该区域的宽高比从0.5～2之间随机取值，然后区域的宽度和高度都被缩放到200像素</p>
<blockquote>
<p>正常都是取均匀分布，除非另外说明</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shape_aug = transforms.RandomResizedCrop(</span><br><span class="line">    (<span class="number">200</span>, <span class="number">200</span>), scale=(<span class="number">0.1</span>, <span class="number">1</span>), ratio=(<span class="number">0.5</span>, <span class="number">2</span>))</span><br><span class="line">apply(img, shape_aug)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511012107.webp" alt="202511012107" style="zoom:80%;">

<h4 id="改变颜色"><a href="#改变颜色" class="headerlink" title="改变颜色"></a>改变颜色</h4><p>另一种增广方法是改变颜色，可以改变图像颜色的四个方面：亮度、对比度、饱和度和色相</p>
<p>在下面的代码中随机更改图像的亮度，随机值为原始图像的50%-150%</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">apply(img, transforms.ColorJitter(</span><br><span class="line">    brightness=<span class="number">0.5</span>, contrast=<span class="number">0</span>, saturation=<span class="number">0</span>, hue=<span class="number">0</span></span><br><span class="line">))</span><br><span class="line"><span class="comment"># 范围为[max(0, 1 - x), 1 + x]，或者直接输入元组指定(min, max)</span></span><br><span class="line"><span class="comment"># 四个参数的设定方法都是一样的</span></span><br></pre></td></tr></tbody></table></figure>

<p>但是一般都是处理灰度图片，所以主要调整亮度</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511012114.webp" alt="202511012114" style="zoom:80%;">

<p>如果同时更改</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">color_aug = transforms.ColorJitter(</span><br><span class="line">    brightness=<span class="number">0.5</span>, contrast=<span class="number">0.5</span>, saturation=<span class="number">0.5</span>, hue=<span class="number">0.5</span>)</span><br><span class="line">apply(img, color_aug)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511012115.webp" alt="202511012115" style="zoom:80%;">

<h4 id="综合使用"><a href="#综合使用" class="headerlink" title="综合使用"></a>综合使用</h4><p>在实践中将结合多种图像增广方法，可以通过使用一个<code>Compose</code>实例来综合上面定义的不同的图像增广方法，并将它们应用到每个图像</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">augs = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    color_aug,</span><br><span class="line">    shape_aug])</span><br><span class="line">apply(img, augs)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>使用图像增广来训练模型，这里使用CIFAR-10数据集，而不是之前使用的Fashion-MNIST数据集，这是因为Fashion-MNIST数据集中对象的位置和大小已被规范化，而CIFAR-10数据集中对象的颜色和大小差异更明显</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_images = datasets.CIFAR10(root=<span class="string">"../data"</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>)</span><br><span class="line">show_images([all_images[i][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>)], <span class="number">4</span>, <span class="number">8</span>, scale=<span class="number">0.8</span>);</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511012145.webp" alt="202511012145" style="zoom:80%;">

<p>为了保证预测结果稳定，只在训练阶段使用图像增广，预测时不进行随机增广操作</p>
<p>在这里只使用最简单的随机左右翻转，使用<code>ToTensor</code>实例将一批图像转换为深度学习框架所要求的格式，即形状为(批量大小，通道数，高度，宽度)的32位浮点数，取值范围为0～1</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_augs = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line">test_augs = transforms.Compose([</span><br><span class="line">    transforms.ToTensor()])</span><br></pre></td></tr></tbody></table></figure>

<p>定义一个辅助函数，以便于读取图像和应用图像增广</p>
<p>PyTorch数据集提供的<code>transform</code>参数应用图像增广来转化图像</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_cifar10</span>(<span class="params">is_train, augs, batch_size</span>):</span><br><span class="line">    dataset = datasets.CIFAR10(</span><br><span class="line">        root=<span class="string">"../data"</span>,   <span class="comment"># 数据保存的路径</span></span><br><span class="line">        train=is_train,   <span class="comment"># 是否加载训练集(True)或测试集(False)</span></span><br><span class="line">        transform=augs,   <span class="comment"># 对图像进行的预处理</span></span><br><span class="line">        download=<span class="literal">True</span>     <span class="comment"># 若本地无数据则自动下载</span></span><br><span class="line">    )</span><br><span class="line">    dataloader = data.DataLoader(dataset, batch_size=batch_size,</span><br><span class="line">                    shuffle=is_train, num_workers=get_dataloader_workers())</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></tbody></table></figure>

<p><code>data.DataLoader</code>：</p>
<ul>
<li><code>dataset</code>：加载的数据集</li>
<li><code>batch_size</code>：每次从数据集中取多少个样本组成一个 batch</li>
<li><code>shuffle</code>：是否在每个 epoch 开始前随机打乱数据，训练一般True</li>
<li><code>num_workers</code>：并行加载数据的子进程数</li>
</ul>
<h4 id="多GPU训练"><a href="#多GPU训练" class="headerlink" title="多GPU训练"></a>多GPU训练</h4><p>在CIFAR-10数据集上训练ResNet-18模型，需要利用多GPU实现</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_batch_ch13</span>(<span class="params">net, X, y, loss, trainer, devices</span>):</span><br><span class="line">    <span class="string">"""用多GPU进行小批量训练"""</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):  <span class="comment"># 如果X是多个张量输入，比如文本</span></span><br><span class="line">        X = [x.to(devices[<span class="number">0</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        X = X.to(devices[<span class="number">0</span>]) <span class="comment"># 普通图像任务：单个张量</span></span><br><span class="line">    y = y.to(devices[<span class="number">0</span>])</span><br><span class="line">    net.train()              <span class="comment"># 切换到训练模式</span></span><br><span class="line">    trainer.zero_grad()      <span class="comment"># 清空上一轮的梯度</span></span><br><span class="line">    pred = net(X)            <span class="comment"># 前向计算，得到预测</span></span><br><span class="line">    l = loss(pred, y)</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()       <span class="comment"># 对所有 GPU 上的损失求和后反向传播</span></span><br><span class="line">    trainer.step()           <span class="comment"># 参数更新</span></span><br><span class="line">    train_loss_sum = l.<span class="built_in">sum</span>()</span><br><span class="line">    train_acc_sum = accuracy(pred, y)</span><br><span class="line">    <span class="keyword">return</span> train_loss_sum, train_acc_sum</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch13</span>(<span class="params">net, train_iter, test_iter, loss, trainer, num_epochs,</span></span><br><span class="line"><span class="params">               devices=try_all_gpus(<span class="params"></span>)</span>):</span><br><span class="line">    <span class="string">"""用多GPU进行模型训练"""</span></span><br><span class="line">    timer, num_batches = Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    animator = Animator(xlabel=<span class="string">'epoch'</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                            legend=[<span class="string">'train loss'</span>, <span class="string">'train acc'</span>, <span class="string">'test acc'</span>])</span><br><span class="line">    <span class="comment"># 把网络包装成多 GPU 模型</span></span><br><span class="line">    net = nn.DataParallel(net, device_ids=devices).to(devices[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 4个维度：储存训练损失，训练准确度，实例数，特点数</span></span><br><span class="line">        metric = Accumulator(<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            l, acc = train_batch_ch13(</span><br><span class="line">                net, features, labels, loss, trainer, devices)</span><br><span class="line">            metric.add(l, acc, labels.shape[<span class="number">0</span>], labels.numel())</span><br><span class="line">            timer.stop()</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">3</span>],</span><br><span class="line">                              <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    animator.show()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'loss <span class="subst">{metric[<span class="number">0</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f}</span>, train acc '</span></span><br><span class="line">          <span class="string">f'<span class="subst">{metric[<span class="number">1</span>] / metric[<span class="number">3</span>]:<span class="number">.3</span>f}</span>, test acc <span class="subst">{test_acc:<span class="number">.3</span>f}</span>'</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'<span class="subst">{metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f}</span> examples/sec on '</span></span><br><span class="line">          <span class="string">f'<span class="subst">{<span class="built_in">str</span>(devices)}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>自定义resnet18网络，把第一层改为3×3卷积层，因为cifar10的图片不是224，也是32×32，如果保持7×7会出问题，而且减少了第一组的池化层</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基本残差块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, use_1x1conv=<span class="literal">False</span>, stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>,</span><br><span class="line">                               padding=<span class="number">1</span>, stride=stride)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>,</span><br><span class="line">                               padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            <span class="comment"># 当通道数不匹配时，用 1x1 卷积调整维度</span></span><br><span class="line">            <span class="variable language_">self</span>.conv3 = nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                                   kernel_size=<span class="number">1</span>, stride=stride)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.conv3 = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.bn1 = nn.BatchNorm2d(out_channels)</span><br><span class="line">        <span class="variable language_">self</span>.bn2 = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        Y = nn.functional.relu(<span class="variable language_">self</span>.bn1(<span class="variable language_">self</span>.conv1(X)))</span><br><span class="line">        Y = <span class="variable language_">self</span>.bn2(<span class="variable language_">self</span>.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.conv3:</span><br><span class="line">            X = <span class="variable language_">self</span>.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> nn.functional.relu(Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ResNet-18主体</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">in_channels, out_channels, num_residuals, first_block=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(in_channels, out_channels,</span><br><span class="line">                                use_1x1conv=<span class="literal">True</span>, stride=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(out_channels, out_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet18</span>(<span class="params">num_classes, in_channels=<span class="number">3</span></span>):</span><br><span class="line">    <span class="comment"># 输入层</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>), <span class="comment"># 这里给改小了，因为图片尺寸小</span></span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">        nn.ReLU() <span class="comment"># 不需要再池化了</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 四个阶段，每个阶段通道数分别是 64, 128, 256, 512</span></span><br><span class="line">    net.add_module(<span class="string">"resnet_block1"</span>, nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>)))</span><br><span class="line">    net.add_module(<span class="string">"resnet_block2"</span>, nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>)))</span><br><span class="line">    net.add_module(<span class="string">"resnet_block3"</span>, nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>)))</span><br><span class="line">    net.add_module(<span class="string">"resnet_block4"</span>, nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全局平均池化 + 全连接层</span></span><br><span class="line">    net.add_module(<span class="string">"global_avg_pool"</span>, nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">    net.add_module(<span class="string">"fc"</span>, nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">512</span>, num_classes)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></tbody></table></figure>

<p>可以定义<code>train_with_data_aug</code>函数，使用图像增广来训练模型</p>
<p>该函数获取所有的GPU，并使用Adam作为训练的优化算法，将图像增广应用于训练集，最后调用刚刚定义的用于训练和评估模型的<code>train_ch13</code>函数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置批量大小、设备列表和网络结构</span></span><br><span class="line">batch_size, devices, net = <span class="number">256</span>, try_all_gpus(), resnet18(<span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 定义权重初始化函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="comment"># 只对线性层和卷积层进行 Xavier 初始化</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) <span class="keyword">in</span> [nn.Linear, nn.Conv2d]:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_with_data_aug</span>(<span class="params">train_augs, test_augs, net, lr=<span class="number">0.001</span></span>):</span><br><span class="line">    <span class="comment"># 加载 CIFAR-10 数据集</span></span><br><span class="line">    train_iter = load_cifar10(<span class="literal">True</span>, train_augs, batch_size)</span><br><span class="line">    test_iter = load_cifar10(<span class="literal">False</span>, test_augs, batch_size)</span><br><span class="line">    <span class="comment"># 定义损失函数(交叉熵损失)</span></span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">"none"</span>)</span><br><span class="line">    <span class="comment"># 定义优化器，这里使用 Adam，自适应学习率算法</span></span><br><span class="line">    trainer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    train_ch13(net, train_iter, test_iter, loss, trainer, <span class="number">10</span>, devices)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 1.427, acc 0.490, test acc 0.498, time 82.98 sec</span><br><span class="line">epoch 2, loss 0.884, acc 0.687, test acc 0.517, time 83.32 sec</span><br><span class="line">epoch 3, loss 0.659, acc 0.770, test acc 0.675, time 82.61 sec</span><br><span class="line">epoch 4, loss 0.524, acc 0.818, test acc 0.758, time 83.24 sec</span><br><span class="line">epoch 5, loss 0.430, acc 0.851, test acc 0.767, time 82.97 sec</span><br><span class="line">epoch 6, loss 0.360, acc 0.875, test acc 0.805, time 83.55 sec</span><br><span class="line">epoch 7, loss 0.307, acc 0.893, test acc 0.819, time 83.18 sec</span><br><span class="line">epoch 8, loss 0.258, acc 0.910, test acc 0.838, time 83.17 sec</span><br><span class="line">epoch 9, loss 0.217, acc 0.926, test acc 0.844, time 82.58 sec</span><br><span class="line">epoch 10, loss 0.186, acc 0.936, test acc 0.851, time 81.58 sec</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/image-20251102145949769.png" alt="image-20251102145949769" style="zoom:80%;">

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.186, train acc 0.936, test acc 0.851</span><br><span class="line">1383.1 examples/sec on [device(type='cuda', index=0)]</span><br></pre></td></tr></tbody></table></figure>

<p>这里仅使用随机左右翻转的图像增广来训练模型</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul>
<li>图像增广基于现有的训练数据生成随机图像，来提高模型的泛化能力</li>
<li>为了在预测过程中得到确切的结果，通常对训练样本只进行图像增广，而在预测过程中不使用带随机操作的图像增广</li>
<li>深度学习框架提供了许多不同的图像增广方法，这些方法可以被同时应用</li>
</ul>
<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><p>由于训练样本数量有限，训练模型的准确性可能无法满足实际要求</p>
<p>另一种解决方案是应用**迁移学习(transfer learning)**将从源数据集学到的知识迁移到目标数据集，尽管ImageNet数据集中的大多数图像与椅子无关，但在此数据集上训练的模型可能会提取更通用的图像特征，这有助于识别边缘、纹理、形状和对象组合，这些类似的特征也可能有效地识别椅子</p>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>迁移学习中的常见技巧:<strong>微调(fine-tuning)</strong></p>
<p>包括以下四个步骤：</p>
<ol>
<li>在源数据集(例如ImageNet数据集)上预训练神经网络模型，即源模型</li>
<li>创建一个新的神经网络模型，即目标模型，这将复制源模型上的所有模型设计及其参数(不包括输出层)。前面层的参数包含通用特征，可直接用于新任务；输出层与旧任务标签相关，因此需重新训练</li>
<li>向目标模型添加输出层，其输出数是目标数据集中的类别数，然后随机初始化该层的模型参数</li>
<li>在目标数据集上训练目标模型，输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调</li>
</ol>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/finetune.jpg" alt="finetune" style="zoom:80%;">

<p>当目标数据集比源数据集小得多时，微调有助于提高模型的泛化能力</p>
<h3 id="热狗识别"><a href="#热狗识别" class="headerlink" title="热狗识别"></a>热狗识别</h3><p>在一个小型数据集上微调ResNet模型，该模型已在ImageNet数据集上进行了预训练。小型数据集包含数千张包含热狗和不包含热狗的图像，使用微调模型来识别图像中是否包含热狗</p>
<h4 id="获取数据集"><a href="#获取数据集" class="headerlink" title="获取数据集"></a>获取数据集</h4><p>使用的热狗数据集来源于网络，该数据集包含1400张热狗的“正类”图像，以及包含尽可能多的其他食物的“负类”图像，含着两个类别的1000张图片用于训练，其余的则用于测试</p>
<p>解压下载的数据集，获得了两个文件夹<code>hotdog/train</code>和<code>hotdog/test</code>，这两个文件夹都有<code>hotdog</code>(有热狗)和<code>not-hotdog</code>(无热狗)两个子文件夹，子文件夹内都包含相应类的图像</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DATA_HUB = <span class="built_in">dict</span>()</span><br><span class="line">DATA_URL = <span class="string">'http://d2l-data.s3-accelerate.amazonaws.com/'</span></span><br><span class="line">DATA_HUB[<span class="string">'hotdog'</span>] = (  <span class="comment">#@save</span></span><br><span class="line">    DATA_URL + <span class="string">'hotdog.zip'</span>,</span><br><span class="line">    <span class="string">'fba480ffa8aa7e0febbb511d181409f899b9baa5'</span>)</span><br><span class="line">data_dir = download_extract(<span class="string">'hotdog'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>创建两个实例来分别读取训练和测试数据集中的所有图像文件</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_imgs = datasets.ImageFolder(os.path.join(data_dir, <span class="string">'train'</span>))</span><br><span class="line">test_imgs = datasets.ImageFolder(os.path.join(data_dir, <span class="string">'test'</span>))</span><br></pre></td></tr></tbody></table></figure>

<p>显示前8个正类样本图片和最后8张负类样本图片，图像的大小和纵横比各有不同</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hotdogs = [train_imgs[i][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>)]</span><br><span class="line">not_hotdogs = [train_imgs[-i - <span class="number">1</span>][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>)]</span><br><span class="line">show_images(hotdogs + not_hotdogs, <span class="number">2</span>, <span class="number">8</span>, scale=<span class="number">1.4</span>);</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511021414.webp" alt="202511021414" style="zoom: 50%;">

<ul>
<li><p>输入图像用源模型(预训练模型)原始数据集的均值和标准差来做标准化</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">normalize = torchvision.transforms.Normalize(</span><br><span class="line">    <span class="comment"># ImageNet 数据集统计得到</span></span><br><span class="line">    [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],   <span class="comment"># 每个通道的均值(mean)</span></span><br><span class="line">    [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]    <span class="comment"># 每个通道的标准差(std)</span></span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>在训练期间，首先从图像中裁切随机大小和随机长宽比的区域，然后将该区域缩放为224×224的输入图像，再进行随机水平翻转和标准化</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_augs = torchvision.transforms.Compose([</span><br><span class="line">    <span class="comment"># 随机裁剪图像到224×224</span></span><br><span class="line">    torchvision.transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">    <span class="comment"># 随机水平翻转图像，默认概率0.5</span></span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    <span class="comment"># 把PIL图像或numpy数组转换为Tensor格式，并把像素归一化到 [0,1]</span></span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    <span class="comment"># 按ImageNet统计值对图像进行标准化</span></span><br><span class="line">    normalize])</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>在测试过程中将图像的高度和宽度都缩放到256像素，然后裁剪中央224×224区域作为输入</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">test_augs = torchvision.transforms.Compose([</span><br><span class="line">    <span class="comment"># 先把图像缩放到256×256，保持长宽比</span></span><br><span class="line">    torchvision.transforms.Resize([<span class="number">256</span>, <span class="number">256</span>]),</span><br><span class="line">    <span class="comment"># 从中心裁剪出224×224区域，与训练时的尺寸一致</span></span><br><span class="line">    torchvision.transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    <span class="comment"># 转换为Tensor格式，并把像素归一化到 [0,1]</span></span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    <span class="comment"># 使用同样的ImageNet标准化参数</span></span><br><span class="line">    normalize])</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<h4 id="定义和初始化模型"><a href="#定义和初始化模型" class="headerlink" title="定义和初始化模型"></a>定义和初始化模型</h4><p>使用在ImageNet数据集上预训练的ResNet-18作为源模型，在这里指定<code>pretrained=True</code>以自动下载预训练的模型参数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>预训练的源模型实例包含许多特征层和一个输出层<code>fc</code>，此划分的主要目的是促进对除输出层以外所有层的模型参数进行微调，下面给出了源模型的成员变量<code>fc</code></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net.fc</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Linear(in_features=512, out_features=1000, bias=True)</span><br></pre></td></tr></tbody></table></figure>

<p>在ResNet的全局平均池化层后，原来的全连接层输出1000个ImageNet类</p>
<p>构建目标模型时，结构与预训练模型相同，只将输出层改为目标数据集的类别数，目标模型的特征层参数继承自预训练模型，只需用较小学习率微调</p>
<p>输出层参数随机初始化，需要更大学习率(约为特征层的10倍)来从头训练</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">finetune_net = torchvision.models.resnet18(pretrained=<span class="literal">True</span>) <span class="comment"># 使用 在 ImageNet 上训练好的权重</span></span><br><span class="line">finetune_net.fc = nn.Linear(finetune_net.fc.in_features, <span class="number">2</span>) <span class="comment"># 调整输出层的输出类</span></span><br><span class="line">nn.init.xavier_uniform_(finetune_net.fc.weight);</span><br></pre></td></tr></tbody></table></figure>

<h4 id="微调模型"><a href="#微调模型" class="headerlink" title="微调模型"></a>微调模型</h4><p>定义了一个训练函数<code>train_fine_tuning</code>，该函数使用微调，因此可以多次调用</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">data_dir = download_extract(<span class="string">'hotdog'</span>) <span class="comment"># 需要前定义data_dir</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果param_group=True，输出层中的模型参数将使用十倍的学习率</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_fine_tuning</span>(<span class="params">net, learning_rate, batch_size=<span class="number">128</span>, num_epochs=<span class="number">5</span>, param_group=<span class="literal">True</span></span>):</span><br><span class="line">    train_iter = data.DataLoader(datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">'train'</span>), transform=train_augs),</span><br><span class="line">        batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">'test'</span>), transform=test_augs),</span><br><span class="line">        batch_size=batch_size)</span><br><span class="line">    devices = try_all_gpus()</span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">"none"</span>)</span><br><span class="line">    <span class="keyword">if</span> param_group:  <span class="comment"># 如果设置不同层不同学习率</span></span><br><span class="line">        params_1x = [param <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()</span><br><span class="line">             <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">"fc.weight"</span>, <span class="string">"fc.bias"</span>]] <span class="comment"># 过滤掉最后的全连接层参数</span></span><br><span class="line">        trainer = torch.optim.SGD([{<span class="string">'params'</span>: params_1x},</span><br><span class="line">                                   {<span class="string">'params'</span>: net.fc.parameters(),</span><br><span class="line">                                    <span class="string">'lr'</span>: learning_rate * <span class="number">10</span>}],</span><br><span class="line">                                lr=learning_rate, weight_decay=<span class="number">0.001</span>)</span><br><span class="line">    <span class="keyword">else</span>: <span class="comment"># 所有层用一样的学习率</span></span><br><span class="line">        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,</span><br><span class="line">                                  weight_decay=<span class="number">0.001</span>)</span><br><span class="line">    train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_fine_tuning(finetune_net, <span class="number">5e-5</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 2.481, acc 0.708, test acc 0.924, time 22.67 sec</span><br><span class="line">epoch 2, loss 0.281, acc 0.912, test acc 0.931, time 19.62 sec</span><br><span class="line">epoch 3, loss 0.446, acc 0.888, test acc 0.879, time 19.63 sec</span><br><span class="line">epoch 4, loss 0.217, acc 0.932, test acc 0.931, time 19.73 sec</span><br><span class="line">epoch 5, loss 0.176, acc 0.930, test acc 0.919, time 19.72 sec</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511021506.webp" alt="202511021506" style="zoom:80%;">

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.176, train acc 0.930, test acc 0.919</span><br><span class="line">339.0 examples/sec on [device(type='cuda', index=0)]</span><br></pre></td></tr></tbody></table></figure>

<p>为了进行比较，定义了一个相同的模型，但是将其所有模型参数初始化为随机值，由于整个模型需要从头开始训练，因此需要使用更大的学习率</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scratch_net = torchvision.models.resnet18()</span><br><span class="line">scratch_net.fc = nn.Linear(scratch_net.fc.in_features, <span class="number">2</span>)</span><br><span class="line">train_fine_tuning(scratch_net, <span class="number">5e-4</span>, param_group=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 1.664, acc 0.673, test acc 0.812, time 19.94 sec</span><br><span class="line">epoch 2, loss 0.385, acc 0.837, test acc 0.844, time 19.95 sec</span><br><span class="line">epoch 3, loss 0.395, acc 0.824, test acc 0.795, time 19.47 sec</span><br><span class="line">epoch 4, loss 0.392, acc 0.830, test acc 0.766, time 19.94 sec</span><br><span class="line">epoch 5, loss 0.371, acc 0.836, test acc 0.844, time 19.73 sec</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511021507.webp" alt="202511021507" style="zoom:80%;">

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss <span class="number">0.371</span>, train acc <span class="number">0.836</span>, test acc <span class="number">0.844</span></span><br><span class="line"><span class="number">340.4</span> examples/sec on [device(<span class="built_in">type</span>=<span class="string">'cuda'</span>, index=<span class="number">0</span>)]</span><br></pre></td></tr></tbody></table></figure>

<p>意料之中，微调模型往往表现更好，因为它的初始参数值更有效</p>
<h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><ul>
<li>迁移学习将从源数据集中学到的知识迁移到目标数据集，微调是迁移学习的常见技巧</li>
<li>除输出层外，目标模型从源模型中复制所有模型设计及其参数，并根据目标数据集对这些参数进行微调。但是目标模型的输出层需要从头开始训练</li>
<li>通常，微调参数使用较小的学习率，而从头开始训练输出层可以使用更大的学习率</li>
</ul>
<h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><ol>
<li><p>将输出层<code>finetune_net</code>之前的参数设置为源模型的参数，在训练期间不要更新它们。模型的准确性如何变化？</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 先冻结所有参数</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> finetune_net.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 再单独解冻输出层(fc层)</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> finetune_net.fc.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></tbody></table></figure>

<p>冻结除输出层外的所有参数时模型训练更快、更稳定，但由于特征无法适应新任务，准确率通常比“微调全部层”略低</p>
</li>
</ol>
<h2 id="目标检测和边界框"><a href="#目标检测和边界框" class="headerlink" title="目标检测和边界框"></a>目标检测和边界框</h2><p>在图像分类中通常假设图像只有一个主要目标，但若图像中存在多个对象，不仅要识别它们的类别，还要确定它们在图像中的位置，将这类任务称为<strong>目标检测(object detection)</strong></p>
<p>目标检测在多个领域中被广泛使用，例如，在无人驾驶里，需要通过识别拍摄到的视频图像里的车辆、行人、道路和障碍物的位置来规划行进线路；机器人也常通过该任务来检测感兴趣的目标；安防领域则需要检测异常目标，如歹徒或者炸弹</p>
<p>接下来以下图为例</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">'imgs/catdog.jpg'</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511021602.webp" alt="202511021602" style="zoom: 50%;">

<h3 id="边界框"><a href="#边界框" class="headerlink" title="边界框"></a>边界框</h3><p>在目标检测中通常使用**边界框(bounding box)**来描述对象的空间位置</p>
<p>边界框是矩形的，由矩形左上角的以及右下角的$(x,y)$坐标决定，另一种常用的边界框表示方法是边界框中心的$(x,y)$坐标以及框的宽度和高度</p>
<p>定义在这两种表示法之间进行转换的函数</p>
<p><code>box_corner_to_center</code>从两角表示法转换为中心宽度表示法，而<code>box_center_to_corner</code>反之</p>
<p>输入参数<code>boxes</code>可以是长度为4的张量，也可以是形状为$(n，4)$的二维张量，其中$n$是边界框的数量</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">box_corner_to_center</span>(<span class="params">boxes</span>):</span><br><span class="line">    <span class="string">"""从(左上，右下)转换到(中间，宽度，高度)"""</span></span><br><span class="line">    x1, y1, x2, y2 = boxes[:, <span class="number">0</span>], boxes[:, <span class="number">1</span>], boxes[:, <span class="number">2</span>], boxes[:, <span class="number">3</span>]</span><br><span class="line">    cx = (x1 + x2) / <span class="number">2</span></span><br><span class="line">    cy = (y1 + y2) / <span class="number">2</span></span><br><span class="line">    w = x2 - x1</span><br><span class="line">    h = y2 - y1</span><br><span class="line">    <span class="comment"># axis=-1 表示在最后一个维度上拼接，也就是每一行变成 [中心x, 中心y, 宽, 高]</span></span><br><span class="line">    boxes = torch.stack((cx, cy, w, h), axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> boxes</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">box_center_to_corner</span>(<span class="params">boxes</span>):</span><br><span class="line">    <span class="string">"""从(中间，宽度，高度)转换到(左上，右下)"""</span></span><br><span class="line">    cx, cy, w, h = boxes[:, <span class="number">0</span>], boxes[:, <span class="number">1</span>], boxes[:, <span class="number">2</span>], boxes[:, <span class="number">3</span>]</span><br><span class="line">    x1 = cx - <span class="number">0.5</span> * w</span><br><span class="line">    y1 = cy - <span class="number">0.5</span> * h</span><br><span class="line">    x2 = cx + <span class="number">0.5</span> * w</span><br><span class="line">    y2 = cy + <span class="number">0.5</span> * h</span><br><span class="line">    <span class="comment"># 把结果在最后一个维度拼接为 [x1, y1, x2, y2]</span></span><br><span class="line">    boxes = torch.stack((x1, y1, x2, y2), axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> boxes</span><br></pre></td></tr></tbody></table></figure>

<p>将根据坐标信息定义图像中狗和猫的边界框</p>
<p><font color="DarkViolet">图像中坐标的原点是图像的左上角，向右的方向为x轴的正方向，向下的方向为y轴的正方向</font></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dog_bbox, cat_bbox = [<span class="number">60.0</span>, <span class="number">45.0</span>, <span class="number">378.0</span>, <span class="number">516.0</span>], [<span class="number">400.0</span>, <span class="number">112.0</span>, <span class="number">655.0</span>, <span class="number">493.0</span>]</span><br></pre></td></tr></tbody></table></figure>

<p>通过转换两次来验证边界框转换函数的正确性</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">boxes = torch.tensor((dog_bbox, cat_bbox))</span><br><span class="line">box_center_to_corner(box_corner_to_center(boxes)) == boxes</span><br><span class="line"><span class="comment"># tensor([[True, True, True, True],</span></span><br><span class="line"><span class="comment">#        [True, True, True, True]])</span></span><br></pre></td></tr></tbody></table></figure>

<p>可以将边界框在图中画出，以检查其是否准确</p>
<p>画之前定义一个辅助函数<code>bbox_to_rect</code>，它将边界框表示成<code>matplotlib</code>的边界框格式</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_to_rect</span>(<span class="params">bbox, color</span>):</span><br><span class="line">    <span class="comment"># 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：</span></span><br><span class="line">    <span class="comment"># ((左上x,左上y),宽,高)</span></span><br><span class="line">    <span class="keyword">return</span> d2l.plt.Rectangle(</span><br><span class="line">        xy=(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), width=bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>], height=bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>],</span><br><span class="line">        fill=<span class="literal">False</span>, edgecolor=color, linewidth=<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">'imgs/catdog.jpg'</span>)</span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.imshow(img)</span><br><span class="line">ax.add_patch(bbox_to_rect(dog_bbox, <span class="string">'blue'</span>)) <span class="comment"># add_patch内输入rect</span></span><br><span class="line">ax.add_patch(bbox_to_rect(cat_bbox, <span class="string">'red'</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511021616.webp" alt="202511021616" style="zoom: 50%;">

<h2 id="锚框"><a href="#锚框" class="headerlink" title="锚框"></a>锚框</h2><p>目标检测算法通常会在输入图像中采样大量的区域然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界从而更准确地预测目标的<strong>真实边界框(ground-truth bounding box)</strong></p>
<p>不同的模型使用的区域采样方法可能不同，这里使用的是以每个像素为中心，生成多个缩放比和<strong>宽高比(aspect ratio)<strong>不同的边界框，这些边界框被称为</strong>锚框(anchor box)</strong></p>
<p>修改输出精度，以获得更简洁的输出</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.set_printoptions(<span class="number">2</span>)  <span class="comment"># 精简输出精度</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="生成多个锚框"><a href="#生成多个锚框" class="headerlink" title="生成多个锚框"></a>生成多个锚框</h3><p>假设输入图像的高度为$h$，高度为$w$，以图像的每个像素为中心生成不同形状的锚框：缩放比为$s\in (0, 1]$，宽高比为$r&gt;0$<br>$$<br>A = hw \rightarrow A_{anchor} = s^2\times A=s^2 hw=h_aw_a<br>$$<br>代入$w_a = rh_a$<br>$$<br>rh_a^2=s^2hw\rightarrow h_a=s\sqrt{hw/r}\rightarrow w_a=s\sqrt{hwr}<br>$$<br>在很多检测网络中输入图像$h=w$，那么锚框的宽度和高度<br>$$<br>w_a = hs\sqrt r\qquad h_a=hs/\sqrt r<br>$$<br>要生成多个不同形状的锚框只需要修改缩放比$s_1,\ldots, s_n$和宽高比$r_1,\ldots, r_m$，当使用这些比例和长宽比的所有组合以每个像素为中心时，输入图像将总共有$whnm$个锚框，尽管这些锚框可能会覆盖所有真实边界框，但计算复杂性很容易过高</p>
<p>在实践中，只考虑包含$s_1$或$r_1$的组合<br>$$<br>(s_1, r_1), (s_1, r_2), \ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \ldots, (s_n, r_1).<br>$$<br>以同一像素为中心的锚框的数量是$n+m-1$，对于整个输入图像，将共生成$wh(n+m-1)$个锚框</p>
<p>上述生成锚框的方法在下面的<code>multibox_prior</code>函数中实现，指定输入图像、尺寸列表和宽高比列表，然后此函数将返回所有的锚框</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multibox_prior</span>(<span class="params">data, sizes, ratios</span>):</span><br><span class="line">    <span class="string">"""生成以每个像素为中心具有不同形状的锚框"""</span></span><br><span class="line">    <span class="comment"># 输入特征图的高和宽，例如(1, 3, 32, 32)，则 in_height=32, in_width=32</span></span><br><span class="line">    in_height, in_width = data.shape[-<span class="number">2</span>:]</span><br><span class="line">    device, num_sizes, num_ratios = data.device, <span class="built_in">len</span>(sizes), <span class="built_in">len</span>(ratios)</span><br><span class="line">    <span class="comment"># 以同一像素为中心的锚框的数量是 n+m-1</span></span><br><span class="line">    boxes_per_pixel = (num_sizes + num_ratios - <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 把缩放比 (sizes) 和宽高比 (ratios) 转成 Tensor</span></span><br><span class="line">    size_tensor = torch.tensor(sizes, device=device)</span><br><span class="line">    ratio_tensor = torch.tensor(ratios, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为了让锚框中心落在像素中心，设置偏移量 0.5</span></span><br><span class="line">    offset_h, offset_w = <span class="number">0.5</span>, <span class="number">0.5</span></span><br><span class="line">    <span class="comment"># 把坐标缩放到 [0, 1] 区间</span></span><br><span class="line">    steps_h = <span class="number">1.0</span> / in_height  <span class="comment"># 在y轴上缩放步长</span></span><br><span class="line">    steps_w = <span class="number">1.0</span> / in_width  <span class="comment"># 在x轴上缩放步长</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成锚框的所有中心点</span></span><br><span class="line">    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h</span><br><span class="line">    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w</span><br><span class="line">    <span class="comment"># 生成所有像素中心点网格(shift_y, shift_x)</span></span><br><span class="line">    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing=<span class="string">'ij'</span>) <span class="comment"># y绑定i(行)，x绑定j(列)</span></span><br><span class="line">    <span class="comment"># 拉平成一维向量(方便后续拼接)</span></span><br><span class="line">    shift_y, shift_x = shift_y.reshape(-<span class="number">1</span>), shift_x.reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 得到每个锚框在归一化坐标系(0~1)下的宽高</span></span><br><span class="line">    <span class="comment"># 只考虑包含s_1或r_1的组合</span></span><br><span class="line">    w = (torch.cat((size_tensor * torch.sqrt(ratio_tensor[<span class="number">0</span>]), sizes[<span class="number">0</span>] * torch.sqrt(ratio_tensor[<span class="number">1</span>:])))</span><br><span class="line">         * in_height / in_width)  <span class="comment"># 处理矩形输入，获得正方形</span></span><br><span class="line">    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[<span class="number">0</span>]),</span><br><span class="line">                   sizes[<span class="number">0</span>] / torch.sqrt(ratio_tensor[<span class="number">1</span>:])))</span><br><span class="line">    <span class="comment"># 除以2来获得半高和半宽</span></span><br><span class="line">    <span class="comment"># stack拼接，每列为一种框，然后转置变成每行一种框，然后复制到每个像素上</span></span><br><span class="line">    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(in_height * in_width, <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个中心点都将有“boxes_per_pixel”个锚框，</span></span><br><span class="line">    <span class="comment"># 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次</span></span><br><span class="line">    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],</span><br><span class="line">                dim=<span class="number">1</span>).repeat_interleave(boxes_per_pixel, dim=<span class="number">0</span>)</span><br><span class="line">    output = out_grid + anchor_manipulations</span><br><span class="line">    <span class="keyword">return</span> output.unsqueeze(<span class="number">0</span>) <span class="comment"># 增加一个 batch 维度</span></span><br></pre></td></tr></tbody></table></figure>

<p>返回的锚框变量<code>Y</code>的形状是(批量大小，锚框的数量，4)</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">'imgs/catdog.jpg'</span>)</span><br><span class="line">w, h = img.size <span class="comment"># PIL 返回 (width, height)</span></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">3</span>, h, w))</span><br><span class="line">Y = multibox_prior(X, sizes=[<span class="number">0.75</span>, <span class="number">0.5</span>, <span class="number">0.25</span>], ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span><br><span class="line">boxes = Y.reshape(h, w, <span class="number">5</span>, <span class="number">4</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>将锚框变量<code>Y</code>的形状更改为(图像高度，图像宽度，以同一像素为中心的锚框的数量，4)后可以获得以指定像素的位置为中心的所有锚框</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">boxes = Y.reshape(h, w, <span class="number">5</span>, <span class="number">4</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>访问以(250,250)为中心的第一个锚框</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">boxes[<span class="number">250</span>, <span class="number">250</span>, <span class="number">0</span>, :]</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0.06, 0.07, 0.63, 0.82])</span><br></pre></td></tr></tbody></table></figure>

<p>为了显示以图像中以某个像素为中心的所有锚框，定义下面的<code>show_bboxes</code>函数来在图像上绘制多个边界框</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_bboxes</span>(<span class="params">axes, bboxes, labels=<span class="literal">None</span>, colors=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">"""显示所有边界框"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_list</span>(<span class="params">obj, default_values=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> obj <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            obj = default_values</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(obj, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            obj = [obj]</span><br><span class="line">        <span class="keyword">return</span> obj</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 处理标签和颜色，使它们一定是列表格式</span></span><br><span class="line">    labels = _make_list(labels)</span><br><span class="line">    colors = _make_list(colors, [<span class="string">'b'</span>, <span class="string">'g'</span>, <span class="string">'r'</span>, <span class="string">'m'</span>, <span class="string">'c'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 遍历每一个边界框进行绘制</span></span><br><span class="line">    <span class="keyword">for</span> i, bbox <span class="keyword">in</span> <span class="built_in">enumerate</span>(bboxes):</span><br><span class="line">        color = colors[i % <span class="built_in">len</span>(colors)]</span><br><span class="line">        <span class="comment"># bbox 为 [xmin, ymin, xmax, ymax]</span></span><br><span class="line">        <span class="comment"># bbox_to_rect 会返回 Rectangle(xy=(xmin, ymin), width=w, height=h)</span></span><br><span class="line">        rect = bbox_to_rect(bbox.detach().numpy(), color)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将矩形框添加到坐标轴上</span></span><br><span class="line">        axes.add_patch(rect)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果传入了标签，就在对应的框上方绘制文字</span></span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">and</span> <span class="built_in">len</span>(labels) &gt; i:</span><br><span class="line">            text_color = <span class="string">'k'</span> <span class="keyword">if</span> color == <span class="string">'w'</span> <span class="keyword">else</span> <span class="string">'w'</span></span><br><span class="line">            axes.text(rect.xy[<span class="number">0</span>], rect.xy[<span class="number">1</span>], labels[i],</span><br><span class="line">                      va=<span class="string">'center'</span>, ha=<span class="string">'center'</span>, fontsize=<span class="number">9</span>, color=text_color,</span><br><span class="line">                      bbox=<span class="built_in">dict</span>(facecolor=color, lw=<span class="number">0</span>))</span><br></pre></td></tr></tbody></table></figure>

<p>变量<code>boxes</code>中xy轴的坐标值已经分别除以图像的宽度和高度，绘制锚框时需要恢复它们原始的坐标值，在下面定义了变量<code>bbox_scale</code>，现在可以绘制出图像中所有以(250,250)为中心的锚框了</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bbox_scale = torch.tensor((w, h, w, h))</span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.imshow(img)</span><br><span class="line">show_bboxes(ax, boxes[<span class="number">250</span>, <span class="number">250</span>, :, :] * bbox_scale,</span><br><span class="line">            [<span class="string">'s=0.75, r=1'</span>, <span class="string">'s=0.5, r=1'</span>, <span class="string">'s=0.25, r=1'</span>, <span class="string">'s=0.75, r=2'</span>,</span><br><span class="line">             <span class="string">'s=0.75, r=0.5'</span>])</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511021727.webp" alt="202511021727" style="zoom: 50%;">

<p>缩放比为0.75且宽高比为1的蓝色锚框很好地围绕着图像中的狗</p>
<h3 id="交并比-IoU"><a href="#交并比-IoU" class="headerlink" title="交并比(IoU)"></a>交并比(IoU)</h3><p>如果已知目标的真实边界框，那么这里的“好”该如何如何量化呢？</p>
<p>直观地说，可以衡量锚框和真实边界框之间的相似性</p>
<p><strong>杰卡德系数(Jaccard)<strong>可以衡量两组之间的相似性，给定集合$\mathcal{A}$和$\mathcal{B}$，他们的杰卡德系数是他们交集的大小除以他们并集的大小：<br>$$<br>J(\mathcal{A},\mathcal{B}) = \frac{\left|\mathcal{A} \cap \mathcal{B}\right|}{\left| \mathcal{A} \cup \mathcal{B}\right|}.<br>$$<br>两个边界框的相似性可用它们像素区域的杰卡德系数衡量，这个系数通常称为交并比</strong>(intersection over union，IoU)</strong>，即两个边界框相交面积与相并面积之比</p>
<p>交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框完全重合</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/iou.jpg" alt="iou" style="zoom:80%;">

<p>接下来部分将使用交并比来衡量锚框和真实边界框之间、以及不同锚框之间的相似度</p>
<p>给定两个锚框或边界框的列表，以下<code>box_iou</code>函数将在这两个列表中计算它们成对的交并比</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">box_iou</span>(<span class="params">boxes1, boxes2</span>):</span><br><span class="line">    <span class="string">"""计算两个锚框或边界框列表中成对的交并比"""</span></span><br><span class="line">    box_area = <span class="keyword">lambda</span> boxes: ((boxes[:, <span class="number">2</span>] - boxes[:, <span class="number">0</span>]) *</span><br><span class="line">                              (boxes[:, <span class="number">3</span>] - boxes[:, <span class="number">1</span>]))</span><br><span class="line">    <span class="comment"># boxes1：(boxes1的数量N,4),</span></span><br><span class="line">    <span class="comment"># boxes2：(boxes2的数量M,4),</span></span><br><span class="line">    <span class="comment"># areas1：(N,),</span></span><br><span class="line">    <span class="comment"># areas2：(M,)</span></span><br><span class="line">    areas1 = box_area(boxes1)</span><br><span class="line">    areas2 = box_area(boxes2)</span><br><span class="line">    <span class="comment"># 利用广播技巧：torch.min/max()后获得 (N, M, 2)</span></span><br><span class="line">    inter_upperlefts = torch.<span class="built_in">max</span>(boxes1[:, <span class="literal">None</span>, :<span class="number">2</span>], boxes2[:, :<span class="number">2</span>])  <span class="comment"># 左上角坐标</span></span><br><span class="line">    inter_lowerrights = torch.<span class="built_in">min</span>(boxes1[:, <span class="literal">None</span>, <span class="number">2</span>:], boxes2[:, <span class="number">2</span>:]) <span class="comment"># 右下角坐标</span></span><br><span class="line">    inters = (inter_lowerrights - inter_upperlefts).clamp(<span class="built_in">min</span>=<span class="number">0</span>) <span class="comment"># 得到交集的宽高，负数置0</span></span><br><span class="line">    <span class="comment"># 交集面积 = 交集宽 * 交集高，形状 (N, M)</span></span><br><span class="line">    inter_areas = inters[:, :, <span class="number">0</span>] * inters[:, :, <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># areas1[:, None] 形状 (N, 1) 经过广播与 (M,) 相加得到 (N, M)</span></span><br><span class="line">    union_areas = areas1[:, <span class="literal">None</span>] + areas2 - inter_areas</span><br><span class="line">    <span class="keyword">return</span> inter_areas / union_areas <span class="comment"># 返回的是一个 N×M 的 IoU 矩阵</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="在训练数据中标注锚框"><a href="#在训练数据中标注锚框" class="headerlink" title="在训练数据中标注锚框"></a>在训练数据中标注锚框</h3><p>在目标检测中，每个锚框都作为一个训练样本，需要预测类别和位置偏移。训练时将锚框与最接近的真实边界框匹配，赋予对应的类别和偏移标签；预测时生成所有锚框的类别与偏移，并根据偏移调整锚框位置，最后筛选出符合条件的预测框</p>
<h4 id="将真实边界框分配给锚框"><a href="#将真实边界框分配给锚框" class="headerlink" title="将真实边界框分配给锚框"></a>将真实边界框分配给锚框</h4><p>给定图像，假设锚框是$A_1, A_2, \ldots, A_{n_a}$，真实边界框是$B_1, B_2, \ldots, B_{n_b}$，其中$n_a \geq n_b$，定义一个矩阵$\mathbf X \in \mathbb{R}^{n_a \times n_b}$，其中第$i$行第$j$列的元素$x_{ij}$是锚框$A_i$和真实边界框$B_j$的IoU，该算法包含以下步骤</p>
<ol>
<li>在IoU矩阵中找到最大的元素，其行列索引分别对应一个锚框和真实边界框，将该真实边界框分配给该锚框，并从IoU矩阵中删除对应的行和列</li>
<li>重复上述步骤，直到每个真实边界框都分配给一个锚框为止</li>
<li>对剩余未分配的$n_a-n_b$个锚框，找到 IoU 最大的真实边界框；若该 IoU 大于预设阈值，则将该真实框分配给该锚框</li>
</ol>
<p>用一个具体的例子来说明上述算法</p>
<p>假设矩阵中的最大值为$x_{23}$，将真实边界框$B_3$分配给$A_2$，然后丢弃矩阵第2行和第3列中的所有元素(左图)；在剩余元素(阴影区域)中找到最大的$x_{71}$，然后将真实边界框$B_1$分配给$A_7$，丢弃矩阵第7行和第1列中的所有元素(中图)；在剩余元素(阴影区域)中找到最大的$x_{54}$，然后将真实边界框$B_4$分配给锚框$A_5$，丢弃后在剩余找到最大的$x_{92}$，然后将真实边界框$B_2$分配给锚框$A_9$(右图)；最后遍历剩余的锚框$A_1, A_3, A_4, A_6, A_8$，然后根据阈值判断是否为它们分配真实边界框</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/anchor-label.jpg" alt="anchor-label" style="zoom:80%;">

<p>此算法在下面的<code>assign_anchor_to_bbox</code>函数中实现</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">assign_anchor_to_bbox</span>(<span class="params">ground_truth, anchors, device, iou_threshold=<span class="number">0.5</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将每个锚框(anchor)分配给最接近的真实边界框(ground truth box)。</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    ground_truth : (n_gt, 4)</span></span><br><span class="line"><span class="string">    anchors : (n_anchor, 4)</span></span><br><span class="line"><span class="string">    iou_threshold : 超过该值的锚框才会被视为“正样本”(即分配给某个真实框)</span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    anchors_bbox_map : (n_anchor,)</span></span><br><span class="line"><span class="string">        其中第 i 个元素表示该锚框对应的真实边界框索引；</span></span><br><span class="line"><span class="string">        若值为 -1，表示该锚框未分配(负样本)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    num_anchors, num_gt_boxes = anchors.shape[<span class="number">0</span>], ground_truth.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 计算所有锚框与真实框之间的 IoU，结果形状为 (num_anchors, num_gt_boxes)</span></span><br><span class="line">    jaccard = box_iou(anchors, ground_truth)</span><br><span class="line">    <span class="comment"># 创建一个映射张量，初始化为 -1，表示“尚未匹配到真实框”</span></span><br><span class="line">    anchors_bbox_map = torch.full((num_anchors,), -<span class="number">1</span>, dtype=torch.long,</span><br><span class="line">                                  device=device)</span><br><span class="line">    <span class="comment"># 对每个锚框，找到它与所有真实框 IoU 中的最大值，以及对应的真实框索引</span></span><br><span class="line">    max_ious, indices = torch.<span class="built_in">max</span>(jaccard, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 找出那些 IoU ≥ 阈值的锚框索引</span></span><br><span class="line">    anc_i = torch.nonzero(max_ious &gt;= iou_threshold).reshape(-<span class="number">1</span>)</span><br><span class="line">    box_j = indices[max_ious &gt;= iou_threshold]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把这些锚框分配给对应的真实框</span></span><br><span class="line">    anchors_bbox_map[anc_i] = box_j</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用于标记已丢弃的行列(避免重复分配)</span></span><br><span class="line">    col_discard = torch.full((num_anchors,), -<span class="number">1</span>)</span><br><span class="line">    row_discard = torch.full((num_gt_boxes,), -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_gt_boxes):</span><br><span class="line">        max_idx = torch.argmax(jaccard)</span><br><span class="line">        box_idx = (max_idx % num_gt_boxes).long()</span><br><span class="line">        anc_idx = (max_idx / num_gt_boxes).long()</span><br><span class="line">        anchors_bbox_map[anc_idx] = box_idx</span><br><span class="line">        <span class="comment"># 把该行与该列都“丢弃”，避免重复匹配</span></span><br><span class="line">        jaccard[:, box_idx] = col_discard</span><br><span class="line">        jaccard[anc_idx, :] = row_discard</span><br><span class="line">    <span class="keyword">return</span> anchors_bbox_map</span><br></pre></td></tr></tbody></table></figure>

<h4 id="标记类别和偏移量"><a href="#标记类别和偏移量" class="headerlink" title="标记类别和偏移量"></a>标记类别和偏移量</h4><p>现在可以为每个锚框标记类别和偏移量了，假设一个锚框$A$被分配给了一个真实边界框$B$</p>
<p>一方面，锚框A的类别将被标记为与B相同，另一方面，锚框A的偏移量将根据B和A中心坐标的相对位置以及这两个框的相对大小进行标记，鉴于数据集内不同的框的位置和大小不同，可以对那些相对位置和大小应用变换，使其获得分布更均匀且易于拟合的偏移量</p>
<p>可以将A的偏移量标记为：<br>$$<br>\left( \frac{ \frac{x_b - x_a}{w_a} - \mu_x }{\sigma_x},<br>\frac{ \frac{y_b - y_a}{h_a} - \mu_y }{\sigma_y},<br>\frac{ \log \frac{w_b}{w_a} - \mu_w }{\sigma_w},<br>\frac{ \log \frac{h_b}{h_a} - \mu_h }{\sigma_h}\right),<br>$$<br>其中常量的默认值为$\mu_x = \mu_y = \mu_w = \mu_h = 0, \sigma_x=\sigma_y=0.1, \sigma_w=\sigma_h=0.2$，这种转换在下面的<code>offset_boxes</code>函数中实现<br>$$<br>\Delta x=10 \times \frac{\left(x_{g t}-x_{a n c}\right)}{w_{a n c}}, \quad \Delta y=10 \times \frac{\left(y_{g t}-y_{a n c}\right)}{h_{a n c}} \\<br>\Delta w=5 \times \log \left(\frac{w_{g t}}{w_{a n c}}+\epsilon\right), \quad \Delta h=5 \times \log \left(\frac{h_{g t}}{h_{a n c}}+\epsilon\right)<br>$$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">offset_boxes</span>(<span class="params">anchors, assigned_bb, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">    <span class="string">"""对锚框偏移量的转换"""</span></span><br><span class="line">    c_anc = box_corner_to_center(anchors)</span><br><span class="line">    c_assigned_bb = box_corner_to_center(assigned_bb)</span><br><span class="line">    <span class="comment"># ×10 常用，人为放大偏移，使偏移值分布更明显，便于训练</span></span><br><span class="line">    offset_xy = <span class="number">10</span> * (c_assigned_bb[:, :<span class="number">2</span>] - c_anc[:, :<span class="number">2</span>]) / c_anc[:, <span class="number">2</span>:]</span><br><span class="line">    <span class="comment"># × 5 是缩放因子，使训练时偏移分布集中</span></span><br><span class="line">    offset_wh = <span class="number">5</span> * torch.log(eps + c_assigned_bb[:, <span class="number">2</span>:] / c_anc[:, <span class="number">2</span>:])</span><br><span class="line">    <span class="comment"># 拼接成完整偏移向量</span></span><br><span class="line">    offset = torch.cat([offset_xy, offset_wh], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> offset</span><br></pre></td></tr></tbody></table></figure>

<p>如果一个锚框没有被分配真实边界框，只需将锚框的类别标记为背景(background)，背景类别的锚框通常被称为负类锚框，其余的被称为正类锚框</p>
<p>使用真实边界框(<code>labels</code>参数)实现以下<code>multibox_target</code>函数，来标记锚框的类别和偏移量(<code>anchors</code>参数)</p>
<p>此函数将背景类别的索引设置为零，然后将新类别的整数索引递增一</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multibox_target</span>(<span class="params">anchors, labels</span>):</span><br><span class="line">    <span class="string">"""使用真实边界框标记锚框</span></span><br><span class="line"><span class="string">    参数</span></span><br><span class="line"><span class="string">    anchors : torch.Tensor, 形状 (1, num_anchors, 4)</span></span><br><span class="line"><span class="string">        所有锚框的坐标 [xmin, ymin, xmax, ymax]，通常来自 multibox_prior()</span></span><br><span class="line"><span class="string">    labels : torch.Tensor, 形状 (batch_size, num_objects, 5)</span></span><br><span class="line"><span class="string">        每张图片的真实标注，格式为 [class, xmin, ymin, xmax, ymax]</span></span><br><span class="line"><span class="string">        - class = 目标类别编号(从0开始)</span></span><br><span class="line"><span class="string">        - 没有目标的地方通常用0填充</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回</span></span><br><span class="line"><span class="string">    bbox_offset : torch.Tensor, 形状 (batch_size, num_anchors * 4)</span></span><br><span class="line"><span class="string">        每个锚框相对于匹配的真实框的偏移量(训练回归目标)</span></span><br><span class="line"><span class="string">    bbox_mask : torch.Tensor, 形状 (batch_size, num_anchors * 4)</span></span><br><span class="line"><span class="string">        掩码，标记哪些锚框是真实匹配(1)哪些是背景(0)</span></span><br><span class="line"><span class="string">    class_labels : torch.Tensor, 形状 (batch_size, num_anchors)</span></span><br><span class="line"><span class="string">        每个锚框的类别标签(0 表示背景)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    batch_size, anchors = labels.shape[<span class="number">0</span>], anchors.squeeze(<span class="number">0</span>)</span><br><span class="line">    batch_offset, batch_mask, batch_class_labels = [], [], []</span><br><span class="line">    device, num_anchors = anchors.device, anchors.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 对每张图片分别处理</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">        label = labels[i, :, :]  <span class="comment">#　当前图片的所有标注框 [num_objects, 5]</span></span><br><span class="line">        <span class="comment"># 调用匹配函数：为每个锚框分配最接近的真实框索引</span></span><br><span class="line">        anchors_bbox_map = assign_anchor_to_bbox(</span><br><span class="line">            label[:, <span class="number">1</span>:], anchors, device)</span><br><span class="line">        <span class="comment"># 生成掩码矩阵：若锚框被分配给某个真实框，则 mask=1，否则 mask=0</span></span><br><span class="line">        bbox_mask = ((anchors_bbox_map &gt;= <span class="number">0</span>).<span class="built_in">float</span>().unsqueeze(-<span class="number">1</span>)).repeat(</span><br><span class="line">            <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># 将类标签和分配的边界框坐标初始化为零</span></span><br><span class="line">        class_labels = torch.zeros(num_anchors, dtype=torch.long,</span><br><span class="line">                                   device=device)</span><br><span class="line">        assigned_bb = torch.zeros((num_anchors, <span class="number">4</span>), dtype=torch.float32,</span><br><span class="line">                                  device=device)</span><br><span class="line">        <span class="comment"># 使用真实边界框来标记锚框的类别。</span></span><br><span class="line">        <span class="comment"># 如果一个锚框没有被分配，标记其为背景(值为零)</span></span><br><span class="line">        indices_true = torch.nonzero(anchors_bbox_map &gt;= <span class="number">0</span>)</span><br><span class="line">        bb_idx = anchors_bbox_map[indices_true]</span><br><span class="line">        class_labels[indices_true] = label[bb_idx, <span class="number">0</span>].long() + <span class="number">1</span></span><br><span class="line">        assigned_bb[indices_true] = label[bb_idx, <span class="number">1</span>:]</span><br><span class="line">        <span class="comment"># 偏移量转换</span></span><br><span class="line">        offset = offset_boxes(anchors, assigned_bb) * bbox_mask</span><br><span class="line">        <span class="comment"># 将结果展平后存入 batch 列表</span></span><br><span class="line">        batch_offset.append(offset.reshape(-<span class="number">1</span>))</span><br><span class="line">        batch_mask.append(bbox_mask.reshape(-<span class="number">1</span>))</span><br><span class="line">        batch_class_labels.append(class_labels)</span><br><span class="line">    bbox_offset = torch.stack(batch_offset)</span><br><span class="line">    bbox_mask = torch.stack(batch_mask)</span><br><span class="line">    class_labels = torch.stack(batch_class_labels)</span><br><span class="line">    <span class="keyword">return</span> (bbox_offset, bbox_mask, class_labels)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>通过一个具体的例子来说明锚框标签，已经为加载图像中的狗和猫定义了真实边界框，其中第一个元素是类别(0代表狗，1代表猫)，其余四个元素是左上角和右下角的轴坐标(范围介于0和1之间)</p>
<p>还构建了五个锚框，用左上角和右下角的坐标进行标记$A_0, \ldots, A_4$(索引从0开始)，然后在图像中绘制这些真实边界框和锚框</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ground_truth = torch.tensor([[<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.08</span>, <span class="number">0.52</span>, <span class="number">0.92</span>],</span><br><span class="line">                         [<span class="number">1</span>, <span class="number">0.55</span>, <span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0.88</span>]])</span><br><span class="line">anchors = torch.tensor([[<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>], [<span class="number">0.15</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.4</span>],</span><br><span class="line">                    [<span class="number">0.63</span>, <span class="number">0.05</span>, <span class="number">0.88</span>, <span class="number">0.98</span>], [<span class="number">0.66</span>, <span class="number">0.45</span>, <span class="number">0.8</span>, <span class="number">0.8</span>],</span><br><span class="line">                    [<span class="number">0.57</span>, <span class="number">0.3</span>, <span class="number">0.92</span>, <span class="number">0.9</span>]])</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">'imgs/catdog.jpg'</span>)</span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.imshow(img)</span><br><span class="line">show_bboxes(ax, ground_truth[:, <span class="number">1</span>:] * bbox_scale, [<span class="string">'dog'</span>, <span class="string">'cat'</span>], <span class="string">'k'</span>)</span><br><span class="line">show_bboxes(ax, anchors * bbox_scale, [<span class="string">'0'</span>, <span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>, <span class="string">'4'</span>]);</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511031610.webp" alt="202511031610" style="zoom:67%;">

<p>使用上面定义的<code>multibox_target</code>函数，可以根据狗和猫的真实边界框，标注这些锚框的分类和偏移量</p>
<p>为锚框和真实边界框样本添加一个维度</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">labels = multibox_target(anchors.unsqueeze(dim=<span class="number">0</span>),</span><br><span class="line">                         ground_truth.unsqueeze(dim=<span class="number">0</span>))</span><br><span class="line">labels[<span class="number">2</span>]</span><br></pre></td></tr></tbody></table></figure>

<p>返回的结果中有三个元素，都是张量格式，第三个元素包含标记的输入锚框的类别</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0, 1, 2, 0, 2]])</span><br></pre></td></tr></tbody></table></figure>

<p>返回的第二个元素是掩码(mask)变量，形状为(批量大小，锚框数的四倍)</p>
<p>掩码变量中的元素与每个锚框的4个偏移量一一对应</p>
<p>由于不关心对背景的检测，负类的偏移量不应影响目标函数，通过元素乘法，掩码变量中的零将在计算目标函数之前过滤掉负类偏移量</p>
<h3 id="使用非极大值抑制预测边界框"><a href="#使用非极大值抑制预测边界框" class="headerlink" title="使用非极大值抑制预测边界框"></a>使用非极大值抑制预测边界框</h3><p>在预测时，先为图像生成多个锚框，再为这些锚框一一预测类别和偏移量，一个预测好的边界框则根据其中某个带有预测偏移量的锚框而生成</p>
<p>实现的<code>offset_inverse</code>函数，该函数将锚框和偏移量预测作为输入，并应用逆偏移变换来返回预测的边界框坐标</p>
<p>当有许多锚框时，可能会输出许多相似的具有明显重叠的预测边界框，都围绕着同一目标，为了简化输出，可以使用**非极大值抑制(non-maximum suppression，NMS)**合并属于同一目标的类似的预测边界框</p>
<p>目标检测模型会计算每个类别的预测概率，在同一张图像中，所有预测的非背景边界框都按置信度降序排序，以生成列表$L$</p>
<ol>
<li>选出置信度最高的预测框作为基准</li>
<li>删除所有与该框的 IoU 大于阈值的其他预测框</li>
<li>对剩余框重复以上步骤，直到没有可删除的框为止</li>
<li>输出保留的预测框集合</li>
</ol>
<p>以下<code>nms</code>函数按降序对置信度进行排序并返回其索引</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nms</span>(<span class="params">boxes, scores, iou_threshold</span>):</span><br><span class="line">    <span class="string">"""对预测边界框的置信度进行排序"""</span></span><br><span class="line">    B = torch.argsort(scores, dim=-<span class="number">1</span>, descending=<span class="literal">True</span>)</span><br><span class="line">    keep = []  <span class="comment"># 保留预测边界框的指标</span></span><br><span class="line">    <span class="keyword">while</span> B.numel() &gt; <span class="number">0</span>:</span><br><span class="line">        i = B[<span class="number">0</span>]</span><br><span class="line">        keep.append(i)</span><br><span class="line">        <span class="keyword">if</span> B.numel() == <span class="number">1</span>: <span class="keyword">break</span></span><br><span class="line">        iou = box_iou(boxes[i, :].reshape(-<span class="number">1</span>, <span class="number">4</span>),</span><br><span class="line">                      boxes[B[<span class="number">1</span>:], :].reshape(-<span class="number">1</span>, <span class="number">4</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">        inds = torch.nonzero(iou &lt;= iou_threshold).reshape(-<span class="number">1</span>)</span><br><span class="line">        B = B[inds + <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(keep, device=boxes.device)</span><br></pre></td></tr></tbody></table></figure>

<p>定义以下<code>multibox_detection</code>函数来将非极大值抑制应用于预测边界框</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multibox_detection</span>(<span class="params">cls_probs, offset_preds, anchors, nms_threshold=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">                       pos_threshold=<span class="number">0.009999999</span></span>):</span><br><span class="line">    <span class="string">"""使用非极大值抑制来预测边界框"""</span></span><br><span class="line">    device, batch_size = cls_probs.device, cls_probs.shape[<span class="number">0</span>]</span><br><span class="line">    anchors = anchors.squeeze(<span class="number">0</span>)</span><br><span class="line">    num_classes, num_anchors = cls_probs.shape[<span class="number">1</span>], cls_probs.shape[<span class="number">2</span>]</span><br><span class="line">    out = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        conf, class_id = torch.<span class="built_in">max</span>(cls_prob[<span class="number">1</span>:], <span class="number">0</span>)</span><br><span class="line">        predicted_bb = offset_inverse(anchors, offset_pred)</span><br><span class="line">        keep = nms(predicted_bb, conf, nms_threshold)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 找到所有的non_keep索引，并将类设置为背景</span></span><br><span class="line">        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)</span><br><span class="line">        combined = torch.cat((keep, all_idx))</span><br><span class="line">        uniques, counts = combined.unique(return_counts=<span class="literal">True</span>)</span><br><span class="line">        non_keep = uniques[counts == <span class="number">1</span>]</span><br><span class="line">        all_id_sorted = torch.cat((keep, non_keep))</span><br><span class="line">        class_id[non_keep] = -<span class="number">1</span></span><br><span class="line">        class_id = class_id[all_id_sorted]</span><br><span class="line">        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]</span><br><span class="line">        <span class="comment"># pos_threshold是一个用于非背景预测的阈值</span></span><br><span class="line">        below_min_idx = (conf &lt; pos_threshold)</span><br><span class="line">        class_id[below_min_idx] = -<span class="number">1</span></span><br><span class="line">        conf[below_min_idx] = <span class="number">1</span> - conf[below_min_idx]</span><br><span class="line">        pred_info = torch.cat((class_id.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                               conf.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                               predicted_bb), dim=<span class="number">1</span>)</span><br><span class="line">        out.append(pred_info)</span><br><span class="line">    <span class="keyword">return</span> torch.stack(out)</span><br></pre></td></tr></tbody></table></figure>

<p>现在将上述算法应用到一个带有四个锚框的具体示例中，为简单起见，假设预测的偏移量都是零，这意味着预测的边界框即是锚框。 对于背景、狗和猫其中的每个类，还定义了它的预测概率</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">anchors = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.08</span>, <span class="number">0.52</span>, <span class="number">0.92</span>], [<span class="number">0.08</span>, <span class="number">0.2</span>, <span class="number">0.56</span>, <span class="number">0.95</span>],</span><br><span class="line">                      [<span class="number">0.15</span>, <span class="number">0.3</span>, <span class="number">0.62</span>, <span class="number">0.91</span>], [<span class="number">0.55</span>, <span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0.88</span>]])</span><br><span class="line">offset_preds = torch.tensor([<span class="number">0</span>] * anchors.numel())</span><br><span class="line">cls_probs = torch.tensor([[<span class="number">0</span>] * <span class="number">4</span>,  <span class="comment"># 背景的预测概率</span></span><br><span class="line">                      [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.7</span>, <span class="number">0.1</span>],  <span class="comment"># 狗的预测概率</span></span><br><span class="line">                      [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.9</span>]])  <span class="comment"># 猫的预测概率</span></span><br></pre></td></tr></tbody></table></figure>

<p>在图像上绘制这些预测边界框和置信度</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511031621.webp" alt="202511031621" style="zoom: 67%;">

<p>可以调用<code>multibox_detection</code>函数来执行非极大值抑制，其中阈值设置为0.5</p>
<p>可以看到返回结果的形状是(批量大小，锚框的数量，6)，最内层维度中的六个元素提供了同一预测边界框的输出信息</p>
<p>第一个元素是预测的类索引，从0开始(0代表狗，1代表猫)，值-1表示背景或在非极大值抑制中被移除了，第二个元素是预测的边界框的置信度，其余四个元素是坐标</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">output = multibox_detection(cls_probs.unsqueeze(dim=<span class="number">0</span>),</span><br><span class="line">                            offset_preds.unsqueeze(dim=<span class="number">0</span>),</span><br><span class="line">                            anchors.unsqueeze(dim=<span class="number">0</span>),</span><br><span class="line">                            nms_threshold=<span class="number">0.5</span>)</span><br><span class="line">output</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 0.00,  0.90,  0.10,  0.08,  0.52,  0.92],</span><br><span class="line">         [ 1.00,  0.90,  0.55,  0.20,  0.90,  0.88],</span><br><span class="line">         [-1.00,  0.80,  0.08,  0.20,  0.56,  0.95],</span><br><span class="line">         [-1.00,  0.70,  0.15,  0.30,  0.62,  0.91]]])</span><br></pre></td></tr></tbody></table></figure>

<p>删除-1类别(背景)的预测边界框后，可以输出由非极大值抑制保存的最终预测边界框</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">_, ax = plt.subplots()</span><br><span class="line">ax.imshow(img)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> output[<span class="number">0</span>].detach().numpy():</span><br><span class="line">    <span class="keyword">if</span> i[<span class="number">0</span>] == -<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    label = (<span class="string">'dog='</span>, <span class="string">'cat='</span>)[<span class="built_in">int</span>(i[<span class="number">0</span>])] + <span class="built_in">str</span>(i[<span class="number">1</span>])</span><br><span class="line">    show_bboxes(ax, [torch.tensor(i[<span class="number">2</span>:]) * bbox_scale], label)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511031624.webp" alt="202511031624" style="zoom: 67%;">

<h2 id="区域卷积神经网络-R-CNN"><a href="#区域卷积神经网络-R-CNN" class="headerlink" title="区域卷积神经网络(R-CNN)"></a>区域卷积神经网络(R-CNN)</h2><p>区域卷积神经网络(region-based CNN或regions with CNN features，R-CNN)(Girshick <em>et al.</em>, 2014)也是将深度模型应用于目标检测的开创性工作之一</p>
<p>本节将介绍R-CNN及其一系列改进方法：快速的R-CNN(Fast R-CNN)(Girshick, 2015)、更快的R-CNN(Faster R-CNN) (<a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_references/zreferences.html#id137">Ren <em>et al.</em>, 2015</a>)和掩码R-CNN(Mask R-CNN)(He <em>et al.</em>, 2017)</p>
<h3 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h3><p>R-CNN首先从输入图像中选取若干(例如2000个)提议区域(如锚框也是一种选取方法)，并标注它们的类别和边界框(如偏移量)，用卷积神经网络对每个提议区域进行前向传播以抽取其特征，用每个提议区域的特征来预测类别和边界框</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2025-11-03_17-16-53.webp" alt="Snipaste_2025-11-03_17-16-53" style="zoom: 67%;">

<p>R-CNN包括以下四个步骤：</p>
<ol>
<li>对输入图像使用选择性搜索来选取多个高质量的提议区域(Uijlings <em>et al.</em>, 2013)，这些区域覆盖不同尺度、形状和位置。每个候选区域都会被标注所属类别及对应的真实边界框</li>
<li>选择一个预训练的卷积神经网络，截取到输出层之前的部分，将每个提议区域裁剪并缩放到网络输入尺寸，通过前向传播得到其特征向量</li>
<li>把每个区域的特征与标签作为样本，训练多个支持向量机(SVM)，每个 SVM 负责判断该区域是否属于特定类别</li>
<li>将每个提议区域的特征连同其标注的边界框作为一个样本，训练线性回归模型来预测真实边界框</li>
</ol>
<p>尽管R-CNN模型通过预训练的卷积神经网络有效地抽取了图像特征，但它的速度很慢，可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测，这种庞大的计算量使得R-CNN在现实世界中难以被广泛应用</p>
<h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><p>R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算</p>
<p>由于这些区域通常有重叠，独立的特征抽取会导致重复的计算，Fast R-CNN(Girshick, 2015)对R-CNN的主要改进之一，是仅在整张图象上执行卷积神经网络的前向传播</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2025-11-03_17-23-39.webp" alt="Snipaste_2025-11-03_17-23-39" style="zoom:67%;">

<p>它的主要计算如下：</p>
<ol>
<li>与 R-CNN 逐个裁剪区域再提特征不同，Fast R-CNN直接把整张图像输入卷积神经网络。网络输出一个特征图，形状为$1 \times c \times h_1 \times w_1$，这一步只需一次前向传播即可完成所有区域的特征提取</li>
<li>选择性搜索生成 $n$ 个提议区域(RoI)，每个区域在特征图上被定位后，通过**兴趣区域汇聚层(RoI Pooling)**变换为固定大小(例如$h_2\times w_2$)的特征块，每个 RoI 都会输出统一形状的特征张量$n \times c \times h_2 \times w_2$ </li>
<li>将每个 RoI 的特征展平并输入全连接层，得到$n \times d$的向量表示，其中超参数$d$取决于模型设计</li>
<li>网络分为两个并行的分支，类别预测输出每个 RoI 属于各类别的概率分布，形状为$n \times q$并使用 softmax；边界框预测输出每个 RoI 的边界框偏移量，形状为$n \times 4$</li>
</ol>
<p>在Fast R-CNN中提出的RoI Pooling与之前介绍的汇聚层有所不同，汇聚层的输出形状是通过卷积核大小、填充方式和步幅 间接决定的，而RoI Pooling可以直接指定每个兴趣区域的输出尺寸</p>
<p>对于任何形状为$h \times w$的兴趣区域窗口，该窗口将被划分为$h_2 \times w_2$子窗口网格，其中每个子窗口的大小约为$(h/h_2) \times (w/w_2)$，在实际计算中，这些子窗口的宽高都会向上取整，以确保覆盖完整区域，取其中的最大元素作为该子窗口的输出</p>
<p>这样兴趣区域汇聚层可从形状各异的兴趣区域中均抽取出形状相同的特征</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/roi.jpg" alt="roi">

<p>下面演示兴趣区域汇聚层的计算方法，假设卷积神经网络抽取的特征<code>X</code>的高度和宽度都是4，且只有单通道</p>
<p>假设输入图像的高度和宽度都是40，且选择性搜索在此图像上生成了两个提议区域，每个区域由5个元素表示：区域目标类别、左上角和右下角的坐标</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape(<span class="number">1</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>)  <span class="comment"># 统一数值类型</span></span><br><span class="line">rois = torch.Tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">20</span>, <span class="number">20</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">30</span>]])</span><br></pre></td></tr></tbody></table></figure>

<p>由于<code>X</code>的高和宽是输入图像高和宽的$1/10$，两个提议区域的坐标先按<code>spatial_scale</code>乘以0.1，然后在<code>X</code>上分别标出这两个兴趣区域<code>X[:, :, 0:3, 0:3]  X[:, :, 1:4, 0:4]</code></p>
<p>在2×2的兴趣区域汇聚层中，每个兴趣区域被划分为子窗口网格，并进一步抽取相同形状的特征</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.ops.roi_pool(X, rois, output_size=(<span class="number">2</span>, <span class="number">2</span>), spatial_scale=<span class="number">0.1</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ 5.,  6.],</span><br><span class="line">          [ 9., 10.]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 9., 11.],</span><br><span class="line">          [13., 15.]]]])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><p>为了较精确地检测目标结果，Fast R-CNN模型通常需要在选择性搜索中生成大量的提议区域</p>
<p><em>Faster R-CNN</em> (Ren <em>et al.</em>, 2015)提出将选择性搜索替换为<strong>区域提议网络(region proposal network)</strong>，从而减少提议区域的生成数量，并保证目标检测的精度</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2025-11-04_10-41-34.webp" alt="Snipaste_2025-11-04_10-41-34" style="zoom:80%;">

<p>与Fast R-CNN相比，Faster R-CNN只将生成提议区域的方法从选择性搜索改为了区域提议网络，模型的其余部分保持不变</p>
<p>区域提议网络的计算步骤如下：</p>
<ol>
<li>使用填充为1的3×3卷积层变换卷积神经网络的输出，输出通道为$c$，把每个特征图位置的上下文揉成一条长度为$c$的新特征</li>
<li>以特征图的每个像素为中心，生成多个不同大小和宽高比的锚框并标注它们</li>
<li>使用锚框中心单元长度为$c$的特征，分别预测该锚框的二元类别(含目标还是背景)和边界框</li>
<li>使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果，最终输出的预测边界框即是兴趣区域汇聚层所需的提议区域</li>
</ol>
<p>区域提议网络作为Faster R-CNN模型的一部分，是和整个模型一起训练得到的</p>
<p>Faster R-CNN的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测</p>
<p>作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度</p>
<h3 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h3><p>如果在训练集中还标注了每个目标在图像上的像素级位置，那么Mask R-CNN(He <em>et al.</em>, 2017)能够有效地利用这些详尽的标注信息进一步提升目标检测的精度</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2025-11-04_11-06-34.webp" alt="Snipaste_2025-11-04_11-06-34" style="zoom:80%;">

<p>Mask R-CNN是基于Faster R-CNN修改而来的</p>
<p>Mask R-CNN将兴趣区域汇聚层替换为了<strong>兴趣区域对齐层</strong>，使用**双线性插值(bilinear interpolation)**来保留特征图上的空间信息，从而更适于像素级预测</p>
<p>兴趣区域对齐层的输出包含了所有与兴趣区域的形状相同的特征图，不仅被用于预测每个兴趣区域的类别和边界框，还通过额外的全卷积网络预测目标的像素级位置</p>
<h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><ul>
<li>R-CNN对图像选取若干提议区域，使用卷积神经网络对每个提议区域执行前向传播以抽取其特征，然后再用这些特征来预测提议区域的类别和边界框</li>
<li>Fast R-CNN对R-CNN的一个主要改进：只对整个图像做卷积神经网络的前向传播，它还引入了兴趣区域汇聚层，从而为具有不同形状的兴趣区域抽取相同形状的特征</li>
<li>Faster R-CNN将Fast R-CNN中使用的选择性搜索替换为参与训练的区域提议网络，这样后者可以在减少提议区域数量的情况下仍保证目标检测的精度</li>
<li>Mask R-CNN在Faster R-CNN的基础上引入了一个全卷积网络，从而借助目标的像素级位置进一步提升目标检测的精度</li>
</ul>
<h2 id="语义分割和数据集"><a href="#语义分割和数据集" class="headerlink" title="语义分割和数据集"></a>语义分割和数据集</h2><p>之前一直使用方形边界框来标注和预测图像中的目标，**语义分割(semantic segmentation)**重点关注于如何将图像分割成属于不同语义类别的区域</p>
<p>与目标检测不同，语义分割可以识别并理解图像中每一个像素的内容，标注和预测是像素级的，语义分割标注的像素级的边框显然更加精细</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2025-11-04_11-10-35.webp" alt="Snipaste_2025-11-04_11-10-35" style="zoom:80%;">

<h3 id="图像分割和实例分割"><a href="#图像分割和实例分割" class="headerlink" title="图像分割和实例分割"></a>图像分割和实例分割</h3><p>计算机视觉领域还有2个与语义分割相似的重要问题，即<strong>图像分割(image segmentation)<strong>和</strong>实例分割(instance segmentation)</strong></p>
<ul>
<li>图像分割将图像划分为若干组成区域，这类问题的方法通常利用图像中像素之间的相关性，它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义；图像分割可能会将狗分为两个区域：一个覆盖以黑色为主的嘴和眼睛，另一个覆盖以黄色为主的其余部分身体</li>
<li>实例分割也叫<strong>同时检测并分割(simultaneous detection and segmentation)</strong>，它研究如何识别图像中各个目标实例的像素级区域，与语义分割不同，实例分割不仅需要区分语义，还要区分不同的目标实例；如果图像中有两条狗，则实例分割需要区分像素属于的两条狗中的哪一条</li>
</ul>
<h3 id="Pascal-VOC2012"><a href="#Pascal-VOC2012" class="headerlink" title="Pascal VOC2012"></a>Pascal VOC2012</h3><p>最重要的语义分割数据集之一是<a target="_blank" rel="noopener" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">Pascal VOC2012</a></p>
<p>数据集的tar文件大约为2GB，所以下载可能需要一段时间，提取出的数据集位于<code>../data/VOCdevkit/VOC2012</code></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DATA_HUB[<span class="string">'voc2012'</span>] = (</span><br><span class="line">    DATA_URL + <span class="string">'VOCtrainval_11-May-2012.tar'</span>,</span><br><span class="line">    <span class="string">'4e443f8a2eca6b1dac8a6c57641b67dd40621a49'</span>)</span><br><span class="line">voc_dir = download_extract(<span class="string">'voc2012'</span>, <span class="string">'VOCdevkit/VOC2012'</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">voc_dir = download_extract(<span class="string">'voc2012'</span>, <span class="string">'VOCdevkit/VOC2012'</span>) <span class="comment"># 很慢，没事别重跑</span></span><br></pre></td></tr></tbody></table></figure>

<p>进入路径<code>../data/VOCdevkit/VOC2012</code>之后可以看到数据集的不同组件</p>
<p><code>ImageSets/Segmentation</code>路径包含用于训练和测试样本的文本文件，而<code>JPEGImages</code>和<code>SegmentationClass</code>路径分别存储着每个示例的输入图像和标签</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_voc_images</span>(<span class="params">voc_dir, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">"""读取所有VOC图像并标注"""</span></span><br><span class="line">    txt_fname = os.path.join(voc_dir, <span class="string">'ImageSets'</span>, <span class="string">'Segmentation'</span>,</span><br><span class="line">                             <span class="string">'train.txt'</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">'val.txt'</span>)</span><br><span class="line">    mode = torchvision.io.image.ImageReadMode.RGB <span class="comment"># 按 RGB 模式读取图片</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(txt_fname, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        images = f.read().split() <span class="comment"># 变为字符串列表</span></span><br><span class="line">    <span class="comment"># features 用来存放读取的图像；</span></span><br><span class="line">    <span class="comment"># labels 存放对应的语义分割标签图</span></span><br><span class="line">    features, labels = [], []</span><br><span class="line">    <span class="keyword">for</span> i, fname <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        features.append(torchvision.io.read_image(os.path.join(</span><br><span class="line">            voc_dir, <span class="string">'JPEGImages'</span>, <span class="string">f'<span class="subst">{fname}</span>.jpg'</span>)))</span><br><span class="line">        labels.append(torchvision.io.read_image(os.path.join(</span><br><span class="line">            voc_dir, <span class="string">'SegmentationClass'</span> ,<span class="string">f'<span class="subst">{fname}</span>.png'</span>), mode))</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_features, train_labels = read_voc_images(voc_dir, <span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>绘制前5个输入图像及其标签，<font color="DarkViolet">在标签图像中，白色和黑色分别表示边框和背景，而其他颜色则对应不同的类别</font></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">5</span></span><br><span class="line">imgs = train_features[<span class="number">0</span>:n] + train_labels[<span class="number">0</span>:n]</span><br><span class="line"><span class="comment">#  PyTorch(read_image、ToTensor)得到的图片格式是(C, H, W)</span></span><br><span class="line"><span class="comment"># .permute 把每张图片的通道顺序从 (C, H, W) 改成 (H, W, C)，匹配 Matplotlib / PIL / OpenCV</span></span><br><span class="line">imgs = [img.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>) <span class="keyword">for</span> img <span class="keyword">in</span> imgs]</span><br><span class="line">show_images(imgs, <span class="number">2</span>, n);</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511041413.webp" alt="202511041413" style="zoom:80%;">

<p>列举RGB颜色值和类名</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line">VOC_COLORMAP = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">64</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">64</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">192</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">192</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">128</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line">VOC_CLASSES = [<span class="string">'background'</span>, <span class="string">'aeroplane'</span>, <span class="string">'bicycle'</span>, <span class="string">'bird'</span>, <span class="string">'boat'</span>,</span><br><span class="line">               <span class="string">'bottle'</span>, <span class="string">'bus'</span>, <span class="string">'car'</span>, <span class="string">'cat'</span>, <span class="string">'chair'</span>, <span class="string">'cow'</span>,</span><br><span class="line">               <span class="string">'diningtable'</span>, <span class="string">'dog'</span>, <span class="string">'horse'</span>, <span class="string">'motorbike'</span>, <span class="string">'person'</span>,</span><br><span class="line">               <span class="string">'potted plant'</span>, <span class="string">'sheep'</span>, <span class="string">'sofa'</span>, <span class="string">'train'</span>, <span class="string">'tv/monitor'</span>]</span><br></pre></td></tr></tbody></table></figure>

<p>通过上面定义的两个常量，可以方便地查找标签中每个像素的类索引</p>
<p>定义<code>voc_colormap2label</code>函数来构建从上述RGB颜色值到类别索引的映射，而<code>voc_label_indices</code>函数将RGB值映射到在Pascal VOC2012数据集中的类别索引</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">voc_colormap2label</span>():</span><br><span class="line">    <span class="string">"""构建从RGB到VOC类别索引的映射"""</span></span><br><span class="line">    colormap2label = torch.zeros(<span class="number">256</span> ** <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">    <span class="keyword">for</span> i, colormap <span class="keyword">in</span> <span class="built_in">enumerate</span>(VOC_COLORMAP):</span><br><span class="line">        colormap2label[  <span class="comment"># 为所有可能的 RGB 颜色准备一个查找表</span></span><br><span class="line">            (colormap[<span class="number">0</span>] * <span class="number">256</span> + colormap[<span class="number">1</span>]) * <span class="number">256</span> + colormap[<span class="number">2</span>]] = i</span><br><span class="line">    <span class="keyword">return</span> colormap2label</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">voc_label_indices</span>(<span class="params">colormap, colormap2label</span>):</span><br><span class="line">    <span class="string">"""将VOC标签中的RGB值映射到它们的类别索引"""</span></span><br><span class="line">    colormap = colormap.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).numpy().astype(<span class="string">'int32'</span>)</span><br><span class="line">    idx = ((colormap[:, :, <span class="number">0</span>] * <span class="number">256</span> + colormap[:, :, <span class="number">1</span>]) * <span class="number">256</span> + colormap[:, :, <span class="number">2</span>])</span><br><span class="line">    <span class="keyword">return</span> colormap2label[idx]</span><br></pre></td></tr></tbody></table></figure>

<p>例如，在第一张样本图像中，飞机头部区域的类别索引为1，而背景索引为0</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = voc_label_indices(train_labels[<span class="number">0</span>], voc_colormap2label())</span><br><span class="line">y[<span class="number">105</span>:<span class="number">115</span>, <span class="number">130</span>:<span class="number">140</span>], VOC_CLASSES[<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]),</span><br><span class="line"> 'aeroplane')</span><br></pre></td></tr></tbody></table></figure>

<h4 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h4><p>在之前的实验通过再缩放图像使其符合模型的输入形状，在语义分割中，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像，这样的映射可能不够精确，尤其在不同语义的分割区域</p>
<p>为了避免这个问题，将图像裁剪为固定尺寸，而不是再缩放，使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">voc_rand_crop</span>(<span class="params">feature,label,height, width</span>):</span><br><span class="line">    <span class="string">"""随机裁剪特征和标签图像"""</span></span><br><span class="line">    rect = torchvision.transforms.RandomCrop.get_params( <span class="comment"># 计算出随机裁剪的位置参数</span></span><br><span class="line">        feature, (height, width))</span><br><span class="line">    feature = torchvision.transforms.functional.crop(feature, *rect) <span class="comment"># 解包数据进行裁剪</span></span><br><span class="line">    label = torchvision.transforms.functional.crop(label, *rect) </span><br><span class="line">    <span class="keyword">return</span> feature, label</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">imgs = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    imgs += voc_rand_crop(train_features[<span class="number">0</span>], train_labels[<span class="number">0</span>], <span class="number">200</span>, <span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">imgs = [img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>) <span class="keyword">for</span> img <span class="keyword">in</span> imgs]</span><br><span class="line">show_images(imgs[::<span class="number">2</span>] + imgs[<span class="number">1</span>::<span class="number">2</span>], <span class="number">2</span>, n);</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511041421.webp" alt="202511041421" style="zoom: 80%;">

<h4 id="自定义语义分割数据集类"><a href="#自定义语义分割数据集类" class="headerlink" title="自定义语义分割数据集类"></a>自定义语义分割数据集类</h4><p>通过继承高级API提供的<code>Dataset</code>类，自定义了一个语义分割数据集类<code>VOCSegDataset</code></p>
<p>通过实现<code>__getitem__</code>函数，可以任意访问数据集中索引为<code>idx</code>的输入图像及其每个像素的类别索引</p>
<p>由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本可以通过自定义的<code>filter</code>函数移除掉</p>
<p>还定义了<code>normalize_image</code>函数，从而对输入图像的RGB三个通道的值分别做标准化</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VOCSegDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="string">"""一个用于加载VOC数据集的自定义数据集"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, is_train, crop_size, voc_dir</span>):</span><br><span class="line">        <span class="comment"># 定义标准化数据</span></span><br><span class="line">        <span class="variable language_">self</span>.transform = torchvision.transforms.Normalize(</span><br><span class="line">            mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">        <span class="variable language_">self</span>.crop_size = crop_size</span><br><span class="line">        features, labels = read_voc_images(voc_dir, is_train=is_train)</span><br><span class="line">        <span class="variable language_">self</span>.features = [<span class="variable language_">self</span>.normalize_image(feature)</span><br><span class="line">                         <span class="keyword">for</span> feature <span class="keyword">in</span> <span class="variable language_">self</span>.<span class="built_in">filter</span>(features)]</span><br><span class="line">        <span class="variable language_">self</span>.labels = <span class="variable language_">self</span>.<span class="built_in">filter</span>(labels)</span><br><span class="line">        <span class="variable language_">self</span>.colormap2label = voc_colormap2label()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'read '</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.features)) + <span class="string">' examples'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normalize_image</span>(<span class="params">self, img</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.transform(img.<span class="built_in">float</span>() / <span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">filter</span>(<span class="params">self, imgs</span>): <span class="comment"># 如果图片尺寸小于裁剪尺寸就跳过</span></span><br><span class="line">        <span class="keyword">return</span> [img <span class="keyword">for</span> img <span class="keyword">in</span> imgs <span class="keyword">if</span> (</span><br><span class="line">            img.shape[<span class="number">1</span>] &gt;= <span class="variable language_">self</span>.crop_size[<span class="number">0</span>] <span class="keyword">and</span></span><br><span class="line">            img.shape[<span class="number">2</span>] &gt;= <span class="variable language_">self</span>.crop_size[<span class="number">1</span>])]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        feature, label = voc_rand_crop(<span class="variable language_">self</span>.features[idx], <span class="variable language_">self</span>.labels[idx],</span><br><span class="line">                                       *<span class="variable language_">self</span>.crop_size)</span><br><span class="line">        <span class="keyword">return</span> (feature, voc_label_indices(label, <span class="variable language_">self</span>.colormap2label))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.features)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h4><p>通过自定义的<code>VOCSegDataset</code>类来分别创建训练集和测试集</p>
<p>假设指定随机裁剪的输出图像的形状为320×480，下面可以查看训练集和测试集所保留的样本个数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">crop_size = (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">voc_train = VOCSegDataset(<span class="literal">True</span>, crop_size, voc_dir)</span><br><span class="line">voc_test = VOCSegDataset(<span class="literal">False</span>, crop_size, voc_dir)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">read 1114 examples</span><br><span class="line">read 1078 examples</span><br></pre></td></tr></tbody></table></figure>

<p>设批量大小为64定义训练集的迭代器，打印第一个小批量的形状会发现：与图像分类或目标检测不同，这里的标签是一个三维数组，因为每个 batch 有多张二维的像素分类图</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                    drop_last=<span class="literal">True</span>,</span><br><span class="line">                                    num_workers=<span class="number">0</span>) <span class="comment"># 这里是因为nootbook写成0</span></span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape)</span><br><span class="line">    <span class="built_in">print</span>(Y.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([64, 3, 320, 480])</span><br><span class="line">torch.Size([64, 320, 480])</span><br></pre></td></tr></tbody></table></figure>

<h4 id="整合所有组件"><a href="#整合所有组件" class="headerlink" title="整合所有组件"></a>整合所有组件</h4><p>定义以下<code>load_data_voc</code>函数来下载并读取Pascal VOC2012语义分割数据集，它返回训练集和测试集的数据迭代器</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_voc</span>(<span class="params">batch_size, crop_size</span>):</span><br><span class="line">    <span class="string">"""加载VOC语义分割数据集"""</span></span><br><span class="line">    voc_dir = download_extract(<span class="string">'voc2012'</span>, os.path.join(</span><br><span class="line">        <span class="string">'VOCdevkit'</span>, <span class="string">'VOC2012'</span>))</span><br><span class="line">    num_workers = get_dataloader_workers()</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(</span><br><span class="line">        VOCSegDataset(<span class="literal">True</span>, crop_size, voc_dir), batch_size,</span><br><span class="line">        shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(</span><br><span class="line">        VOCSegDataset(<span class="literal">False</span>, crop_size, voc_dir), batch_size,</span><br><span class="line">        drop_last=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br></pre></td></tr></tbody></table></figure>

<h3 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h3><ul>
<li>语义分割通过将图像划分为属于不同语义类别的区域，来识别并理解图像中像素级别的内容</li>
<li>语义分割的一个重要的数据集叫做Pascal VOC2012</li>
<li>由于语义分割的输入图像和标签在像素上一一对应，输入图像会被随机裁剪为固定尺寸而不是缩放</li>
</ul>
<h2 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h2><p>所见到的卷积神经网络层，例如卷积层和汇聚层通常会减少下采样输入图像的空间维度(高和宽)，然而如果输入和输出图像的空间维度相同，在以像素级分类的语义分割中将会很方便，比如输出像素所处的通道维可以保有输入像素在同一位置上的分类结果</p>
<p>为了实现这一点，尤其是在空间维度被卷积神经网络层缩小后，可以使用另一种类型的卷积神经网络层，它可以增加上采样中间层特征图的空间维度</p>
<p><strong>转置卷积(transposed convolution)</strong> (Dumoulin and Visin, 2016)， 用于逆转下采样导致的空间尺寸减小</p>
<h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><p>设步幅为1且没有填充，假设有一个$n_h \times n_w$的输入张量和一个$k_h \times k_w$的卷积核，以步幅为1滑动卷积核窗口，每行$n_w$次，每列$n_h$次，共产生$n_h n_w$个中间结果，每个中间结果都是一个$(n_h + k_h - 1) \times (n_w + k_w - 1)$的张量，初始化为0</p>
<p>输入张量中的每个元素都要乘以卷积核，从而使所得的$k_h \times k_w$张量替换中间张量的一部分，每个中间张量被替换部分的位置与输入张量中元素的位置相对应。最后，所有中间结果相加以获得最终结果</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/trans_conv.jpg" alt="trans_conv" style="zoom:80%;">

<p>可以对输入矩阵<code>X</code>和卷积核矩阵<code>K</code>实现基本的转置卷积运算<code>trans_conv</code></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trans_conv</span>(<span class="params">X, K</span>):</span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] + h - <span class="number">1</span>, X.shape[<span class="number">1</span>] + w - <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i: i + h, j: j + w] += X[i, j] * K</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></tbody></table></figure>

<p>与通过卷积核“减少”输入元素的常规卷积相比，转置卷积通过卷积核“广播”输入元素，从而产生大于输入的输出</p>
<p>当输入<code>X</code>和卷积核<code>K</code>都是四维张量时，可以使用高级API获得相同的结果</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tconv = nn.ConvTranspose2d(</span><br><span class="line">    in_channels, out_channels, kernel_size,</span><br><span class="line">    stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>

<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><code>in_channels</code></td>
<td>输入通道数(输入特征图的深度)</td>
</tr>
<tr>
<td><code>out_channels</code></td>
<td>输出通道数(输出特征图的深度)</td>
</tr>
<tr>
<td><code>kernel_size</code></td>
<td>卷积核大小(单个数(3)或元组(3,3))</td>
</tr>
<tr>
<td><code>stride</code></td>
<td>步幅(控制上采样扩大倍数)，默认1</td>
</tr>
<tr>
<td><code>padding</code></td>
<td>对输出边缘进行裁剪</td>
</tr>
<tr>
<td><code>bias</code></td>
<td>是否学习偏置项，默认True</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">X, K = X.reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), K.reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">tconv = nn.ConvTranspose2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, bias=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.data = K</span><br><span class="line">tconv(X)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ 0.,  0.,  1.],</span><br><span class="line">          [ 0.,  4.,  6.],</span><br><span class="line">          [ 4., 12.,  9.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="填充、步幅和多通道"><a href="#填充、步幅和多通道" class="headerlink" title="填充、步幅和多通道"></a>填充、步幅和多通道</h3><p>与常规卷积不同，在转置卷积中，填充被应用于输出（常规卷积将填充应用于输入）</p>
<p>当将高和宽两侧的填充数指定为1时，转置卷积的输出中将删除第一和最后的行与列</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tconv = nn.ConvTranspose2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.data = K</span><br><span class="line">tconv(X)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</span><br></pre></td></tr></tbody></table></figure>

<p>在转置卷积中，步幅被指定为中间结果（输出），而不是输入，使用相同输入和卷积核张量，将步幅从1更改为2会增加中间张量的高和权重，因此输出张量为：</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/trans_conv_stride2.jpg" alt="trans_conv_stride2" style="zoom:80%;">

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tconv = nn.ConvTranspose2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, bias=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.data = K</span><br><span class="line">tconv(X)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[0., 0., 0., 1.],</span><br><span class="line">          [0., 0., 2., 3.],</span><br><span class="line">          [0., 2., 0., 3.],</span><br><span class="line">          [4., 6., 6., 9.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</span><br></pre></td></tr></tbody></table></figure>

<p>对于多个输入和输出通道，转置卷积与常规卷积以相同方式运作</p>
<p>假设输入有$c_i$个通道，且转置卷积为每个输入通道分配了一个$k_h\times k_w$的卷积核张量，当指定多个输出通道时，每个输出通道将有一个$c_i\times k_h\times k_w$的卷积核</p>
<p>卷积核参数的整体形状是<code>(out_channels, in_channels, kh, kw)</code></p>
<p>卷积和转置卷积之间存在反向关系，如果参数（kernel、stride、padding）设得对称，那么转置卷积的输出形状就能还原成卷积的输入形状</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">10</span>, <span class="number">16</span>, <span class="number">16</span>))  <span class="comment"># 输入特征图</span></span><br><span class="line">conv = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>, stride=<span class="number">3</span>)</span><br><span class="line">tconv = nn.ConvTranspose2d(<span class="number">20</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>, stride=<span class="number">3</span>)</span><br><span class="line">tconv(conv(X)).shape == X.shape <span class="comment"># 输出True</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="与矩阵变换的联系"><a href="#与矩阵变换的联系" class="headerlink" title="与矩阵变换的联系"></a>与矩阵变换的联系</h3><p>卷积其实就是一种矩阵乘法，假设输入矩阵为 <code>X = torch.arange(9).reshape(3,3)</code></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0 1 2</span><br><span class="line">3 4 5</span><br><span class="line">6 7 8</span><br></pre></td></tr></tbody></table></figure>

<p>卷积核为 <code>K = torch.tensor([[1.0, 2.0], [3.0, 4.0]])</code></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 2</span><br><span class="line">3 4</span><br></pre></td></tr></tbody></table></figure>

<p>普通卷积相当于在X上滑动这个小窗口，不断做“乘加”运算，可以把这个滑动过程，展开成矩阵乘法</p>
<p>构造一个稀疏矩阵 W，它包含了卷积核的权重</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W =</span><br><span class="line">[1 2 0 3 4 0 0 0 0]</span><br><span class="line">[0 1 2 0 3 4 0 0 0]</span><br><span class="line">[0 0 0 1 2 0 3 4 0]</span><br><span class="line">[0 0 0 0 1 2 0 3 4]</span><br></pre></td></tr></tbody></table></figure>

<p>W 的每一行代表卷积核 K 在图像上滑动到的一个位置</p>
<p><code>torch.matmul(W, X)</code> 就能得到卷积的输出（展平形式），<code>reshape</code>就得到了卷积的输出矩阵，所以在数学上反卷积（或者说转置卷积）自然就是把这个乘法反过来</p>
<p>也就是用转置矩阵来做乘法<br>$$<br>X = W^TY<br>$$<br>所以转置卷积不仅是名字上的“转置”，也是在神经网络梯度传播中真正出现的数学转置</p>
<h2 id="全卷积网络"><a href="#全卷积网络" class="headerlink" title="全卷积网络"></a>全卷积网络</h2><p>语义分割是对图像中的每个像素分类，**全卷积网络（fully convolutional network，FCN）**采用卷积神经网络实现了从图像像素到像素类别的变换(Long <em>et al.</em>, 2015)，与之前不一样的是全卷积网络通过转置卷积，将中间层特征图的高和宽变换回输入图像的尺寸</p>
<h3 id="构造模型"><a href="#构造模型" class="headerlink" title="构造模型"></a>构造模型</h3><p>全卷积网络先使用卷积神经网络抽取图像特征，然后通过1×1卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸，因此模型输出与输入图像的高和宽相同，且最终输出通道包含了该空间位置像素的类别预测</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2025-11-04_15-17-33.webp" alt="Snipaste_2025-11-04_15-17-33" style="zoom:80%;">

<p>使用在ImageNet数据集上预训练的ResNet-18模型来提取图像特征，并将该网络记为<code>pretrained_net</code></p>
<p>ResNet-18模型的最后几层包括全局平均汇聚层和全连接层，然而全卷积网络中不需要它们</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>创建一个全卷积网络<code>net</code>，它复制了ResNet-18中大部分的预训练层，除了最后的全局平均汇聚层和最接近输出的全连接层</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(*<span class="built_in">list</span>(pretrained_net.children())[:-<span class="number">2</span>])</span><br></pre></td></tr></tbody></table></figure>

<p>给定高度为320和宽度为480的输入，<code>net</code>的前向传播将输入的高和宽减小至原来的1/32，即10和15</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">320</span>, <span class="number">480</span>))</span><br><span class="line">net(X).shape</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([1, 512, 10, 15])</span><br></pre></td></tr></tbody></table></figure>

<p>接下来使用1×1卷积层将输出通道数转换为Pascal VOC2012数据集的类数（21类），最后需要将特征图的高度和宽度增加32倍，从而将其变回输入图像的高和宽</p>
<p>根据公式<br>$$<br>\color{purple}H = \lfloor \frac{n_h-k_h+p_h}{s_h}+1\rfloor \qquad W=\lfloor\frac{n_w-k_w+p_w}{s_w}+1\rfloor.<br>$$<br>一般倍数通过stride控制，所以步幅设为32，主要考虑卷积核和填充大小，根据经验将卷积核的高和宽设为64，填充为16</p>
<p>经验公式：步幅为$s$，填充为$s/2$（如果整除），卷积核的高和宽为$2s$，这样转置卷积核使得输入放大$s$倍</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num_classes = <span class="number">21</span></span><br><span class="line">net.add_module(<span class="string">'final_conv'</span>, nn.Conv2d(<span class="number">512</span>, num_classes, kernel_size=<span class="number">1</span>))</span><br><span class="line">net.add_module(<span class="string">'transpose_conv'</span>, nn.ConvTranspose2d(num_classes, num_classes,</span><br><span class="line">                                    kernel_size=<span class="number">64</span>, padding=<span class="number">16</span>, stride=<span class="number">32</span>))</span><br></pre></td></tr></tbody></table></figure>

<h3 id="初始化转置卷积层"><a href="#初始化转置卷积层" class="headerlink" title="初始化转置卷积层"></a>初始化转置卷积层</h3><p>图像放大通过<strong>上采样（upsampling）</strong>，**双线性插值（bilinear interpolation）**是常用的上采样方法之一，它也经常用于初始化转置卷积层</p>
<p>假设给定输入图像，想要计算上采样输出图像上的每个像素</p>
<ol>
<li>将输出图像的坐标$(x,y)$映射到输入图像的坐标$(x’,y’)$，根据输入与输出的尺寸之比来映射</li>
<li>在输入图像上找到离坐标$(x’,y’)$最近的4个像素</li>
<li>输出图像在坐标$(x,y)$上的像素依据输入图像上这4个像素及其与$(x’,y’)$的相对距离来计算</li>
</ol>
<p>双线性插值的上采样可以通过转置卷积层实现，内核由以下<code>bilinear_kernel</code>函数构造</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bilinear_kernel</span>(<span class="params">in_channels, out_channels, kernel_size</span>):</span><br><span class="line">    <span class="string">"""生成一个卷积核的权重张量，值是双线性插值滤波器的系数"""</span></span><br><span class="line">    factor = (kernel_size + <span class="number">1</span>) // <span class="number">2</span> <span class="comment"># 计算卷积核的中心点</span></span><br><span class="line">    <span class="keyword">if</span> kernel_size % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">        center = factor - <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        center = factor - <span class="number">0.5</span> <span class="comment"># 核大小是偶数，中心在两个中间值之间</span></span><br><span class="line">    <span class="comment"># 表示卷积核中每个位置的坐标</span></span><br><span class="line">    og = (torch.arange(kernel_size).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">          torch.arange(kernel_size).reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 计算滤波器的值</span></span><br><span class="line">    <span class="comment"># 让权重在距离中心越远时越小，形成一个平滑的“金字塔”分布</span></span><br><span class="line">    filt = (<span class="number">1</span> - torch.<span class="built_in">abs</span>(og[<span class="number">0</span>] - center) / factor) * \</span><br><span class="line">           (<span class="number">1</span> - torch.<span class="built_in">abs</span>(og[<span class="number">1</span>] - center) / factor)</span><br><span class="line">    <span class="comment"># 扩展到多个通道</span></span><br><span class="line">    weight = torch.zeros((in_channels, out_channels,</span><br><span class="line">                          kernel_size, kernel_size))</span><br><span class="line">    weight[<span class="built_in">range</span>(in_channels), <span class="built_in">range</span>(out_channels), :, :] = filt</span><br><span class="line">    <span class="keyword">return</span> weight</span><br></pre></td></tr></tbody></table></figure>

<p>构造一个将输入的高和宽放大2倍的转置卷积层，并将其卷积核用<code>bilinear_kernel</code>函数初始化</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conv_trans = nn.ConvTranspose2d(<span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="number">4</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>,</span><br><span class="line">                                bias=<span class="literal">False</span>)</span><br><span class="line">conv_trans.weight.data.copy_(bilinear_kernel(<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>));</span><br></pre></td></tr></tbody></table></figure>

<p>读取图像<code>X</code>，将上采样的结果记作<code>Y</code>，为了打印图像，需要调整通道维的位置</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img = torchvision.transforms.ToTensor()(Image.<span class="built_in">open</span>(<span class="string">'imgs/catdog.jpg'</span>))</span><br><span class="line">X = img.unsqueeze(<span class="number">0</span>)</span><br><span class="line">Y = conv_trans(X)</span><br><span class="line">out_img = Y[<span class="number">0</span>].permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).detach()</span><br></pre></td></tr></tbody></table></figure>

<p>可以看到，转置卷积层将图像的高和宽分别放大了2倍，除了坐标刻度不同，双线性插值放大的图像和原图看上去没什么两样</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'input image shape:'</span>, img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'output image shape:'</span>, out_img.shape)</span><br><span class="line">plt.imshow(out_img);</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input image shape: torch.Size([561, 728, 3])</span><br><span class="line">output image shape: torch.Size([1122, 1456, 3])</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511041542.webp" alt="202511041542" style="zoom:67%;">

<p>全卷积网络用双线性插值的上采样初始化转置卷积层(稳定、快速收敛)，对于1×1卷积层使用Xavier初始化参数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = bilinear_kernel(num_classes, num_classes, <span class="number">64</span>)</span><br><span class="line">net.transpose_conv.weight.data.copy_(W);</span><br></pre></td></tr></tbody></table></figure>

<h3 id="读取数据集-1"><a href="#读取数据集-1" class="headerlink" title="读取数据集"></a>读取数据集</h3><p>用Pascal VOC2012语义分割读取数据集，指定随机裁剪的输出图像的形状为$320\times 480$，高宽都可被32整除</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size, crop_size = <span class="number">32</span>, (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">train_iter, test_iter = load_data_voc(batch_size, crop_size)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">read 1114 examples</span><br><span class="line">read 1078 examples</span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><p>可以训练全卷积网络了，这里的损失函数和准确率计算与图像分类中的并没有本质上的不同，因为使用转置卷积层的通道来预测像素的类别，所以需要在损失计算中指定通道维，此外，模型基于每个像素的预测类别是否正确来计算准确率</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">inputs, targets</span>):</span><br><span class="line">    <span class="comment"># 两次mean(1)，第一次对高度方向求平均，第二次对宽度方向求平均</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    	等价写法：</span></span><br><span class="line"><span class="string">    	per_pixel_loss = F.cross_entropy(inputs, targets, reduction='none') # 每像素累计</span></span><br><span class="line"><span class="string">    	per_image_loss = per_pixel_loss.mean(dim=(1, 2)) # 每图片平均</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> F.cross_entropy(inputs, targets, reduction=<span class="string">'none'</span>).mean(<span class="number">1</span>).mean(<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr, wd, devices = <span class="number">5</span>, <span class="number">0.001</span>, <span class="number">1e-3</span>, try_all_gpus()</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)</span><br><span class="line">train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 1.128, acc 0.745, test acc 0.817, time 27.11 sec</span><br><span class="line">epoch 2, loss 0.631, acc 0.825, test acc 0.834, time 20.57 sec</span><br><span class="line">epoch 3, loss 0.522, acc 0.846, test acc 0.840, time 20.31 sec</span><br><span class="line">epoch 4, loss 0.477, acc 0.853, test acc 0.850, time 21.17 sec</span><br><span class="line">epoch 5, loss 0.413, acc 0.871, test acc 0.854, time 20.96 sec</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511051139.png" alt="202511051139" style="zoom:80%;">

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.413, train acc 0.871, test acc 0.854</span><br><span class="line">82.4 examples/sec on [device(type='cuda', index=0)]</span><br></pre></td></tr></tbody></table></figure>

<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>在预测时，需要将输入图像在各个通道做标准化，并转成卷积神经网络所需要的四维输入格式</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">img</span>):</span><br><span class="line">    X = test_iter.dataset.normalize_image(img).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    pred = net(X.to(devices[<span class="number">0</span>])).argmax(dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> pred.reshape(pred.shape[<span class="number">1</span>], pred.shape[<span class="number">2</span>])</span><br></pre></td></tr></tbody></table></figure>

<p>为了可视化预测的类别给每个像素，将预测类别映射回它们在数据集中的标注颜色</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">label2image</span>(<span class="params">pred</span>):</span><br><span class="line">    colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[<span class="number">0</span>])</span><br><span class="line">    X = pred.long()</span><br><span class="line">    <span class="keyword">return</span> colormap[X, :]</span><br></pre></td></tr></tbody></table></figure>

<p>测试数据集中的图像大小和形状各异，由于模型使用了步幅为32的转置卷积层，因此当输入图像的高或宽无法被32整除时，转置卷积层输出的高或宽会与输入图像的尺寸有偏差</p>
<p>为了解决这个问题，可以在图像中截取多块高和宽为32的整数倍的矩形区域，并分别对这些区域中的像素做前向传播</p>
<p>这些区域的并集需要完整覆盖输入图像，当一个像素被多个区域所覆盖时，它在不同区域前向传播中转置卷积层输出的平均值可以作为<code>softmax</code>运算的输入，从而预测类别</p>
<p>为简单起见，只读取几张较大的测试图像，并从图像的左上角开始截取形状为320×480的区域用于预测</p>
<p>对于这些测试图像，逐一打印它们截取的区域，再打印预测结果，最后打印标注的类别</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">voc_dir = download_extract(<span class="string">'voc2012'</span>, <span class="string">'VOCdevkit/VOC2012'</span>)</span><br><span class="line">test_images, test_labels = read_voc_images(voc_dir, <span class="literal">False</span>) <span class="comment"># 读取数据集中的验证集</span></span><br><span class="line">n, imgs = <span class="number">4</span>, []  <span class="comment"># 只取前 n=4 张图片进行可视化</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    crop_rect = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">    X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)</span><br><span class="line">    pred = label2image(predict(X))  <span class="comment"># 用模型预测分割结果，把类别编号转成可视化的 RGB 颜色</span></span><br><span class="line">    <span class="comment"># X.permute(1,2,0) 原图</span></span><br><span class="line">    <span class="comment"># pred.cpu() 预测的彩色分割结果</span></span><br><span class="line">    <span class="comment"># test_labels[i] 裁剪并.permute(1,2,0)：真实的标签彩色图</span></span><br><span class="line">    imgs += [X.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>), pred.cpu(),</span><br><span class="line">             torchvision.transforms.functional.crop(</span><br><span class="line">                 test_labels[i], *crop_rect).permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)]</span><br><span class="line"><span class="comment"># 从0开始每3个取一张(原图) + 从1开始每3个取一张(预测图) + 从2开始每3个取一张(标签图)</span></span><br><span class="line">show_images(imgs[::<span class="number">3</span>] + imgs[<span class="number">1</span>::<span class="number">3</span>] + imgs[<span class="number">2</span>::<span class="number">3</span>], <span class="number">3</span>, n, scale=<span class="number">2</span>);</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511051151.webp" alt="202511051151" style="zoom:80%;">

<h2 id="风格迁移"><a href="#风格迁移" class="headerlink" title="风格迁移"></a>风格迁移</h2><p>使用卷积神经网络，自动将一个图像中的风格应用在另一图像之上，即<strong>风格迁移（style transfer）</strong></p>
<p>需要两张输入图像：一张是内容图像，另一张是风格图像，将使用神经网络修改内容图像，使其在风格上接近风格图像</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2025-11-05_13-18-56.webp" alt="Snipaste_2025-11-05_13-18-56" style="zoom:80%;">

<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>首先，初始化合成图像（通常用内容图像当起点），这张图就是唯一需要更新的变量</p>
<p>然后，选择一个已经训练好的卷积神经网络（比如 VGG），模型参数在训练中无须更新，它只是被拿来当作特征提取器，可以选择其中某些层的输出作为内容特征或风格特征</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2025-11-05_13-20-56.webp" alt="Snipaste_2025-11-05_13-20-56" style="zoom:80%;">

<p>图中选取的预训练的神经网络含有3个卷积层，其中第二层输出内容特征，第一层和第三层输出风格特征</p>
<p>通过前向传播（实线箭头方向）计算风格迁移的损失函数，并通过反向传播（虚线箭头方向）迭代模型参数，即不断更新合成图像</p>
<p>风格迁移常用的损失函数由3部分组成：</p>
<ol>
<li>内容损失使合成图像与内容图像在内容特征上接近</li>
<li>风格损失使合成图像与风格图像在风格特征上接近</li>
<li>全变分损失则有助于减少合成图像中的噪点</li>
</ol>
<p>当模型训练结束时，输出风格迁移的模型参数，即得到最终的合成图像</p>
<p>读取内容和风格图像，从打印出的图像坐标轴可以看出，它们的尺寸并不一样</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511051339.webp" alt="202511051339" style="zoom:80%;">

<h3 id="预处理和后处理"><a href="#预处理和后处理" class="headerlink" title="预处理和后处理"></a>预处理和后处理</h3><p>定义图像的预处理函数和后处理函数，预处理函数<code>preprocess</code>对输入图像在RGB三个通道分别做标准化，并将结果变换成卷积神经网络接受的输入格式，后处理函数<code>postprocess</code>则将输出图像中的像素值还原回标准化之前的值</p>
<p>由于图像打印函数要求每个像素的浮点数值在0～1之间，对小于0和大于1的值分别取0和1</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">rgb_mean = torch.tensor([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">rgb_std = torch.tensor([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">img, image_shape</span>):</span><br><span class="line">    transforms = torchvision.transforms.Compose([</span><br><span class="line">        torchvision.transforms.Resize(image_shape),</span><br><span class="line">        torchvision.transforms.ToTensor(),</span><br><span class="line">        <span class="comment"># (img - mean) / std</span></span><br><span class="line">        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])</span><br><span class="line">    <span class="keyword">return</span> transforms(img).unsqueeze(<span class="number">0</span>)  <span class="comment"># 在最前面添加一个“batch维度”，变成网络输入形状</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">postprocess</span>(<span class="params">img</span>):</span><br><span class="line">    img = img[<span class="number">0</span>].to(rgb_std.device) <span class="comment"># 去掉 batch 维度，并且保证张量在同一设备上</span></span><br><span class="line">    <span class="comment"># clamp把所有像素值限制在 [0, 1] 范围内，防止越界</span></span><br><span class="line">    <span class="comment"># 反标准化操作 img * std + mean</span></span><br><span class="line">    img = torch.clamp(img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>) * rgb_std + rgb_mean, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> torchvision.transforms.ToPILImage()(img.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br></pre></td></tr></tbody></table></figure>

<h3 id="抽取图像特征"><a href="#抽取图像特征" class="headerlink" title="抽取图像特征"></a>抽取图像特征</h3><p>使用基于ImageNet数据集预训练的VGG-19模型来抽取图像特征</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net = torchvision.models.vgg19(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (3): ReLU(inplace=True)</span><br><span class="line">    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (6): ReLU(inplace=True)</span><br><span class="line">    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (8): ReLU(inplace=True)</span><br><span class="line">    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (11): ReLU(inplace=True)</span><br><span class="line">    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (13): ReLU(inplace=True)</span><br><span class="line">    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (15): ReLU(inplace=True)</span><br><span class="line">    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (17): ReLU(inplace=True)</span><br><span class="line">    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (20): ReLU(inplace=True)</span><br><span class="line">    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (22): ReLU(inplace=True)</span><br><span class="line">    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (24): ReLU(inplace=True)</span><br><span class="line">    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (26): ReLU(inplace=True)</span><br><span class="line">    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (29): ReLU(inplace=True)</span><br><span class="line">    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (31): ReLU(inplace=True)</span><br><span class="line">    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (33): ReLU(inplace=True)</span><br><span class="line">    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (35): ReLU(inplace=True)</span><br><span class="line">    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (0): Linear(in_features=25088, out_features=4096, bias=True)</span><br><span class="line">    (1): ReLU(inplace=True)</span><br><span class="line">    (2): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (3): Linear(in_features=4096, out_features=4096, bias=True)</span><br><span class="line">    (4): ReLU(inplace=True)</span><br><span class="line">    (5): Dropout(p=0.5, inplace=False)</span><br><span class="line">    (6): Linear(in_features=4096, out_features=1000, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>

<p>为了抽取图像的内容特征和风格特征，可以选择VGG网络中某些层的输出，<font color="DarkViolet">一般来说，越靠近输入层，越容易抽取图像的细节信息，反之，则越容易抽取图像的全局信息</font></p>
<p>为了避免合成图像过多保留内容图像的细节，选择VGG较靠近输出的层，即内容层，来输出图像的内容特征</p>
<p>还从VGG中选择不同层的输出来匹配局部和全局的风格，这些图层也称为风格层</p>
<p>VGG网络使用了5个卷积块，选择第四卷积块的最后一个卷积层作为内容层，选择每个卷积块的第一个卷积层作为风格层</p>
<p>这些层的索引可以通过打印<code>pretrained_net</code>实例获取</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">style_layers, content_layers = [<span class="number">0</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">19</span>, <span class="number">28</span>], [<span class="number">25</span>]</span><br></pre></td></tr></tbody></table></figure>

<p>使用VGG层抽取特征时，只需要用到从输入层到最靠近输出层的内容层或风格层之间的所有层</p>
<p>下面构建一个新的网络<code>net</code>，它只保留需要用到的VGG的所有层</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(*[pretrained_net.features[i] <span class="keyword">for</span> i <span class="keyword">in</span></span><br><span class="line">                      <span class="built_in">range</span>(<span class="built_in">max</span>(content_layers + style_layers) + <span class="number">1</span>)])</span><br></pre></td></tr></tbody></table></figure>

<p>给定输入<code>X</code>，如果简单地调用前向传播<code>net(X)</code>，只能获得最后一层的输出，由于还需要中间层的输出，因此这里逐层计算，并保留内容层和风格层的输出</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_features</span>(<span class="params">X, content_layers, style_layers</span>):</span><br><span class="line">    contents = []</span><br><span class="line">    styles = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(net)):</span><br><span class="line">        X = net[i](X)</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> style_layers:</span><br><span class="line">            styles.append(X)</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> content_layers:</span><br><span class="line">            contents.append(X)</span><br><span class="line">    <span class="keyword">return</span> contents, styles</span><br></pre></td></tr></tbody></table></figure>

<p>定义两个函数：<code>get_contents</code>函数对内容图像抽取内容特征； <code>get_styles</code>函数对风格图像抽取风格特征</p>
<p>因为在训练时无须改变预训练的VGG的模型参数，所以可以在训练开始之前就提取出内容特征和风格特征</p>
<p>由于合成图像是风格迁移所需迭代的模型参数，只能在训练过程中通过调用<code>extract_features</code>函数来抽取合成图像的内容特征和风格特征</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_contents</span>(<span class="params">image_shape, device</span>):</span><br><span class="line">    content_X = preprocess(content_img, image_shape).to(device)</span><br><span class="line">    contents_Y, _ = extract_features(content_X, content_layers, style_layers)</span><br><span class="line">    <span class="keyword">return</span> content_X, contents_Y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_styles</span>(<span class="params">image_shape, device</span>):</span><br><span class="line">    style_X = preprocess(style_img, image_shape).to(device)</span><br><span class="line">    _, styles_Y = extract_features(style_X, content_layers, style_layers)</span><br><span class="line">    <span class="keyword">return</span> style_X, styles_Y</span><br></pre></td></tr></tbody></table></figure>

<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>风格迁移的损失函数，它由内容损失、风格损失和全变分损失3部分组成</p>
<h4 id="内容损失"><a href="#内容损失" class="headerlink" title="内容损失"></a>内容损失</h4><p>与线性回归中的损失函数类似，内容损失通过平方误差函数衡量合成图像与内容图像在内容特征上的差异</p>
<p>平方误差函数的两个输入均为<code>extract_features</code>函数计算所得到的内容层的输出</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">content_loss</span>(<span class="params">Y_hat, Y</span>):</span><br><span class="line">    <span class="comment"># 我们从动态计算梯度的树中分离目标：</span></span><br><span class="line">    <span class="comment"># 这是一个规定的值，而不是一个变量</span></span><br><span class="line">    <span class="keyword">return</span> torch.square(Y_hat - Y.detach()).mean() <span class="comment"># .detch()切断梯度的传递</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="风格损失"><a href="#风格损失" class="headerlink" title="风格损失"></a>风格损失</h4><p>风格损失与内容损失类似，也通过平方误差函数衡量合成图像与风格图像在风格上的差异</p>
<p>为了表达风格层输出的风格，先通过<code>extract_features</code>函数计算风格层的输出</p>
<p>假设该输出的样本数为1，通道数为$c$，高和宽分别为$h$和$w$，可以将此输出转换为矩阵$\mathbf X$，其有$c$行和$hw$列，这个矩阵可以被看作由$c$个长度为$hw$的向量$\mathbf x_1, \ldots, \mathbf x_c$，组合而成的，每个向量代表不同通道上的风格特征</p>
<p>这些向量的<strong>格拉姆矩阵</strong>$\mathbf X\mathbf X^\top \in \mathbb{R}^{c \times c}$，$i$行$j$列的元素$x_{ij}$即向量$x_i$和$x_j$的内积，表达了通道$i$和$j$上风格特征的相关性，用这样的格拉姆矩阵来表达风格层输出的风格</p>
<p>当$hw$的值较大时，格拉姆矩阵中的元素容易出现较大的值，此外格拉姆矩阵的高和宽皆为通道数$c$</p>
<p>为了让风格损失不受这些值的大小影响，下面定义的<code>gram</code>函数将格拉姆矩阵除以了矩阵中元素的个数$chw$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gram</span>(<span class="params">X</span>):</span><br><span class="line">    num_channels, n = X.shape[<span class="number">1</span>], X.numel() // X.shape[<span class="number">1</span>]</span><br><span class="line">    X = X.reshape((num_channels, n))</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, X.T) / (num_channels * n)</span><br></pre></td></tr></tbody></table></figure>

<p>自然地，风格损失的平方误差函数的两个格拉姆矩阵输入分别基于合成图像与风格图像的风格层输出</p>
<p>这里假设基于风格图像的格拉姆矩阵<code>gram_Y</code>已经预先计算好了</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">style_loss</span>(<span class="params">Y_hat, gram_Y</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.square(gram(Y_hat) - gram_Y.detach()).mean()</span><br></pre></td></tr></tbody></table></figure>

<h4 id="全变分损失"><a href="#全变分损失" class="headerlink" title="全变分损失"></a>全变分损失</h4><p>合成图像里面有大量高频噪点，即有特别亮或者特别暗的颗粒像素，一种常见的去噪方法是<strong>全变分去噪（total variation denoising）</strong></p>
<p>假设$x_{i, j}$表示坐标$(i, j)$处的像素值，降低全变分损失<br>$$<br>\sum_{i, j} \left|x_{i, j} - x_{i+1, j}\right| + \left|x_{i, j} - x_{i, j+1}\right|<br>$$<br>能够尽可能使邻近的像素值相似</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tv_loss</span>(<span class="params">Y_hat</span>):</span><br><span class="line">    <span class="comment"># 0.5综合两个方向的平滑程度</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * (torch.<span class="built_in">abs</span>(Y_hat[:, :, <span class="number">1</span>:, :] - Y_hat[:, :, :-<span class="number">1</span>, :]).mean() + <span class="comment"># 与正上方像素之间的差</span></span><br><span class="line">                  torch.<span class="built_in">abs</span>(Y_hat[:, :, :, <span class="number">1</span>:] - Y_hat[:, :, :, :-<span class="number">1</span>]).mean())  <span class="comment"># 与左侧像素之间的差</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>风格转移的损失函数是内容损失、风格损失和总变化损失的加权和，通过调节这些权重超参数，可以权衡合成图像在保留内容、迁移风格以及去噪三方面的相对重要性</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">content_weight, style_weight, tv_weight = <span class="number">1</span>, <span class="number">1e3</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_loss</span>(<span class="params">X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram</span>):</span><br><span class="line">    <span class="comment"># 分别计算内容损失、风格损失和全变分损失</span></span><br><span class="line">    contents_l = [content_loss(Y_hat, Y) * content_weight <span class="keyword">for</span> Y_hat, Y <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">        contents_Y_hat, contents_Y)]</span><br><span class="line">    styles_l = [style_loss(Y_hat, Y) * style_weight <span class="keyword">for</span> Y_hat, Y <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">        styles_Y_hat, styles_Y_gram)]</span><br><span class="line">    tv_l = tv_loss(X) * tv_weight</span><br><span class="line">    <span class="comment"># 对所有损失求和</span></span><br><span class="line">    l = <span class="built_in">sum</span>(<span class="number">10</span> * styles_l + contents_l + [tv_l])  <span class="comment"># 风格权重更大</span></span><br><span class="line">    <span class="keyword">return</span> contents_l, styles_l, tv_l, l</span><br></pre></td></tr></tbody></table></figure>

<h3 id="初始化合成图像"><a href="#初始化合成图像" class="headerlink" title="初始化合成图像"></a>初始化合成图像</h3><p>可以定义一个简单的模型<code>SynthesizedImage</code>，并将合成的图像视为模型参数，模型的前向传播只需返回模型参数即可</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SynthesizedImage</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_shape, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.rand(*img_shape))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.weight</span><br></pre></td></tr></tbody></table></figure>

<p>定义<code>get_inits</code>函数，该函数创建了合成图像的模型实例，并将其初始化为图像<code>X</code>，风格图像在各个风格层的格拉姆矩阵<code>styles_Y_gram</code>将在训练前预先计算好</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_inits</span>(<span class="params">X, device, lr, styles_Y</span>):</span><br><span class="line">    gen_img = SynthesizedImage(X.shape).to(device)</span><br><span class="line">    <span class="comment"># 把合成图像的初始值设为内容图像的像素</span></span><br><span class="line">    gen_img.weight.data.copy_(X.data) </span><br><span class="line">    <span class="comment"># 不训练网络参数，而是把图像本身当成参数去训练</span></span><br><span class="line">    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)</span><br><span class="line">    <span class="comment"># 计算风格图像的 Gram 矩阵</span></span><br><span class="line">    styles_Y_gram = [gram(Y) <span class="keyword">for</span> Y <span class="keyword">in</span> styles_Y]</span><br><span class="line">    <span class="keyword">return</span> gen_img(), styles_Y_gram, trainer</span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>在训练模型进行风格迁移时，不断抽取合成图像的内容特征和风格特征，然后计算损失函数，下面定义了训练循环</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch</span>):</span><br><span class="line">    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)</span><br><span class="line">    <span class="comment"># 学习率调度器，经过lr_decay_epoch 轮，学习率乘上 0.8，用于让训练后期更平稳</span></span><br><span class="line">    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, <span class="number">0.8</span>)</span><br><span class="line">    animator = Animator(xlabel=<span class="string">'epoch'</span>, ylabel=<span class="string">'loss'</span>,</span><br><span class="line">                            xlim=[<span class="number">10</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">'content'</span>, <span class="string">'style'</span>, <span class="string">'TV'</span>],</span><br><span class="line">                            ncols=<span class="number">2</span>, figsize=(<span class="number">7</span>, <span class="number">2.5</span>))</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="comment"># 清除上一次迭代残留的梯度</span></span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        <span class="comment"># 提取当前合成图像的内容和风格特征</span></span><br><span class="line">        contents_Y_hat, styles_Y_hat = extract_features(</span><br><span class="line">            X, content_layers, style_layers)</span><br><span class="line">        <span class="comment"># 计算各项损失及总损失</span></span><br><span class="line">        contents_l, styles_l, tv_l, l = compute_loss(</span><br><span class="line">            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)</span><br><span class="line">        l.backward()</span><br><span class="line">        <span class="comment"># 优化：更新合成图像的像素值</span></span><br><span class="line">        trainer.step()</span><br><span class="line">        scheduler.step()</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            animator.axes[<span class="number">1</span>].imshow(postprocess(X))</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, [<span class="built_in">float</span>(<span class="built_in">sum</span>(contents_l)),</span><br><span class="line">                                     <span class="built_in">float</span>(<span class="built_in">sum</span>(styles_l)), <span class="built_in">float</span>(tv_l)])</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">content_weight, style_weight, tv_weight = <span class="number">1</span>, <span class="number">1e3</span>, <span class="number">10</span></span><br><span class="line">device, image_shape = try_gpu(), (<span class="number">300</span>, <span class="number">450</span>)</span><br><span class="line">net = net.to(device)</span><br><span class="line">content_X, contents_Y = get_contents(image_shape, device)</span><br><span class="line">_, styles_Y = get_styles(image_shape, device)</span><br><span class="line">output = train(content_X, contents_Y, styles_Y, device, <span class="number">0.3</span>, <span class="number">500</span>, <span class="number">50</span>)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511051610.webp" alt="202511051610" style="zoom:80%;">

<p>可以看到，合成图像保留了内容图像的风景和物体，并同时迁移了风格图像的色彩</p>
<p>例如，合成图像具有与风格图像中一样的色彩块，其中一些甚至具有画笔笔触的细微纹理</p>
<h2 id="实战-Kaggle-比赛：图像分类-CIFAR-10"><a href="#实战-Kaggle-比赛：图像分类-CIFAR-10" class="headerlink" title="实战 Kaggle 比赛：图像分类 (CIFAR-10)"></a>实战 Kaggle 比赛：图像分类 (CIFAR-10)</h2><p>之前一直用深度学习框架的高级API直接获取张量格式的图像数据集；在实践中，图像数据集通常以图像文件的形式出现</p>
<p>将从原始图像文件开始，然后逐步组织、读取并将它们转换为张量格式</p>
<p>导入竞赛所需的包和模块</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> shutil  <span class="comment"># 系统文件操作库，功能比 os 更高级</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br></pre></td></tr></tbody></table></figure>

<h3 id="获取并组织数据集"><a href="#获取并组织数据集" class="headerlink" title="获取并组织数据集"></a>获取并组织数据集</h3><p>比赛数据集分为训练集和测试集，其中训练集包含50000张、测试集包含300000张图像</p>
<p>在测试集中，10000张图像将被用于评估，而剩下的290000张图像将不会被进行评估，包含它们只是为了防止手动标记测试集并提交标记结果</p>
<p>两个数据集中的图像都是png格式，高度和宽度均为32像素并有三个颜色通道（RGB）</p>
<p>这些图片共涵盖10个类别：飞机、汽车、鸟类、猫、鹿、狗、青蛙、马、船和卡车</p>
<h4 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h4><p>登录Kaggle后可以点击显示的CIFAR-10图像分类竞赛网页上的“Data”选项卡，然后单击“Download All”按钮下载数据集</p>
<p>在<code>../data</code>中解压下载的文件并在其中解压缩<code>train.7z</code>和<code>test.7z</code>后，在以下路径中可以找到整个数据集：</p>
<ul>
<li><code>../data/cifar-10/train/[1-50000].png</code></li>
<li><code>../data/cifar-10/test/[1-300000].png</code></li>
<li><code>../data/cifar-10/trainLabels.csv</code></li>
<li><code>../data/cifar-10/sampleSubmission.csv</code></li>
</ul>
<p>为了便于入门，提供包含前1000个训练图像和5个随机测试图像的数据集的小规模样本</p>
<p>要使用Kaggle竞赛的完整数据集，需要将以下<code>demo</code>变量设置为<code>False</code></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DATA_HUB = <span class="built_in">dict</span>()</span><br><span class="line">DATA_URL = <span class="string">'http://d2l-data.s3-accelerate.amazonaws.com/'</span></span><br><span class="line">DATA_HUB[<span class="string">'cifar10_tiny'</span>] = (</span><br><span class="line">    DATA_URL + <span class="string">'kaggle_cifar10_tiny.zip'</span>,</span><br><span class="line">    <span class="string">'2068874e4b9a9f0fb07ebe0ad2b29754449ccacd'</span>)</span><br><span class="line">    </span><br><span class="line">demo = <span class="literal">True</span></span><br><span class="line"><span class="keyword">if</span> demo:</span><br><span class="line">    data_dir = download_extract(<span class="string">'cifar10_tiny'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data_dir = <span class="string">'../data/cifar-10/'</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="整理数据集"><a href="#整理数据集" class="headerlink" title="整理数据集"></a>整理数据集</h4><p>需要整理数据集来训练和测试模型，首先用以下函数读取CSV文件中的标签，它返回一个字典，该字典将文件名中不带扩展名的部分映射到其标签</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read_csv_labels</span>(<span class="params">fname</span>):</span><br><span class="line">    <span class="string">"""读取fname来给标签字典返回一个文件名"""</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># 跳过文件头行(列名)</span></span><br><span class="line">        lines = f.readlines()[<span class="number">1</span>:]</span><br><span class="line">    tokens = [l.rstrip().split(<span class="string">','</span>) <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(((name, label) <span class="keyword">for</span> name, label <span class="keyword">in</span> tokens))</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))</span><br><span class="line">print('# 训练样本 :', len(labels))</span><br><span class="line">print('# 类别 :', len(set(labels.values())))</span><br></pre></td></tr></tbody></table></figure>

<p>定义<code>reorg_train_valid</code>函数来将验证集从原始的训练集中拆分出来，此函数中的参数<code>valid_ratio</code>是验证集中的样本数与原始训练集中的样本数之比</p>
<p>组织数据集后，同类别的图像将被放置在同一文件夹下</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">copyfile</span>(<span class="params">filename, target_dir</span>):</span><br><span class="line">    <span class="string">"""将文件复制到目标目录"""</span></span><br><span class="line">    os.makedirs(target_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    shutil.copy(filename, target_dir)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reorg_train_valid</span>(<span class="params">data_dir, labels, valid_ratio</span>):</span><br><span class="line">    <span class="string">"""将验证集从原始训练集中拆分出来</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        data_dir：数据集的主目录路径</span></span><br><span class="line"><span class="string">        labels：字典，键是图片文件名（不带后缀），值是类别名称</span></span><br><span class="line"><span class="string">        valid_ratio：验证集比例（例如 0.1 表示 10% 数据作为验证集）</span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">        n_valid_per_label：每个类别划入验证集的样本数量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 计算每个类别的样本数量</span></span><br><span class="line">    <span class="comment"># collections.Counter(labels.values()) 统计每个类别出现次数</span></span><br><span class="line">    <span class="comment"># most_common()[-1][1] 取样本最少的类别的样本数 n</span></span><br><span class="line">    n = collections.Counter(labels.values()).most_common()[-<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算每个类别分配给验证集的样本数</span></span><br><span class="line">    <span class="comment"># 如果样本太少至少保证每类有1个验证样本</span></span><br><span class="line">    n_valid_per_label = <span class="built_in">max</span>(<span class="number">1</span>, math.floor(n * valid_ratio))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 记录每个类别已放入验证集的数量</span></span><br><span class="line">    label_count = {}</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历所有训练图片文件</span></span><br><span class="line">    <span class="keyword">for</span> train_file <span class="keyword">in</span> os.listdir(os.path.join(data_dir, <span class="string">'train'</span>)):</span><br><span class="line">        <span class="comment"># 根据文件名提取对应的类别标签</span></span><br><span class="line">        label = labels[train_file.split(<span class="string">'.'</span>)[<span class="number">0</span>]]</span><br><span class="line">        <span class="comment"># 拼出原始文件路径</span></span><br><span class="line">        fname = os.path.join(data_dir, <span class="string">'train'</span>, train_file)</span><br><span class="line">        <span class="comment"># 先将图片都复制到 train_valid 文件夹（包含全部数据）</span></span><br><span class="line">        copyfile(fname, os.path.join(data_dir, <span class="string">'train_valid_test'</span>,</span><br><span class="line">                                     <span class="string">'train_valid'</span>, label))</span><br><span class="line">        <span class="comment"># 控制验证集的划分</span></span><br><span class="line">        <span class="comment"># 若该类别还没达到验证集样本上限 → 复制到 valid 文件夹</span></span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> label_count <span class="keyword">or</span> label_count[label] &lt; n_valid_per_label:</span><br><span class="line">            copyfile(fname, os.path.join(data_dir, <span class="string">'train_valid_test'</span>,</span><br><span class="line">                                         <span class="string">'valid'</span>, label))</span><br><span class="line">            <span class="comment"># 计数 +1</span></span><br><span class="line">            label_count[label] = label_count.get(label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 否则复制到 train 文件夹（训练集）</span></span><br><span class="line">            copyfile(fname, os.path.join(data_dir, <span class="string">'train_valid_test'</span>,</span><br><span class="line">                                         <span class="string">'train'</span>, label))</span><br><span class="line">    <span class="comment"># 返回每个类别的验证样本数</span></span><br><span class="line">    <span class="keyword">return</span> n_valid_per_label</span><br></pre></td></tr></tbody></table></figure>

<p><code>reorg_test</code>函数用来在预测期间整理测试集，以方便读取</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reorg_test</span>(<span class="params">data_dir</span>):</span><br><span class="line">    <span class="string">"""在预测期间整理测试集，以方便读取"""</span></span><br><span class="line">    <span class="keyword">for</span> test_file <span class="keyword">in</span> os.listdir(os.path.join(data_dir, <span class="string">'test'</span>)):</span><br><span class="line">        copyfile(os.path.join(data_dir, <span class="string">'test'</span>, test_file),</span><br><span class="line">                 os.path.join(data_dir, <span class="string">'train_valid_test'</span>, <span class="string">'test'</span>,</span><br><span class="line">                              <span class="string">'unknown'</span>))</span><br></pre></td></tr></tbody></table></figure>

<p>最后使用一个函数来调用前面定义的函数<code>read_csv_labels</code>、<code>reorg_train_valid</code>和<code>reorg_test</code></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reorg_cifar10_data</span>(<span class="params">data_dir, valid_ratio</span>):</span><br><span class="line">    labels = read_csv_labels(os.path.join(data_dir, <span class="string">'trainLabels.csv'</span>))</span><br><span class="line">    reorg_train_valid(data_dir, labels, valid_ratio)</span><br><span class="line">    reorg_test(data_dir)</span><br></pre></td></tr></tbody></table></figure>

<p>在这里只将样本数据集的批量大小设置为32，在实际训练和测试中，应该使用Kaggle竞赛的完整数据集，并将<code>batch_size</code>设置为更大的整数，例如128，将10％的训练样本作为调整超参数的验证集</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">32</span> <span class="keyword">if</span> demo <span class="keyword">else</span> <span class="number">128</span></span><br><span class="line">valid_ratio = <span class="number">0.1</span></span><br><span class="line">reorg_cifar10_data(data_dir, valid_ratio)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="图像增广-1"><a href="#图像增广-1" class="headerlink" title="图像增广"></a>图像增广</h3><p>使用图像增广来解决过拟合的问题，在训练中可以随机水平翻转图像，还可以对彩色图像的三个RGB通道执行标准化</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">transform_train = torchvision.transforms.Compose([</span><br><span class="line">    <span class="comment"># 在高度和宽度上将图像放大到40像素的正方形</span></span><br><span class="line">    torchvision.transforms.Resize(<span class="number">40</span>),</span><br><span class="line">    <span class="comment"># 随机裁剪出一个高度和宽度均为40像素的正方形图像，</span></span><br><span class="line">    <span class="comment"># 生成一个面积为原始图像面积0.64～1倍的小正方形，</span></span><br><span class="line">    <span class="comment"># 然后将其缩放为高度和宽度均为32像素的正方形</span></span><br><span class="line">    torchvision.transforms.RandomResizedCrop(<span class="number">32</span>, scale=(<span class="number">0.64</span>, <span class="number">1.0</span>),</span><br><span class="line">                                                   ratio=(<span class="number">1.0</span>, <span class="number">1.0</span>)),</span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    <span class="comment"># 标准化图像的每个通道</span></span><br><span class="line">    torchvision.transforms.Normalize([<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>],</span><br><span class="line">                                     [<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>])])</span><br></pre></td></tr></tbody></table></figure>

<p>在测试期间只对图像执行标准化，以消除评估结果中的随机性</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transform_test = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    torchvision.transforms.Normalize([<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>],</span><br><span class="line">                                     [<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>])])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="读取数据集-2"><a href="#读取数据集-2" class="headerlink" title="读取数据集"></a>读取数据集</h3><p>读取由原始图像组成的数据集，每个样本都包括一张图片和一个标签</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(</span><br><span class="line">    os.path.join(data_dir, <span class="string">'train_valid_test'</span>, folder),</span><br><span class="line">    transform=transform_train) <span class="keyword">for</span> folder <span class="keyword">in</span> [<span class="string">'train'</span>, <span class="string">'train_valid'</span>]]</span><br><span class="line"></span><br><span class="line">valid_ds, test_ds = [torchvision.datasets.ImageFolder(</span><br><span class="line">    os.path.join(data_dir, <span class="string">'train_valid_test'</span>, folder),</span><br><span class="line">    transform=transform_test) <span class="keyword">for</span> folder <span class="keyword">in</span> [<span class="string">'valid'</span>, <span class="string">'test'</span>]]</span><br></pre></td></tr></tbody></table></figure>

<p>在训练期间需要指定上面定义的所有图像增广操作，当验证集在超参数调整过程中用于模型评估时，不应引入图像增广的随机性</p>
<p>在最终预测之前，根据训练集和验证集组合而成的训练模型进行训练，以充分利用所有标记的数据</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_iter, train_valid_iter = [torch.utils.data.DataLoader(</span><br><span class="line">    dataset, batch_size, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> dataset <span class="keyword">in</span> (train_ds, train_valid_ds)]</span><br><span class="line"></span><br><span class="line">valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                                         drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                                        drop_last=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>使用ResNet18</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    num_classes = <span class="number">10</span></span><br><span class="line">    net = resnet18(num_classes, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">"none"</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="定义训练函数"><a href="#定义训练函数" class="headerlink" title="定义训练函数"></a>定义训练函数</h3><p>将根据模型在验证集上的表现来选择模型并调整超参数，下面定义了模型训练函数<code>train</code></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,</span></span><br><span class="line"><span class="params">          lr_decay</span>):</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=<span class="number">0.9</span>,</span><br><span class="line">                              weight_decay=wd)</span><br><span class="line">    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)</span><br><span class="line">    num_batches, timer = <span class="built_in">len</span>(train_iter), Timer()</span><br><span class="line">    legend = [<span class="string">'train loss'</span>, <span class="string">'train acc'</span>]</span><br><span class="line">    <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        legend.append(<span class="string">'valid acc'</span>)</span><br><span class="line">    animator = Animator(xlabel=<span class="string">'epoch'</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=legend)</span><br><span class="line">    net = nn.DataParallel(net, device_ids=devices).to(devices[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        net.train()</span><br><span class="line">        metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            l, acc = train_batch_ch13(net, features, labels,</span><br><span class="line">                                          loss, trainer, devices)</span><br><span class="line">            metric.add(l, acc, labels.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>],</span><br><span class="line">                              <span class="literal">None</span>))</span><br><span class="line">        <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            valid_acc = evaluate_accuracy_gpu(net, valid_iter)</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, valid_acc))</span><br><span class="line">        scheduler.step()</span><br><span class="line">    measures = (<span class="string">f'train loss <span class="subst">{metric[<span class="number">0</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f}</span>, '</span></span><br><span class="line">                <span class="string">f'train acc <span class="subst">{metric[<span class="number">1</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f}</span>'</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        measures += <span class="string">f', valid acc <span class="subst">{valid_acc:<span class="number">.3</span>f}</span>'</span></span><br><span class="line">    <span class="built_in">print</span>(measures + <span class="string">f'\n<span class="subst">{metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f}</span>'</span></span><br><span class="line">          <span class="string">f' examples/sec on <span class="subst">{<span class="built_in">str</span>(devices)}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练和验证模型"><a href="#训练和验证模型" class="headerlink" title="训练和验证模型"></a>训练和验证模型</h3><p>以下所有超参数都可以调整，比如可以增加周期的数量</p>
<p>当<code>lr_period</code>和<code>lr_decay</code>分别设置为4和0.9时，优化算法的学习速率将在每4个周期乘以0.9，在这里只训练20个周期</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">devices, num_epochs, lr, wd = try_all_gpus(), <span class="number">20</span>, <span class="number">2e-4</span>, <span class="number">5e-4</span></span><br><span class="line">lr_period, lr_decay, net = <span class="number">4</span>, <span class="number">0.9</span>, get_net()</span><br><span class="line">train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,</span><br><span class="line">      lr_decay)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="对测试集进行分类并提交结果"><a href="#对测试集进行分类并提交结果" class="headerlink" title="对测试集进行分类并提交结果"></a>对测试集进行分类并提交结果</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net, preds = get_net(), []</span><br><span class="line">train(net, train_valid_iter, <span class="literal">None</span>, num_epochs, lr, wd, devices, lr_period,</span><br><span class="line">      lr_decay)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, _ <span class="keyword">in</span> test_iter:</span><br><span class="line">    y_hat = net(X.to(devices[<span class="number">0</span>]))</span><br><span class="line">    preds.extend(y_hat.argmax(dim=<span class="number">1</span>).<span class="built_in">type</span>(torch.int32).cpu().numpy())</span><br><span class="line">sorted_ids = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(test_ds) + <span class="number">1</span>))</span><br><span class="line">sorted_ids.sort(key=<span class="keyword">lambda</span> x: <span class="built_in">str</span>(x))</span><br><span class="line">df = pd.DataFrame({<span class="string">'id'</span>: sorted_ids, <span class="string">'label'</span>: preds})</span><br><span class="line">df[<span class="string">'label'</span>] = df[<span class="string">'label'</span>].apply(<span class="keyword">lambda</span> x: train_valid_ds.classes[x])</span><br><span class="line">df.to_csv(<span class="string">'submission.csv'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://yhblogs.cn">今天睡够了吗</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://yhblogs.cn/posts/62963.html">http://yhblogs.cn/posts/62963.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yhblogs.cn" target="_blank">がんばろう</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E2%8C%A8%EF%B8%8Fpython/">⌨️python</a></div><div class="post_share"><div class="social-share" data-image="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-po97l3_1280x720.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/3339.html" title="注意力机制"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-lyyx5q_2560x1440.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">注意力机制</div></div></a></div><div class="next-post pull-right"><a href="/posts/7224.html" title="循环神经网络"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-d8633m_1280x720.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">循环神经网络</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/30698.html" title="BERT_Pytorch"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7jjyd9_2560x1440.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-09</div><div class="title">BERT_Pytorch</div></div></a></div><div><a href="/posts/31208.html" title="FunRec 推荐系统_精排模型"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7j931e_1280x720_(1) (1).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-18</div><div class="title">FunRec 推荐系统_精排模型</div></div></a></div><div><a href="/posts/24333.html" title="FunRec推荐系统_召回模型"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-vpp725_1280x720_(1).webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-14</div><div class="title">FunRec推荐系统_召回模型</div></div></a></div><div><a href="/posts/58676.html" title="Leetcode100记录"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-9ozdyx_1280x720.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-26</div><div class="title">Leetcode100记录</div></div></a></div><div><a href="/posts/22642.html" title="windows安装ROCm"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/ROCm_logo.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-10</div><div class="title">windows安装ROCm</div></div></a></div><div><a href="/posts/3865533702.html" title="pyqt5简单实践"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071521231.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-28</div><div class="title">pyqt5简单实践</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/b_2a1aef95f351a5f7ef72eb81e6838fd6.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">今天睡够了吗</div><div class="author-info__description">相遇是最小单位的奇迹</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071549233.webp" target="_blank" title="QQ"><i class="iconfont icon-QQ"></i></a><a class="social-icon" href="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071549234.webp" target="_blank" title="微信"><i class="iconfont icon-weixin"></i></a><a class="social-icon" href="https://space.bilibili.com/277953459?spm_id_from=333.1007.0.0" target="_blank" title="bilibili"><i class="iconfont icon-bilibili"></i></a><a class="social-icon" href="https://github.com/YaoHui-Wu06022" target="_blank" title="Github"><i class="iconfont icon-GitHub"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">保持理智，相信明天</div><div class="twopeople"><div class="twopeople"><div class="container" style="height:200px;"><canvas class="illo" width="800" height="800" style="max-width: 200px; max-height: 200px; touch-action: none; width: 640px; height: 640px;"></canvas></div> <script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople1.js"></script> <script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/zdog.dist.js"></script> <script id="rendered-js" src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople.js"></script> <style>.twopeople{margin:0;align-items:center;justify-content:center;text-align:center}canvas{display:block;margin:0 auto;cursor:move}</style></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%B9%BF"><span class="toc-number">1.</span> <span class="toc-text">图像增广</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">1.1.</span> <span class="toc-text">常用方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BF%BB%E8%BD%AC%E5%92%8C%E8%A3%81%E5%89%AA"><span class="toc-number">1.1.1.</span> <span class="toc-text">翻转和裁剪</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%B9%E5%8F%98%E9%A2%9C%E8%89%B2"><span class="toc-number">1.1.2.</span> <span class="toc-text">改变颜色</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%BC%E5%90%88%E4%BD%BF%E7%94%A8"><span class="toc-number">1.1.3.</span> <span class="toc-text">综合使用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.2.</span> <span class="toc-text">训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9AGPU%E8%AE%AD%E7%BB%83"><span class="toc-number">1.2.1.</span> <span class="toc-text">多GPU训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.3.</span> <span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83"><span class="toc-number">2.</span> <span class="toc-text">微调</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4"><span class="toc-number">2.1.</span> <span class="toc-text">步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%83%AD%E7%8B%97%E8%AF%86%E5%88%AB"><span class="toc-number">2.2.</span> <span class="toc-text">热狗识别</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.2.1.</span> <span class="toc-text">获取数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E5%92%8C%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.2.2.</span> <span class="toc-text">定义和初始化模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.2.3.</span> <span class="toc-text">微调模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-1"><span class="toc-number">2.3.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%9D%E8%80%83%E9%A2%98"><span class="toc-number">2.4.</span> <span class="toc-text">思考题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%92%8C%E8%BE%B9%E7%95%8C%E6%A1%86"><span class="toc-number">3.</span> <span class="toc-text">目标检测和边界框</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%B9%E7%95%8C%E6%A1%86"><span class="toc-number">3.1.</span> <span class="toc-text">边界框</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%94%9A%E6%A1%86"><span class="toc-number">4.</span> <span class="toc-text">锚框</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%A4%9A%E4%B8%AA%E9%94%9A%E6%A1%86"><span class="toc-number">4.1.</span> <span class="toc-text">生成多个锚框</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%B9%B6%E6%AF%94-IoU"><span class="toc-number">4.2.</span> <span class="toc-text">交并比(IoU)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E6%A0%87%E6%B3%A8%E9%94%9A%E6%A1%86"><span class="toc-number">4.3.</span> <span class="toc-text">在训练数据中标注锚框</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86%E7%9C%9F%E5%AE%9E%E8%BE%B9%E7%95%8C%E6%A1%86%E5%88%86%E9%85%8D%E7%BB%99%E9%94%9A%E6%A1%86"><span class="toc-number">4.3.1.</span> <span class="toc-text">将真实边界框分配给锚框</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%87%E8%AE%B0%E7%B1%BB%E5%88%AB%E5%92%8C%E5%81%8F%E7%A7%BB%E9%87%8F"><span class="toc-number">4.3.2.</span> <span class="toc-text">标记类别和偏移量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-number">4.3.3.</span> <span class="toc-text">例子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6%E9%A2%84%E6%B5%8B%E8%BE%B9%E7%95%8C%E6%A1%86"><span class="toc-number">4.4.</span> <span class="toc-text">使用非极大值抑制预测边界框</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8C%BA%E5%9F%9F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-R-CNN"><span class="toc-number">5.</span> <span class="toc-text">区域卷积神经网络(R-CNN)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#R-CNN"><span class="toc-number">5.1.</span> <span class="toc-text">R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fast-R-CNN"><span class="toc-number">5.2.</span> <span class="toc-text">Fast R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Faster-R-CNN"><span class="toc-number">5.3.</span> <span class="toc-text">Faster R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mask-R-CNN"><span class="toc-number">5.4.</span> <span class="toc-text">Mask R-CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-2"><span class="toc-number">5.5.</span> <span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">6.</span> <span class="toc-text">语义分割和数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%92%8C%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2"><span class="toc-number">6.1.</span> <span class="toc-text">图像分割和实例分割</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pascal-VOC2012"><span class="toc-number">6.2.</span> <span class="toc-text">Pascal VOC2012</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="toc-number">6.2.1.</span> <span class="toc-text">预处理数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E6%95%B0%E6%8D%AE%E9%9B%86%E7%B1%BB"><span class="toc-number">6.2.2.</span> <span class="toc-text">自定义语义分割数据集类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">6.2.3.</span> <span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E6%89%80%E6%9C%89%E7%BB%84%E4%BB%B6"><span class="toc-number">6.2.4.</span> <span class="toc-text">整合所有组件</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-3"><span class="toc-number">6.3.</span> <span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF"><span class="toc-number">7.</span> <span class="toc-text">转置卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">7.1.</span> <span class="toc-text">基本操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A1%AB%E5%85%85%E3%80%81%E6%AD%A5%E5%B9%85%E5%92%8C%E5%A4%9A%E9%80%9A%E9%81%93"><span class="toc-number">7.2.</span> <span class="toc-text">填充、步幅和多通道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E7%9F%A9%E9%98%B5%E5%8F%98%E6%8D%A2%E7%9A%84%E8%81%94%E7%B3%BB"><span class="toc-number">7.3.</span> <span class="toc-text">与矩阵变换的联系</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="toc-number">8.</span> <span class="toc-text">全卷积网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E9%80%A0%E6%A8%A1%E5%9E%8B"><span class="toc-number">8.1.</span> <span class="toc-text">构造模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">8.2.</span> <span class="toc-text">初始化转置卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86-1"><span class="toc-number">8.3.</span> <span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="toc-number">8.4.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B"><span class="toc-number">8.5.</span> <span class="toc-text">预测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB"><span class="toc-number">9.</span> <span class="toc-text">风格迁移</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">9.1.</span> <span class="toc-text">方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E5%A4%84%E7%90%86%E5%92%8C%E5%90%8E%E5%A4%84%E7%90%86"><span class="toc-number">9.2.</span> <span class="toc-text">预处理和后处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%BD%E5%8F%96%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81"><span class="toc-number">9.3.</span> <span class="toc-text">抽取图像特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">9.4.</span> <span class="toc-text">定义损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AE%B9%E6%8D%9F%E5%A4%B1"><span class="toc-number">9.4.1.</span> <span class="toc-text">内容损失</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A3%8E%E6%A0%BC%E6%8D%9F%E5%A4%B1"><span class="toc-number">9.4.2.</span> <span class="toc-text">风格损失</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%A8%E5%8F%98%E5%88%86%E6%8D%9F%E5%A4%B1"><span class="toc-number">9.4.3.</span> <span class="toc-text">全变分损失</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">9.4.4.</span> <span class="toc-text">损失函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%90%88%E6%88%90%E5%9B%BE%E5%83%8F"><span class="toc-number">9.5.</span> <span class="toc-text">初始化合成图像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.6.</span> <span class="toc-text">训练模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E6%88%98-Kaggle-%E6%AF%94%E8%B5%9B%EF%BC%9A%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-CIFAR-10"><span class="toc-number">10.</span> <span class="toc-text">实战 Kaggle 比赛：图像分类 (CIFAR-10)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E5%B9%B6%E7%BB%84%E7%BB%87%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">10.1.</span> <span class="toc-text">获取并组织数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">10.1.1.</span> <span class="toc-text">下载数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E7%90%86%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">10.1.2.</span> <span class="toc-text">整理数据集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%B9%BF-1"><span class="toc-number">10.2.</span> <span class="toc-text">图像增广</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86-2"><span class="toc-number">10.3.</span> <span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-number">10.4.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%87%BD%E6%95%B0"><span class="toc-number">10.5.</span> <span class="toc-text">定义训练函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E9%AA%8C%E8%AF%81%E6%A8%A1%E5%9E%8B"><span class="toc-number">10.6.</span> <span class="toc-text">训练和验证模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%B5%8B%E8%AF%95%E9%9B%86%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB%E5%B9%B6%E6%8F%90%E4%BA%A4%E7%BB%93%E6%9E%9C"><span class="toc-number">10.7.</span> <span class="toc-text">对测试集进行分类并提交结果</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">©2022 - 2026 By 今天睡够了吗</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">You must always have faith in who you are！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="false"></script><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>