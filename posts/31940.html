<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>线性神经网络 | がんばろう</title><meta name="author" content="今天睡够了吗"><meta name="copyright" content="今天睡够了吗"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="从经典算法————线性神经网络开始，介绍神经网络的基础知识 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络 线性回归基本元素**回归(regression)**是能为一个或多个自变量与因变量之间关系建模的一类方法 在机器学习领域中的大多数任务通常都与**预测(prediction)**有关，但不是所有的预测都是回归问题 线性回归基于两个简单假设  自变量$\mathbf{x}">
<meta property="og:type" content="article">
<meta property="og:title" content="线性神经网络">
<meta property="og:url" content="http://yhblogs.cn/posts/31940.html">
<meta property="og:site_name" content="がんばろう">
<meta property="og:description" content="从经典算法————线性神经网络开始，介绍神经网络的基础知识 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络 线性回归基本元素**回归(regression)**是能为一个或多个自变量与因变量之间关系建模的一类方法 在机器学习领域中的大多数任务通常都与**预测(prediction)**有关，但不是所有的预测都是回归问题 线性回归基于两个简单假设  自变量$\mathbf{x}">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-1qpqrw_1280x720.webp">
<meta property="article:published_time" content="2025-10-14T10:30:57.000Z">
<meta property="article:modified_time" content="2026-01-31T12:00:30.728Z">
<meta property="article:author" content="今天睡够了吗">
<meta property="article:tag" content="⌨️python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-1qpqrw_1280x720.webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yhblogs.cn/posts/31940.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '线性神经网络',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-01-31 12:00:30'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_3319458_ks437t3n4r.css"><link rel="stylesheet" href="/css/modify.css"><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/b_2a1aef95f351a5f7ef72eb81e6838fd6.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouye"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw iconfont icon-rili"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw iconfont icon-biaoqian"></i><span> 标签</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="がんばろう"><img class="site-icon" src="/img/favicon.png"><span class="site-name">がんばろう</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouye"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw iconfont icon-rili"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw iconfont icon-biaoqian"></i><span> 标签</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">线性神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-10-14T10:30:57.000Z" title="发表于 2025-10-14 10:30:57">2025-10-14</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">14.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>55分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="线性神经网络"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p>从经典算法————线性神经网络开始，介绍神经网络的基础知识</p>
<p>经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="基本元素"><a href="#基本元素" class="headerlink" title="基本元素"></a>基本元素</h3><p>**回归(regression)**是能为一个或多个自变量与因变量之间关系建模的一类方法</p>
<p>在机器学习领域中的大多数任务通常都与**预测(prediction)**有关，但不是所有的预测都是回归问题</p>
<p><font color="Violetred">线性回归基于两个简单假设</font></p>
<ul>
<li>自变量$\mathbf{x}$和因变量$y$之间的关系是线性的，即$y$可以表示为$\mathbf{x}$中元素的加权和，通常允许包含观测值的一些噪声</li>
<li>任何噪声都比较正常，如噪声遵循正态分布</li>
</ul>
<p>为了开发一个能预测房价的模型，需要收集一个真实的数据集，包括房屋的销售价格、面积和房龄，该数据集称为<strong>训练集(training set)</strong>，每行数据称为<strong>样本(sample)</strong></p>
<p>把试图预测的目标(房屋价格)称为<strong>标签(label)或目标(target)</strong>，预测所依据的自变量(面积和房龄)称为<strong>特征(feature)</strong></p>
<p>通常使用$n$来表示数据集中的样本数，对索引为$i$的样本，其输入表示为$\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\top$，其对应的标签是$y^{(i)}$</p>
<h4 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h4><p>根据线性假设，价格表示为<br>$$<br>\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b.<br>$$<br>$w_{\mathrm{area}}$和$w_{\mathrm{age}}$称为权重，权重决定了每个特征对预测值的影响，$b$称为<strong>偏置(bias)</strong></p>
<p>这个式子是输入特征的<strong>仿射变换(affine transformation)</strong>，即通过加权和对特征进行<strong>线性变换(linear transformation)</strong>，并通过偏置项来进行<strong>平移(translation)</strong></p>
<p><font color="DarkViolet">给定一个数据集，目标是寻找模型的权重$\mathbf{w}$和偏置$b$</font></p>
<p>输入包含$d$个特征时，将预测结果$\hat{y}$表示为<br>$$<br>\hat{y} = w_1  x_1 + … + w_d  x_d + b.<br>$$<br>将所有特征放到向量$\mathbf{x} \in \mathbb{R}^d$，并将所有权重放到向量$\mathbf{w} \in \mathbb{R}^d$中，可以用点积形式来简洁地表达模型<br>$$<br>\hat{y} = \mathbf{w}^\top \mathbf{x} + b.<br>$$<br>用矩阵$\mathbf{X} \in \mathbb{R}^{n \times d}$可以很方便地引用整个数据集的$n$个样本，每一行是一个样本，每一列是一种特征</p>
<p>预测值$\hat{\mathbf{y}} \in \mathbb{R}^n$通过矩阵-向量乘法表示为<br>$$<br>\color{purple} {\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b<br>$$<br>无论使用什么手段来观察特征$\mathbf{X}$和标签$\mathbf{y}$都可能会出现少量的观测误差</p>
<p>因此即使确信特征与标签的潜在关系是线性的，也会加入一个噪声项来考虑观测误差带来的影响</p>
<p>在开始寻找最好的模型参数$\mathbf{w}$和$b$之前，需要确定两项</p>
<ul>
<li>模型质量的度量方式</li>
<li>能够更新模型以提高模型预测质量的方法</li>
</ul>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>**损失函数(loss function)**能够量化目标的实际值与预测值之间的差距，通常选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0</p>
<p><font color="DarkViolet">回归问题中最常用的损失函数是均方误差函数</font>，样本$i$的预测值为$\hat{y}^{(i)}$，其真实标签为$y^{(i)}$时，平方误差表示为<br>$$<br>l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.<br>$$<br>常数1/2不会带来本质的差别，但这样方便后续求导</p>
<p>计算在训练集$n$个样本上的<strong>均方误差</strong><br>$$<br>L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2=\frac{1}{n}\mid\mid \mathbf y-\mathbf X \mathbf w\mid\mid^2<br>$$</p>
<h4 id="解析解"><a href="#解析解" class="headerlink" title="解析解"></a>解析解</h4><p>线性回归的解可以用一个公式简单地表达出来，这类解叫作<strong>解析解(analytical solution)</strong></p>
<p>可以直接通过代数公式求出最优解，不需要迭代算法(如梯度下降)</p>
<p>将损失关于$\mathbf{w}$的导数设为0，得到解析解</p>
<p>计算过程</p>
<ol>
<li><p>先把损失函数展开<br>$$<br>L(\mathbf w,b)=(\mathbf y-\mathbf X \mathbf w)^{\top}(\mathbf y-\mathbf X \mathbf w)<br>$$</p>
</li>
<li><p>对$\mathbf w$求导，并令导数为零(最小值点的一阶导数为零)<br>$$<br>\frac{\partial L}{\partial \mathbf w}=-2 \mathbf X^{\top}(\mathbf y-\mathbf X \mathbf w)=0<br>$$</p>
</li>
<li><p>移项得<br>$$<br>\mathbf X^{\top} \mathbf X \mathbf w=\mathbf X^{\top} \mathbf y<br>$$</p>
</li>
<li><p>当$\mathbf X^{\top} \mathbf X $ 可逆时，求得解析解<br>$$<br>\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}.<br>$$</p>
</li>
</ol>
<p>但并不是所有的问题都存在解析解</p>
<p>解析解对问题的限制很严格，导致它无法广泛应用在深度学习里</p>
<h4 id="随机梯度下降SGD"><a href="#随机梯度下降SGD" class="headerlink" title="随机梯度下降SGD"></a>随机梯度下降SGD</h4><p><strong>随机梯度下降SGD</strong>几乎可以优化所有深度学习模型，它通过在损失函数递减的方向上更新参数来降低误差</p>
<p>最简单的用法是计算损失函数关于模型参数的导数，但计算量可能很大，通常会随机抽取一小批样本，这种变体叫做<strong>小批量随机梯度下降(minibatch stochastic gradient descent)</strong></p>
<p>在每次迭代中，首先随机抽样一个小批量，由<strong>批量大小(batch size)<strong>为$\mid \mathcal{B}\mid $个的训练样本组成的，计算平均损失关于模型参数的导数，将梯度乘以</strong>学习率$\eta$(learning rate)</strong>，并从当前参数的值中减掉<br>$$<br>(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).<br>$$<br>使得参数沿着让误差变小的方向移动</p>
<p>算法的步骤如下：</p>
<ol>
<li>初始化模型参数的值，如随机初始化</li>
<li>从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代</li>
</ol>
<p>对于均方损失和仿射变换可以写成<br>$$<br>\begin{split}\begin{aligned} \mathbf{w} &amp;\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\<br>b &amp;\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}\end{split}<br>$$<br>批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的</p>
<p><font color="Violetred">可以调整但不在训练过程中更新的参数称为超参数</font><strong>(hyperparameter)</strong>，调参是选择超参数的过程</p>
<p><font color="Violetred">超参数通常是根据训练迭代结果来调整的</font>，而训练迭代结果是在独立的**验证数据集(validation dataset)**上评估得到的</p>
<p>线性回归是在整个域中只有一个最小值的学习问题，但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。更难做到的是找到一组参数，这组参数能够在从未见过的数据上实现较低的损失，这一挑战被称为<strong>泛化(generalization)</strong></p>
<h3 id="矢量化加速"><a href="#矢量化加速" class="headerlink" title="矢量化加速"></a>矢量化加速</h3><p>训练模型时，经常希望能够同时处理整个小批量的样本，为了实现这一点，需要对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环</p>
<p>因为频繁对运行时间进行基准测试，需要定义一个计时器类</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Timer</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""记录多次运行时间"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.times = [] <span class="comment"># 保存每次运行的耗时</span></span><br><span class="line">        <span class="variable language_">self</span>.start()    <span class="comment"># 初始化时就自动调用start()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""启动计时器"""</span></span><br><span class="line">        <span class="variable language_">self</span>.tik = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">stop</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""停止计时器并将时间记录在列表中"""</span></span><br><span class="line">        <span class="variable language_">self</span>.times.append(time.time() - <span class="variable language_">self</span>.tik)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.times[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">avg</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""返回平均时间"""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="variable language_">self</span>.times) / <span class="built_in">len</span>(<span class="variable language_">self</span>.times)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sum</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""返回时间总和"""</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(<span class="variable language_">self</span>.times)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cumsum</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""返回累计时间"""</span></span><br><span class="line">        <span class="keyword">return</span> np.array(<span class="variable language_">self</span>.times).cumsum().tolist()</span><br></pre></td></tr></tbody></table></figure>

<p>现在可以对工作负载进行基准测试</p>
<p>使用for循环，每次执行一位的加法</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">10000</span></span><br><span class="line">a = torch.ones(n)</span><br><span class="line">b = torch.ones(n)</span><br><span class="line">c = torch.zeros(n)</span><br><span class="line">timer = Timer()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    c[i] = a[i] + b[i]</span><br><span class="line"><span class="string">f'<span class="subst">{timer.stop():<span class="number">.5</span>f}</span> sec'</span></span><br><span class="line"><span class="comment"># 输出'0.06851 sec'</span></span><br></pre></td></tr></tbody></table></figure>

<p>使用重载的+运算符来计算按元素的和</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">timer.start()</span><br><span class="line">d = a + b</span><br><span class="line"><span class="string">f'<span class="subst">{timer.stop():<span class="number">.5</span>f}</span> sec'</span></span><br><span class="line"><span class="comment"># 输出'0.00000 sec'</span></span><br></pre></td></tr></tbody></table></figure>

<p>矢量化代码通常会带来数量级的加速</p>
<h3 id="正态分布与平方损失"><a href="#正态分布与平方损失" class="headerlink" title="正态分布与平方损失"></a>正态分布与平方损失</h3><p><strong>正态分布(normal distribution)<strong>也称为</strong>高斯分布(Gaussian distribution)</strong>，和线性回归之间的关系很密切</p>
<p>若随机变量$x$具有均值$\mu$和方差$\sigma^2$，其正态分布概率密度函数如下<br>$$<br>p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).<br>$$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">x, mu, sigma</span>):</span><br><span class="line">    p = <span class="number">1</span> / np.sqrt(<span class="number">2</span> * np.pi * sigma**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> p * np.exp(-<span class="number">0.5</span> / sigma**<span class="number">2</span> * (x - mu)**<span class="number">2</span>)</span><br><span class="line">x = np.arange(-<span class="number">7</span>, <span class="number">7</span>, <span class="number">0.01</span>)</span><br><span class="line">params = [(<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">1</span>)]</span><br><span class="line"><span class="keyword">for</span> mu, sigma <span class="keyword">in</span> params:</span><br><span class="line">    plt.plot(x, normal(x, mu, sigma), label=<span class="string">f'$\mu$=<span class="subst">{mu}</span>, $\sigma$=<span class="subst">{sigma}</span>'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'p(x)'</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend()</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510141622.webp" alt="202510141622" style="zoom:67%;">

<p>改变均值会产生沿轴的偏移，增加方差将会分散分布、降低其峰值</p>
<p>假设了观测中包含噪声，其中噪声服从正态分布$\epsilon \sim \mathcal{N}(0, \sigma^2)$<br>$$<br>y = \mathbf{w}^\top \mathbf{x} + b + \epsilon<br>$$<br>通过给定的$\mathbf{x}$观测到特定$y$的<strong>似然(likelihood)</strong><br>$$<br>P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).<br>$$<br>根据<font color="DarkViolet">极大似然估计法(MLE)</font>，参数$\mathbf{w}$和$b$的最优值是使整个数据集似然最大的值<br>$$<br>P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)}).<br>$$<br>由于历史原因，优化通常是说最小化而不是最大化，所以改为<strong>最小化负对数似然</strong>$-\log P(\mathbf y \mid \mathbf X)$</p>
<p>$$<br>-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2<br>$$<br>第一项与$\mathbf{w},b$无关；第二项除了系数，其余部分与平方方误差相同，所以<font color="DarkViolet">在高斯噪声的假设下，最小化平方误差等价于对线性模型的极大似然估计</font></p>
<h3 id="从线性回归到深度网络"><a href="#从线性回归到深度网络" class="headerlink" title="从线性回归到深度网络"></a>从线性回归到深度网络</h3><p>将线性回归模型描述为一个神经网络</p>
<p><img src="https://zh.d2l.ai/_images/singleneuron.svg"></p>
<p>该图只显示连接模式，即只显示每个输入如何连接到输出，隐去了权重和偏置的值</p>
<p>由于模型重点在发生计算的地方，所以通常在计算层数时不考虑输入层，图中神经网络的层数为1</p>
<p>可以将线性回归模型视为仅由单个人工神经元组成的神经网络，或称为<strong>单层神经网络</strong></p>
<p>对于线性回归，每个输入都与每个输出相连，将这种变换(图中输出层)称为<strong>全连接层(fully-connected layer)</strong></p>
<h3 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h3><ol>
<li><p>假设有一些数据$x_1, \ldots, x_n \in \mathbb{R}$，目标是找到一个常数$b$，使最小化$\sum_i (x_i - b)^2$</p>
<ul>
<li>找到最优值$b$的解析解</li>
<li>这个问题及其解与正态分布有什么关系</li>
</ul>
<p>$$<br>\begin{aligned}<br>&amp;f^{\prime}(b)=\sum_{i=1}^n 2\left(b-x_i\right)=2\left(n b-\sum_i x_i\right)=0 \Rightarrow b^*=\frac{1}{n} \sum_{i=1}^n x_i=\bar{x} .<br>\end{aligned}<br>$$</p>
<p>二阶导$f^{\prime \prime}(b)=2 n&gt;0$，因此这是唯一全局最小值</p>
<p><font color="Violetred">所以最小二乘下，最优常数就是样本均值</font></p>
<p>若数据独立同分布于$\mathcal{N}(\mu , \sigma^2)$，极大化似然就等价于最小化平方和，因此MLE的$\mu$就是$\bar x$，这解释了为什么“最小二乘”天然匹配“高斯噪声”</p>
<p>如果把损失从$L_2$改为$L_1$范数，最优常数会变成中位数</p>
</li>
<li><p>用矩阵和向量表示法写出优化问题</p>
<p>带上偏置$b$的写法<br>$$<br>\tilde {\mathbf X} = [\mathbf X \quad \mathbf 1] \in \mathbb{R}^{n\times (d+1)},\tilde {\mathbf w} =  [\mathbf w^\top\quad b]^\top<br>$$</p>
</li>
<li><p>损失对$w$的梯度<br>$$<br>\nabla_{\mathbf w} L=-2 \mathbf X^{\top}(\mathbf y-\mathbf X \mathbf w) \rightarrow \nabla^2_{\mathbf w} L = 2\mathbf X^{\top} \mathbf X<br>$$<br>二阶导数大于等于0</p>
</li>
<li><p>什么时候使用随机梯度下降更好？这种方法何时会失效？</p>
<p>样本数量大，特征维度也大时使用随机梯度下降更好，深度神经网络训练几乎都是用 SGD 及其变种</p>
<p>学习率/调度不当，损失函数过于崎岖时将失效</p>
</li>
<li><p>假定控制附加噪声的噪声模型是指数分布$p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)$</p>
<ul>
<li><p>写出模型$-\log P(\mathbf y \mid \mathbf X)$下数据的负对数似然<br>$$<br>P(y \mid \mathbf{x}) = \frac{1}{2}\exp (-\mid y - \mathbf{w}^\top \mathbf{x} - b \mid)<br>$$<br>负对数似然<br>$$<br>-\log P(\mathbf y \mid \mathbf X) =  n\log2 + \sum_{i=1}^n\mid y - \mathbf{w}^\top \mathbf{x} - b \mid<br>$$<br>常数项忽略以后等价于最小化 L1 回归损失</p>
</li>
<li><p>提出一种随机梯度下降算法来解决这个问题。哪里可能出错？</p>
<p>L1 在 0 处有“拐点”，$\operatorname{sgn}(r)$会在正负之间反复切换，如果学习率不衰减就会持续震荡，导致无法收敛</p>
</li>
</ul>
</li>
</ol>
<h2 id="线性回归的底层实现"><a href="#线性回归的底层实现" class="headerlink" title="线性回归的底层实现"></a>线性回归的底层实现</h2><p>虽然深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保知道自己在做什么</p>
<h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>将根据带有噪声的线性模型构造一个数据集，合成数据集是一个矩阵$\mathbf{X}\in \mathbb{R}^{1000 \times 2}$</p>
<p>使用线性模型参数$\mathbf{w} = [2, -3.4]^\top$，$b = 4.2$以及噪声$\varepsilon $生成数据集及其标签</p>
<p>$\epsilon$可以视为模型预测和标签时的潜在观测误差，假设服从均值为0的正态分布，标准差为0.01</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""生成y=Xw+b+噪声"""</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    y = torch.matmul(X, w) + b <span class="comment"># 等价于 X @ w + b，在高维推荐matmul</span></span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>通过生成第二个特征<code>features[:, 1]</code>和<code>labels</code>的散点图，可以直观观察到两者之间的线性关系</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要从张量换为numpy</span></span><br><span class="line">plt.scatter(features[:,<span class="number">1</span>].detach().numpy(), labels.detach().numpy(),s=<span class="number">5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/image-20251014173316948.png" alt="image-20251014173316948" style="zoom:67%;">

<h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><p>训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新模型</p>
<p>有必要定义一个函数，该函数能打乱数据集中的样本并以小批量方式获取数据</p>
<p>定义一个<code>data_iter</code>函数，该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为<code>batch_size</code>的小批量，每个小批量包含一组特征和标签</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features) <span class="comment"># 统计样本数</span></span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples)) <span class="comment"># 创建索引列表</span></span><br><span class="line">    <span class="comment"># 这些样本是随机读取的，没有特定的顺序</span></span><br><span class="line">    random.shuffle(indices)  <span class="comment"># 打乱索引顺序</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size): <span class="comment"># 遍历样本</span></span><br><span class="line">        <span class="comment"># 获得按batch_size大小获取随机索引</span></span><br><span class="line">        batch_indices = torch.tensor(  <span class="comment"># 张量索引为了减少隐式拷贝</span></span><br><span class="line">            indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)]) </span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices] <span class="comment"># 逐次返回</span></span><br></pre></td></tr></tbody></table></figure>

<p>利用GPU并行运算的优势，处理合理大小的“小批量”</p>
<p>每个样本都可以并行地进行模型计算，且每个样本损失函数的梯度也可以被并行计算</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(X, <span class="string">'\n'</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure>

<p>当运行迭代时，会连续地获得不同的小批量，直至遍历完整个数据集</p>
<p>但这种迭代的执行效率很低，因为需要把所有数据读入，并且执行大量的随机内存访问</p>
<p>在深度学习框架中实现的内置迭代器效率要高得多，它可以处理存储在文件中的数据和数据流提供的数据</p>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，并将偏置初始化为0</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>之后更新这些参数，直到这些参数足够拟合数据，每次更新都需要计算损失函数关于模型参数的梯度，有了这个梯度，就可以向减小损失的方向更新每个参数</p>
<p>根据之前引入的自动微分来计算梯度(<code>requires_grad=True</code>)</p>
<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>将模型的输入和参数同模型的输出关联起来</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""线性回归模型"""</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br></pre></td></tr></tbody></table></figure>

<h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>定义损失函数是重中之重，这里使用之前描述的平方损失函数</p>
<p>需要将真实值<code>y</code>的形状转换为和预测值<code>y_hat</code>的形状相同</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""平方损失"""</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line">    <span class="comment"># 这里reshape是为了避免出现广播 </span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="定义优化算法"><a href="#定义优化算法" class="headerlink" title="定义优化算法"></a>定义优化算法</h3><p>线性回归有解析解，但其实大部分模型是没有解析解的，这里利用小批量梯度下降</p>
<p>使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度，朝着减少损失的方向更新参数，该函数接受模型参数集合、学习速率和批量大小作为输入</p>
<ul>
<li>每一步更新的大小由学习速率<code>lr</code>决定</li>
<li>计算的损失是一个批量样本的总和，用批量大小(<code>batch_size</code>)来规范化步长</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""小批量随机梯度下降"""</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 以下操作不追踪梯度，优化步骤常用</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            <span class="comment"># param.grad PyTorch 自动计算得到的梯度</span></span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            param.grad.zero_()  <span class="comment"># 清除梯度累积，结尾 _ 表示这是原地操作</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>已经准备好了模型训练所有需要的要素，可以实现主要的训练过程部分了</p>
<p>将执行以下循环：</p>
<ul>
<li>初始化参数</li>
<li>重复以下训练，直到完成<ol>
<li>计算梯度$\mathbf{g} \leftarrow \partial_{(\mathbf{w},b)} \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} l(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{w}, b)$</li>
<li>更新参数$(\mathbf{w}, b) \leftarrow (\mathbf{w}, b) - \eta \mathbf{g}$</li>
</ol>
</li>
</ul>
<p>在每个迭代周期(epoch)中，使用<code>data_iter</code>函数遍历整个数据集，并将训练数据集中所有样本都使用一次(假设样本数能够被批量大小整除)</p>
<p>在该例子中迭代周期个数<code>num_epochs</code>和学习率<code>lr</code>都是超参数，分别设为3和0.03</p>
<p>设置超参数很棘手，需要通过反复试验进行调整</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span>         <span class="comment"># 学习率</span></span><br><span class="line">num_epochs = <span class="number">3</span>    <span class="comment"># 迭代次数</span></span><br><span class="line">batch_size = <span class="number">10</span>   <span class="comment"># 小样本数</span></span><br><span class="line">net = linreg      <span class="comment"># 模型函数(线性回归 y = Xw + b)</span></span><br><span class="line">loss = squared_loss   <span class="comment"># 损失函数(平方损失)</span></span><br><span class="line"><span class="comment"># 初始化随机参数</span></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>,<span class="number">1</span>), requires_grad=<span class="literal">True</span>) </span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 根据真实参数创建含噪样本</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y) <span class="comment"># net前向传播，计算预测值</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward()</span><br><span class="line">        <span class="comment"># 因为l形状是(batch_size,1)，而不是一个标量</span></span><br><span class="line">        <span class="comment"># l中的所有元素被加到一起，并以此计算关于[w,b]的梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用参数的梯度更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 性能评估不追踪梯度</span></span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f'epoch <span class="subst">{epoch + <span class="number">1</span>}</span>, loss <span class="subst">{<span class="built_in">float</span>(train_l.mean()):f}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>输出</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 0.025137</span><br><span class="line">epoch 2, loss 0.000085</span><br><span class="line">epoch 3, loss 0.000046</span><br></pre></td></tr></tbody></table></figure>

<p>对比之前设置的真实参数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f'w的估计误差: <span class="subst">{true_w - w.reshape(true_w.shape)}</span>'</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'b的估计误差: <span class="subst">{true_b - b}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w的估计误差: tensor([1.0513e-03, 6.3181e-05], grad_fn=&lt;SubBackward0&gt;)</span><br><span class="line">b的估计误差: tensor([0.0004], grad_fn=&lt;RsubBackward1&gt;)</span><br></pre></td></tr></tbody></table></figure>

<p>在机器学习中，通常不太关心恢复真正的参数，而更关心如何高度准确预测参数</p>
<p>即使是在复杂的优化问题上，随机梯度下降通常也能找到非常好的解，因为在深度网络中存在许多参数组合能够实现高度精确的预测</p>
<h3 id="练习题-1"><a href="#练习题-1" class="headerlink" title="练习题"></a>练习题</h3><ol>
<li><p>如果将权重初始化为零，会发生什么，算法仍然有效吗？</p>
<p>像线性回归这种问题，因为损失函数是凸函数，梯度在所有方向上都是对称的，不管初始点在哪，梯度下降都会沿着唯一方向走到全局最优</p>
<p>但在神经网络这种多层模型里就不行了，如果权重都初始化为0，那么所有的神经元都一样，梯度也完全一样，就会出现对称性问题，陷入死局</p>
<table>
<thead>
<tr>
<th>模型类型</th>
<th>全零初始化后果</th>
<th>原因</th>
</tr>
</thead>
<tbody><tr>
<td>线性回归</td>
<td>可行</td>
<td>损失函数是凸的，无对称问题</td>
</tr>
<tr>
<td>逻辑回归</td>
<td>可行</td>
<td>一层模型，不存在多通道对称性</td>
</tr>
<tr>
<td>神经网络(多层)</td>
<td>失败</td>
<td>神经元对称、梯度相同、学习停滞</td>
</tr>
</tbody></table>
</li>
<li><p>计算二阶导数时可能会遇到什么问题？这些问题可以如何解决？</p>
<p>只在第一次求导时指定 <code>create_graph=True</code>，PyTorch 才会保留梯度计算图，从而允许继续求导</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">3</span></span><br><span class="line">y.backward(create_graph=<span class="literal">True</span>) <span class="comment"># 保留梯度计算图</span></span><br><span class="line"><span class="built_in">print</span>(x.grad) <span class="comment"># 输出48 3x^2=48符合</span></span><br><span class="line">grad_x = x.grad.clone() <span class="comment"># 不能"="，只是赋地址，grad.zero_()清掉了</span></span><br><span class="line">x.grad.zero_() <span class="comment"># 避免梯度累加，因为我只看二阶导梯度</span></span><br><span class="line">grad_x.backward() <span class="comment"># 再次反向传播</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>为什么在<code>squared_loss</code>函数中需要使用<code>reshape</code>函数？</p>
<p><code>y_hat</code>的形状可能是 <code>(batch_size, 1)</code> 或 <code>(batch_size,)</code></p>
<p><code>y</code>的形状常常是 <code>(batch_size,)</code></p>
<p>如果直接计算可能会出现广播机制</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_hat.shape = (<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">y.shape     = (<span class="number">3</span>,)</span><br></pre></td></tr></tbody></table></figure>

<p>广播后变为(3,3)的矩阵计算，这肯定是不对的</p>
<p>显式地让 <code>y</code> 的形状与 <code>y_hat</code> 完全一致，确保预测值和真实值形状一致，从而进行逐元素平方损失计算</p>
</li>
<li><p>尝试使用不同的学习率，观察损失函数值下降的快慢</p>
<p>学习率越大，损失函数下降越快</p>
</li>
<li><p>如果样本个数不能被批量大小整除，<code>data_iter</code>函数的行为会有什么变化？</p>
<table>
<thead>
<tr>
<th>情况</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>保留不满批次</td>
<td>所有样本都用上</td>
<td>最后一批大小不一致，梯度波动略大</td>
</tr>
<tr>
<td>丢弃不满批次</td>
<td>批次形状一致，利于并行</td>
<td>一部分样本没被训练</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h2><p>由于数据迭代器、损失函数、优化器和神经网络层很常用，现代深度学习库已经实现了这些组件</p>
<p>生成数据集一般没有特殊的封装函数，毕竟大部分情况下数据都是已经获取好，读入即可</p>
<h3 id="读取数据集-1"><a href="#读取数据集-1" class="headerlink" title="读取数据集"></a>读取数据集</h3><p>可以调用PyTorch框架中现有的API来读取数据，将<code>features</code>和<code>labels</code>作为API的参数传递</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features,labels), batch_size, <span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><p>对于标准深度学习模型，可以使用框架的预定义好的层，只需关注使用哪些层来构造模型，而不必关注层的实现细节</p>
<p>首先定义一个模型变量<code>net</code>，它是一个<code>Sequential</code>类的实例</p>
<p><code>Sequential</code>类将多个层串联在一起，当给定输入数据时，<code>Sequential</code>实例将数据传入到第一层，然后将第一层的输出作为第二层的输入，以此类推</p>
<p>当前单层连接所有输入，所以为全连接层</p>
<p>在PyTorch中，全连接层在<code>Linear</code>类中定义，将两个参数传递到<code>nn.Linear</code>中</p>
<ul>
<li><p>参数1指定输入特征形状，即2</p>
</li>
<li><p>参数2指定输出特征形状，输出特征形状为单个标量，因此为1</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br></pre></td></tr></tbody></table></figure>

<h3 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>在使用<code>net</code>之前，需要初始化模型参数，如在线性回归模型中的权重和偏置</p>
<p>深度学习框架通常有预定义的方法来初始化参数</p>
<p>在这里指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样，偏置参数将初始化为零</p>
<p>通过<code>net[0]</code>选择网络中的第一个图层，然后使用<code>weight.data</code>和<code>bias.data</code>方法访问参数，还可以使用替换方法<code>normal_</code>和<code>fill_</code>来重写参数值</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="定义损失函数-1"><a href="#定义损失函数-1" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>计算均方损失使用的是<code>MSELoss</code>类，也称为均方$L_2$范数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss(reduction=<span class="string">'mean'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>参数<code>reduction</code></p>
<ul>
<li>默认<code>mean</code>，对所有元素的损失取平均值，<font color="DarkViolet">最常用，标准化损失</font>  $ \frac{1}{n} \sum_{i}\left(x_{i}-y_{i}\right)^{2}$</li>
<li><code>none</code>，不进行任何聚合，返回与输入相同形状的逐元素损失 $(x_{i}-y_{i})^{2}$</li>
<li><code>sum</code>，对所有元素的损失求和，希望损失值与样本规模成比例，常出现于最大似然  $\sum_{i}\left(x_{i}-y_{i}\right)^{2}$</li>
</ul>
<h3 id="定义优化算法-1"><a href="#定义优化算法-1" class="headerlink" title="定义优化算法"></a>定义优化算法</h3><p>小批量随机梯度下降算法是一种优化神经网络的标准工具，PyTorch在<code>optim</code>模块中实现了该算法的许多变种</p>
<p>当实例化一个<code>SGD</code>实例时，要指定优化的参数(可通过<code>net.parameters()</code>从模型中获得)以及优化算法所需的超参数字典</p>
<p>小批量随机梯度下降只需要设置<code>lr</code>值，这里设置为0.03</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><p>通过深度学习框架的高级API来实现，只需要相对较少的代码</p>
<p>不必单独分配参数、不必定义损失函数，也不必手动实现小批量随机梯度下降</p>
<p>当有了所有的基本组件，训练过程代码与从零开始实现时所做的非常相似</p>
<p>对于每一个小批量，会进行以下步骤</p>
<ol>
<li>通过调用<code>net(X)</code>生成预测并计算损失<code>l</code>(前向传播)</li>
<li>通过进行反向传播来计算梯度</li>
<li>通过调用优化器来更新模型参数</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X) ,y)</span><br><span class="line">        trainer.zero_grad() <span class="comment"># 在每个 batch 训练前清空上一次的梯度</span></span><br><span class="line">        l.backward()        <span class="comment"># 反向传播，自动计算损失 l 对所有参数的梯度</span></span><br><span class="line">        <span class="comment"># 如果要访问参数的梯度</span></span><br><span class="line">        <span class="comment"># net.weight.grad</span></span><br><span class="line">        <span class="comment"># net.bias.grad</span></span><br><span class="line">        trainer.step()      <span class="comment"># 根据刚刚算出的梯度，更新模型参数</span></span><br><span class="line">    <span class="comment"># 在每个 epoch 结束后，重新计算整个数据集上的损失</span></span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'epoch <span class="subst">{epoch + <span class="number">1</span>}</span>, loss <span class="subst">{l:f}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>输出</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 0.000252</span><br><span class="line">epoch 2, loss 0.000100</span><br><span class="line">epoch 3, loss 0.000102</span><br></pre></td></tr></tbody></table></figure>

<p>访问训练出的权重和偏置</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = net[<span class="number">0</span>].weight.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'w的估计误差：'</span>, true_w - w.reshape(true_w.shape))</span><br><span class="line">b = net[<span class="number">0</span>].bias.data</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'b的估计误差：'</span>, true_b - b)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w的估计误差： tensor([0.0014, 0.0002])</span><br><span class="line">b的估计误差： tensor([0.0005])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h3><p>所有损失函数都在 <code>torch.nn</code> 中实现为类</p>
<p>回归问题的损失函数：</p>
<table>
<thead>
<tr>
<th>损失函数</th>
<th>说明</th>
<th>默认公式</th>
</tr>
</thead>
<tbody><tr>
<td><code>nn.MSELoss()</code></td>
<td>均方误差(Mean Squared Error)</td>
<td>$\frac{1}{n}\sum(y - \hat{y})^2$</td>
</tr>
<tr>
<td><code>nn.L1Loss()</code></td>
<td>平均绝对误差(Mean Absolute Error)</td>
<td>$\frac{1}{n}\sum\mid y- \hat y \mid $</td>
</tr>
<tr>
<td><code>nn.SmoothL1Loss()</code></td>
<td>平滑版 L1，融合 L1 与 L2 优点</td>
<td>对小误差用平方<br>大误差用绝对值</td>
</tr>
<tr>
<td><code>nn.HuberLoss()</code></td>
<td>类似 SmoothL1，可设阈值 δ 控制</td>
<td>兼顾稳健性与可导性</td>
</tr>
</tbody></table>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul>
<li>在PyTorch中，<code>data</code>模块提供了数据处理工具，<code>nn</code>模块定义了大量的神经网络层和常见损失函数</li>
<li>可以通过<code>_</code>结尾的方法将参数替换，从而初始化参数</li>
</ul>
<h2 id="softmax回归的概念"><a href="#softmax回归的概念" class="headerlink" title="softmax回归的概念"></a>softmax回归的概念</h2><p><strong>分类</strong>这个词可描述两个有微妙差别的问题：</p>
<ol>
<li>只对样本的“硬性”类别感兴趣，即属于哪个类别</li>
<li>希望得到“软性”类别，即得到属于每个类别的概率</li>
</ol>
<p>这两者的界限往往很模糊，因为即使只关心硬类别，仍然使用软类别的模型</p>
<h3 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h3><p>从一个图像分类问题开始，假设每次输入是一个$2\times 2$的灰度图像，可以用一个标量表示每个像素值，每个图像对应四个特征$x_1, x_2, x_3, x_4$，假设每个图像属于类别“猫”“鸡”和“狗”中的一个</p>
<p>接下来要考虑如何表示标签，最直接的想法是选择$y \in {1, 2, 3}$来分别表示${\text{狗}, \text{猫}, \text{鸡}}$，这是在计算机上存储此类信息的有效方法，但这种方法需要类别间有一些自然顺序</p>
<p>但是一般的分类问题并不与类别之间的自然顺序有关，所以需要利用<strong>独热编码(one-hot encoding)</strong></p>
<p>独热编码是一个向量，它的分量和类别一样多，类别对应的分量设置为1，其他所有分量设置为0</p>
<p>在例子中，标签$y$将是一个三维向量，其中$(1, 0, 0)$对应于“猫”、$(0, 1, 0)$对应于“鸡”、$(0, 0, 1)$对应于“狗”<br>$$<br>y \in {(1, 0, 0), (0, 1, 0), (0, 0, 1)}.<br>$$</p>
<h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p>为了估计所有可能类别的条件概率，需要一个有多个输出的模型，每个类别对应一个输出</p>
<p>为了解决线性模型的分类问题，需要和输出一样多的仿射函数，每个输出对应于它自己的仿射函数</p>
<p>回到简单例子中，有4个特征和3个可能的输出类别，将需要12个标量来表示权重(带下标的$w$)，3个标量来表示偏置(带下标的$b$)</p>
<p>为每个输入计算三个未规范化的预测<br>$$<br>\begin{split}\begin{aligned}<br>o_1 &amp;= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\<br>o_2 &amp;= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\<br>o_3 &amp;= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.<br>\end{aligned}\end{split}<br>$$<br>和线性回归一样，softmax回归也是一个单层神经网络且输出层也是全连接层</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/softmaxreg.svg" alt="softmaxreg">

<p>为了更简洁地表达模型，仍然使用线性代数符号，通过向量形式表达为$\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$</p>
<p>权重在一个$3 \times 4$的矩阵中，对于给定数据样本的特征$\mathbf{x}$，输出是由权重与输入特征进行矩阵-向量乘法再加上偏置得到的</p>
<h3 id="全连接层的参数开销"><a href="#全连接层的参数开销" class="headerlink" title="全连接层的参数开销"></a>全连接层的参数开销</h3><p>在深度学习中，全连接层无处不在，但是全连接层可能有很多可学习的参数</p>
<p>对于任何具有$d$个输入和$q$个输出的全连接层，参数开销为$\mathcal{O}(dq)$，但通过低秩分解将矩阵拆分，引入一个中间的低维隐空间，用较少的参数去近似原始映射</p>
<p>此时参数数量从$dq$下降为$dn+nq=O(\frac{dq}{n})$(当$n\ll d,q$时)</p>
<p>其中超参数$n$可以灵活指定，但是需要平衡参数节约和模型有效性</p>
<h3 id="softmax运算"><a href="#softmax运算" class="headerlink" title="softmax运算"></a>softmax运算</h3><p>为了得到预测结果将设置一个阈值，如选择具有最大概率的标签</p>
<p>希望模型的输出$\hat{y}_j$可以视为属于类$j$的概率，然后选择具有最大输出值的类别$\operatorname*{argmax}_j y_j$作为预测</p>
<p>能否将未规范化的预测$o$直接视作我们感兴趣的输出呢？这肯定是不行的，将线性层的输出直接视为概率时存在一些问题：</p>
<ul>
<li>没有限制输出总和为1</li>
<li>根据输入不同，输出可能为负值</li>
</ul>
<p>这已经违反了概率基本公理</p>
<p>要将输出视为概率，必须保证在任何数据上的输出都是非负的且总和为1</p>
<p>此外，需要一个训练的目标函数，来激励模型精准地估计概率</p>
<p>比如在分类器输出0.5的所有样本中，希望这些样本是刚好有一半实际上属于预测的类别，这个属性叫做<strong>校准(calibration)</strong></p>
<p>在选择模型理论基础上发明的softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质</p>
<p><font color="Violetred">首先对每个未规范化的预测求幂(这样可以确保输出非负)，再让每个求幂后的结果除以它们的总和</font><br>$$<br>\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}<br>$$<br>这样$\hat{\mathbf{y}}$可以视为一个正确的概率分布</p>
<p>softmax运算不会改变未规范化的预测之间的大小次序，只会确定分配给每个类别的概率，所以仍然可以用下式来选择最有可能的类别<br>$$<br>\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.<br>$$</p>
<h3 id="小批量样本的矢量化"><a href="#小批量样本的矢量化" class="headerlink" title="小批量样本的矢量化"></a>小批量样本的矢量化</h3><p>假设读取了一个批量的样本$\mathbf{X}$，其中特征维度(输入数量)为$d$，批量大小为$n$，并假设在输出中有$q$个类别</p>
<p>那么小批量样本的特征为$\mathbf{X} \in \mathbb{R}^{n \times d}$，权重为$\mathbf{W} \in \mathbb{R}^{d \times q}$，偏置为$\mathbf{b} \in \mathbb{R}^{1\times q}$</p>
<p>softmax回归的矢量计算表达式为：<br>$$<br>\begin{split}\begin{aligned} \mathbf{O} &amp;= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} &amp; = \mathrm{softmax}(\mathbf{O}). \end{aligned}\end{split}<br>$$<br>相对于一次处理一个样本，小批量样本的矢量化加快了$\mathbf{X},\mathbf{W}$的矩阵-向量乘法</p>
<p>由于$\mathbf{X}$中的每一行代表一个数据样本，那么softmax运算可以按行执行</p>
<p>这里的偏置依旧会触发广播机制，小批量的未规范化预测$\mathbf{O}$和输出概率$\hat{\mathbf{Y}}$都是形状为$n \times q$的矩阵</p>
<h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>需要一个损失函数来度量预测的效果，将使用最大似然估计，这与在线性回归中的方法相同</p>
<h4 id="对数似然"><a href="#对数似然" class="headerlink" title="对数似然"></a>对数似然</h4><p>softmax函数给出了一个向量$\hat{\mathbf{y}}$，可以将其视为“对给定任意输入$\mathbf{x}$的每个类的条件概率”</p>
<p>假设整个数据集${\mathbf{X}, \mathbf{Y}}$具有$n$个样本，其中索引$i$的样本由特征向量$\mathbf{x}^{(i)}$和独热标签向量$\mathbf{y}^{(i)}$组成</p>
<p>可以将估计值与实际值进行比较<br>$$<br>P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).<br>$$<br>根据最大似然估计，最大化$P(\mathbf{Y} \mid \mathbf{X})$相当于最小化负对数似然<br>$$<br>-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})<br>= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),<br>$$<br>其中，对于任何标签$\mathbf{y}$和模型预测$\hat{\mathbf{y}}$，损失函数为<br>$$<br>l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.<br>$$<br>通常被称为<strong>交叉熵损失(cross-entropy loss)</strong></p>
<ul>
<li>由于$\mathbf{y}$是一个长度为$q$的独热编码向量，所以除了一个项以外的所有项$j$都消失了</li>
<li>由于所有$\hat{y}_j$都是预测的概率，所以对数永远不会大于0</li>
</ul>
<p>如果正确预测实际标签，即$P(\mathbf{y} \mid \mathbf{x})=1$，则损失函数不能进一步最小化，但是基本不会出现</p>
<p>因为数据集中可能存在标签噪声(比如某些样本可能被误标)</p>
<h4 id="softmax及其导数"><a href="#softmax及其导数" class="headerlink" title="softmax及其导数"></a>softmax及其导数</h4><p>利用softmax的定义得到<br>$$<br>\begin{split}\begin{aligned}<br>l(\mathbf{y}, \hat{\mathbf{y}}) &amp;=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\<br>&amp;= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\<br>&amp;= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.<br>\end{aligned}\end{split}<br>$$<br>考虑相对于任何未规范化的预测$o_j$的导数得到<br>$$<br>\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j = \hat y_j-y_j<br>$$<br>发现导数是softmax模型分配的概率与实际发生的情况(由独热标签向量表示)之间的差异</p>
<p>这与之前在回归中看到的非常相似，其中梯度是观测值与预估值之间的差异</p>
<p>这不是巧合，<font color="DarkViolet">在任何指数族分布模型中，对数似然的梯度正是由此得出的</font></p>
<h4 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a>交叉熵损失</h4><p>对于标签$\mathbf{y}$可以使用与以前相同的表示形式，唯一的区别是现在用一个概率向量表示，而不是仅包含二元项的向量</p>
<p>使用交叉熵来定义损失<br>$$<br>\begin{align*}<br>l(\mathbf{y}, \hat{\mathbf{y}})<br>= -\sum_{j=1}^q y_j \log \hat y_j<br>= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j<br>\end{align*}<br>$$<br>它是所有标签分布的预期损失值，<font color="DarkViolet">是分类问题最常用的损失之一</font></p>
<h3 id="从信息论看交叉熵"><a href="#从信息论看交叉熵" class="headerlink" title="从信息论看交叉熵"></a>从信息论看交叉熵</h3><h4 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h4><p>信息论的核心思想是量化数据中的信息内容，该数值被称为分布的<strong>熵(entropy)</strong><br>$$<br>H[P] = \sum_j - P(j) \log P(j).<br>$$<br>信息论的基本定理之一指出，为了对从分布$p$中随机抽取的数据进行编码，至少需要“$H[P]$纳特(nat)”对其进行编码</p>
<p>注意，这里的 “纳特”相当于比特(bit)，但是对数底为$e$而非2，1纳特约为1.44比特<br>$$<br>\frac{1}{\log(2)} \approx 1.44<br>$$</p>
<h4 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h4><p>如果一个数据流很容易预测，那它就很容易压缩</p>
<p><font color="Violetred">信息量小 = 可预测性强 = 可压缩性高</font></p>
<p>如果不能完全预测每一个事件，有时可能会感到“惊异”，克劳德·香农决定用信息量来量化这种惊异程度</p>
<p>$$<br>\log \frac{1}{P(j)} = -\log P(j)<br>$$<br>在观察一个事件$j$时赋予它主观概率$P(j)$，概率越低惊异会更大，该事件的信息量也就更大</p>
<p><font color="Violetred">当考虑所有可能事件时，信息量的期望就是熵</font></p>
<h4 id="再看交叉熵"><a href="#再看交叉熵" class="headerlink" title="再看交叉熵"></a>再看交叉熵</h4><p>可以把交叉熵想象为“主观概率为$Q$的观察者在看到根据概率$P$生成的数据时的预期惊异”</p>
<p>当$P=Q$时，交叉熵达到最低，在这种情况下，从$P$到$Q$的交叉熵是$H(P, P)= H(P)$，没有额外的惊讶</p>
<p>可以从两方面来考虑交叉熵分类目标：</p>
<ol>
<li>最大化观测数据的似然</li>
<li>最小化传达标签所需的惊异(压缩)</li>
</ol>
<h3 id="练习题-2"><a href="#练习题-2" class="headerlink" title="练习题"></a>练习题</h3><p>计算softmax交叉熵损失的二阶导数，并计算$\mathrm{softmax}(\mathbf{o})$给出的分布方差，并与上面计算的二阶导数匹配</p>
<p>对梯度再求一次导得到<br>$$<br>\frac{\partial^2 l}{\partial o_i \partial o_j}=\frac{\partial}{\partial o_j}\left(\hat y_i-y_i\right)=\frac{\partial \hat y_i}{\partial o_j}<br>$$<br>Softmax的导数公式是<br>$$<br>\frac{\partial \hat y_i}{\partial o_j}=\hat y_i\left(\delta_{i j}-\hat y_j\right)<br>$$<br>所以二阶导矩阵(Hessian)为<br>$$<br>H_{i j}=\hat y_i\left(\delta_{i j}-\hat y_j\right)<br>$$<br>即<br>$$<br>H=\operatorname{diag}(\hat{\mathbf{y}})-\hat{\mathbf{y}} \hat{\mathbf{y}}^{\top}<br>$$<br>对于一个类别分布(多项分布)的协方差<br>$$<br>\operatorname{Cov}[\hat{\mathbf{y}}]=\operatorname{diag}(\hat{\mathbf{y}})-\hat{\mathbf{y}} \hat{\mathbf{y}}^{\top}<br>$$<br>会发现这正好和上面的 Hessian 完全一样！</p>
<h2 id="图像分类数据集"><a href="#图像分类数据集" class="headerlink" title="图像分类数据集"></a>图像分类数据集</h2><p>MNIST数据集(LeCun <em>et al.</em>, 1998)是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单，将使用类似但更复杂的Fashion-MNIST数据集 (Xiao <em>et al.</em>, 2017)</p>
<p>Fashion-MNIST由10个类别的图像组成，每个类别由训练数据集中的6000张图像和测试数据集中的1000张图像组成，因此训练集和测试集分别包含60000和10000张图像</p>
<p>先导入一部分包</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="comment"># 图像数据 + 图像模型 + 图像处理</span></span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data <span class="comment"># 数据加载接口</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="comment"># 图像数据预处理与增强</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="常用数据集"><a href="#常用数据集" class="headerlink" title="常用数据集"></a>常用数据集</h3><p>分类任务数据集(Image Classification)</p>
<table>
<thead>
<tr>
<th>数据集名称</th>
<th>用途</th>
</tr>
</thead>
<tbody><tr>
<td><strong>MNIST</strong></td>
<td>手写数字识别，灰度图(28×28)</td>
</tr>
<tr>
<td><strong>FashionMNIST</strong></td>
<td>服装分类任务，灰度图(28×28)</td>
</tr>
<tr>
<td><strong>CIFAR10</strong></td>
<td>10类彩色小图(32×32)，经典视觉分类</td>
</tr>
<tr>
<td><strong>CIFAR100</strong></td>
<td>CIFAR10 的扩展(100 类)</td>
</tr>
<tr>
<td><strong>ImageNet</strong></td>
<td>大规模分类基准，1000 类</td>
</tr>
<tr>
<td><strong>STL10</strong></td>
<td>类似 CIFAR，但图像更高分辨率(96×96)</td>
</tr>
<tr>
<td><strong>SVHN</strong></td>
<td>街景数字识别(彩色数字)</td>
</tr>
<tr>
<td><strong>Caltech101 / 256</strong></td>
<td>多种物体类别图像</td>
</tr>
</tbody></table>
<h3 id="读取数据集-2"><a href="#读取数据集-2" class="headerlink" title="读取数据集"></a>读取数据集</h3><p>可以通过框架中的内置函数将Fashion-MNIST数据集下载并读取到内存中</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">trans = transforms.ToTensor()</span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">"data"</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">"data"</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>

<p><code>transforms.ToTensor()</code>：把一张PIL图片(NumPy数组)转换成张量(Tensor)，并把像素值从<code>[0, 255]</code>映射到<code>[0, 1]</code>之间，图像的维度从<code>(H, W, C)</code>变为<code>(C, H, W)</code></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mnist_train[<span class="number">0</span>][<span class="number">0</span>].shape <span class="comment"># torch.Size([1, 28, 28])</span></span><br></pre></td></tr></tbody></table></figure>

<p>Fashion-MNIST中包含的10个类别，以下函数用于在数字标签索引及其文本名称之间进行转换</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""返回Fashion-MNIST数据集的文本标签"""</span></span><br><span class="line">    text_labels = [<span class="string">'t-shirt'</span>, <span class="string">'trouser'</span>, <span class="string">'pullover'</span>, <span class="string">'dress'</span>, <span class="string">'coat'</span>,</span><br><span class="line">                   <span class="string">'sandal'</span>, <span class="string">'shirt'</span>, <span class="string">'sneaker'</span>, <span class="string">'bag'</span>, <span class="string">'ankle boot'</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br></pre></td></tr></tbody></table></figure>

<p>现在可以创建一个函数来可视化这些样本</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""绘制图像列表"""</span></span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(img):</span><br><span class="line">            <span class="comment"># 图片张量</span></span><br><span class="line">            ax.imshow(img.numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># PIL图片</span></span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axis(<span class="string">'off'</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iter() 把它变成迭代器</span></span><br><span class="line"><span class="comment"># next() 取出其中的第一批数据</span></span><br><span class="line">X, y = <span class="built_in">next</span>(<span class="built_in">iter</span>(data.DataLoader(mnist_train, batch_size=<span class="number">18</span>))) </span><br><span class="line"><span class="comment"># 默认情况下X的形状是[18,1,28,28]，但是现在不需要通道1</span></span><br><span class="line">show_images(X.reshape(<span class="number">18</span>, <span class="number">28</span>, <span class="number">28</span>), <span class="number">2</span>, <span class="number">9</span>, titles=get_fashion_mnist_labels(y));</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510152255.png" alt="202510152255" style="zoom: 50%;">

<h3 id="读取小批量"><a href="#读取小批量" class="headerlink" title="读取小批量"></a>读取小批量</h3><p>通过内置数据迭代器，可以随机打乱所有样本，并在每次迭代中读取一小批量数据，大小为<code>batch_size</code></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""使用4个进程来读取数据"""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">train_iter = data.DataLoader(</span><br><span class="line">    mnist_train,          <span class="comment"># 数据集</span></span><br><span class="line">    batch_size,           <span class="comment"># 每批样本数量</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,         <span class="comment"># 打乱数据顺序(提高泛化性)</span></span><br><span class="line">    num_workers=get_dataloader_workers()  <span class="comment"># 并行加载线程数</span></span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>

<p>利用之前定义的定时器类来计时</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">timer = Timer()</span><br><span class="line"><span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"<span class="subst">{timer.stop():<span class="number">.2</span>f}</span> seconds"</span>) </span><br><span class="line"><span class="comment"># 4.32 seconds</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="整合所有组件"><a href="#整合所有组件" class="headerlink" title="整合所有组件"></a>整合所有组件</h3><p>定义<code>load_data_fashion_mnist</code>函数，用于获取和读取Fashion-MNIST数据集</p>
<p>这个函数返回训练集和验证集的数据迭代器</p>
<p>此外这个函数还接受一个可选参数<code>resize</code>，用来将图像大小调整为另一种形状</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""使用4个进程来读取数据"""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""下载Fashion-MNIST数据集，然后将其加载到内存中"""</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize: <span class="comment"># 如果传入了 resize 参数，先进行一个图像尺寸调整操作</span></span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans) <span class="comment"># 把多个图像变换组合成一个整体</span></span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">"data"</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">"data"</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 返回两个加载器</span></span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_iter, test_iter = load_data_fashion_mnist(<span class="number">32</span>, resize=<span class="number">64</span>)</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape, X.dtype, y.shape, y.dtype)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64</span><br></pre></td></tr></tbody></table></figure>

<p>如果减少<code>batch_size</code>(如减少到1)是否会影响读取性能？</p>
<p>当然，会让 I/O 频率变高、CPU 多进程效率下降、GPU 并行利用率变差</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">timer = Timer()</span><br><span class="line"><span class="keyword">for</span> bs <span class="keyword">in</span> [<span class="number">1</span>,<span class="number">16</span>,<span class="number">64</span>,<span class="number">256</span>]:</span><br><span class="line">    timer.start()</span><br><span class="line">    train_iter = data.DataLoader(mnist_train, batch_size=bs, shuffle=<span class="literal">True</span>,num_workers=get_dataloader_workers())</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    total_time = timer.stop()  <span class="comment"># 得到这次的总时间</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"batch size: <span class="subst">{bs}</span>, total time: <span class="subst">{total_time:<span class="number">.2</span>f}</span>"</span>)</span><br></pre></td></tr></tbody></table></figure>

<h2 id="softmax回归的底层实现"><a href="#softmax回归的底层实现" class="headerlink" title="softmax回归的底层实现"></a>softmax回归的底层实现</h2><p>使用刚刚引入的Fashion-MNIST数据集，并设置数据迭代器的批量大小为256</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="初始化模型参数-2"><a href="#初始化模型参数-2" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>现在暂时只把每个像素位置看作一个特征，原始数据集中的每个样本都是$28 \times 28$的图像，展平每个图像看作长度为784的向量</p>
<p>在softmax回归中，输出与类别一样多，所以网络输出维度为10</p>
<p>权重构建为$784 \times 10$的矩阵，偏置将构成一个$1 \times 10$的行向量，将使用正态分布初始化权重，偏置初始化为0</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="定义softmax"><a href="#定义softmax" class="headerlink" title="定义softmax"></a>定义softmax</h3><p>当调用<code>sum</code>运算符时，可以指定保持在原始张量的轴数，而不折叠求和的维度</p>
<p>实现softmax由三个步骤组成：</p>
<ol>
<li>对每个项求幂(使用<code>exp</code>)</li>
<li>对每一行求和(小批量中每个样本是一行)，得到每个样本的规范化常数</li>
<li>将每一行除以其规范化常数，确保结果的和为1</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></tbody></table></figure>

<p>对于任何随机输入，将每个元素变成一个非负数，并且依据概率原理，每行总和为1</p>
<p>虽然这在数学上看起来是正确的，但在代码实现中有点草率，矩阵中的非常大或非常小的元素可能造成数值上溢或下溢</p>
<blockquote>
<p>因为计算机中并不是用“数学上的实数”，而是用<strong>浮点数</strong>，可表示范围有限，如果x很大或者很小，分子分母都可能出现inf，导致出现NaN，这对于训练神经网络是灾难</p>
</blockquote>
<p>如何解决这个问题呢？</p>
<p>对 softmax 的每一行向量减去它的最大值，不影响结果，却能避免溢出</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_max = X.<span class="built_in">max</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>).values</span><br><span class="line">    X_exp = torch.exp(X - X_max)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp / partition</span><br></pre></td></tr></tbody></table></figure>

<p>$$<br>\operatorname{softmax}\left(x_{i}\right)=\frac{e^{x_{i}}}{\sum_{j} e^{x_{j}}}=\frac{e^{x_{i}-\max (x)}}{\sum_{j} e^{x_{j}-\max (x)}}<br>$$</p>
<p>数值上完全等价，但避免了 <code>exp(大数)</code> 的爆炸</p>
<h3 id="定义模型-2"><a href="#定义模型-2" class="headerlink" title="定义模型"></a>定义模型</h3><p>神经网络要求输入是二维的 <code>(batch_size, feature_dim)</code></p>
<p>将数据传递到模型之前，使用<code>reshape</code>函数将每张原始图像展平为向量</p>
<p>定义输入如何通过网络映射到输出<br>$$<br>\color{purple} {\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b<br>$$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape((-<span class="number">1</span>, W.shape[<span class="number">0</span>])), W) + b)</span><br><span class="line"><span class="comment"># X从 (batch_size, 1, 28, 28) 被展平为 (batch_size, W.shape[0])</span></span><br><span class="line"><span class="comment"># 点乘后矩阵大小为(batch_size, 10)</span></span><br><span class="line"><span class="comment"># b被广播为(batch_size, 10)</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="定义损失函数-2"><a href="#定义损失函数-2" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><p>引入交叉熵损失函数，深度学习中最常见的损失函数，因为目前分类问题的数量远远超过回归问题的数量</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y]) <span class="comment"># 批量索引y_hat[i, y[i]]</span></span><br></pre></td></tr></tbody></table></figure>

<blockquote>
<p>这里也会出现一个定义问题，假如<code>y_hat</code>(预测概率)非常接近 0，会因为浮点误差导致变成0，出现inf</p>
</blockquote>
<p>可以给log加一个小常数避免数值溢出</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    eps = <span class="number">1e-8</span></span><br><span class="line">    <span class="keyword">return</span> -torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y] + eps)</span><br></pre></td></tr></tbody></table></figure>

<p>这里用到多维索引，一次性指定所有维度的索引，不需要逐层访问</p>
<h3 id="分类精度"><a href="#分类精度" class="headerlink" title="分类精度"></a>分类精度</h3><p>给定预测概率分布<code>y_hat</code>，必须输出硬预测(hard prediction)时，通常选择预测概率最高的类，分类精度即正确预测数量与总预测数量之比</p>
<p>虽然直接优化精度可能很困难(因为精度的计算不可导)，但精度通常是最关心的性能衡量标准</p>
<p>如果<code>y_hat</code>是矩阵，那么假定第二个维度存储每个类的预测分数，使用<code>argmax</code>获得每行中最大元素的索引来获得预测类别，然后将预测类别与真实<code>y</code>元素进行比较</p>
<p>结果是一个包含0(错)和1(对)的张量，最后求和会得到正确预测的数量</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""计算预测正确的数量"""</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>: <span class="comment"># 判断是否是多分类概率输出</span></span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>) <span class="comment"># 取出预测概率最大的类别索引</span></span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y   <span class="comment"># == 保证数据类型一致</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())  <span class="comment"># 把bool转为数值求sum，转为float方便后续</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.9</span>, <span class="number">0.0</span>], [<span class="number">0.8</span>, <span class="number">0.1</span>, <span class="number">0.1</span>], [<span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.4</span>]])</span><br><span class="line">y = torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">accuracy(y_hat, y)  <span class="comment"># 输出3.0，因为都匹配上了</span></span><br></pre></td></tr></tbody></table></figure>

<p>对于任意数据迭代器<code>data_iter</code>可访问的数据集，可以评估在任意模型<code>net</code>的精度</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""在n个变量上累加"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>): <span class="comment"># n表示多少个变量</span></span><br><span class="line">        <span class="variable language_">self</span>.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):  <span class="comment"># *args 允许同时传入多个值，但个数必须等于n</span></span><br><span class="line">        <span class="comment"># zip(self.data, args) 会把当前累计值与新输入配对相加</span></span><br><span class="line">        <span class="variable language_">self</span>.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>): <span class="comment"># 配置索引访问</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""计算在指定数据集上模型的精度"""</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module): <span class="comment"># 判断 net 是否是 PyTorch 模型</span></span><br><span class="line">        net.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式，数值更稳定</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 正确预测数、预测总数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 禁用梯度计算</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="comment"># 累积预测正确数量与样本总数</span></span><br><span class="line">            metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure>

<h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><p>对于优化函数，使用之前定义的小批量随机梯度下降来优化模型的损失函数，设置学习率为0.1</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""小批量随机梯度下降"""</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 以下操作不追踪梯度，优化步骤常用</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            <span class="comment"># param.grad PyTorch 自动计算得到的梯度</span></span><br><span class="line">            param -= lr * param.grad / batch_size</span><br><span class="line">            param.grad.zero_()  <span class="comment"># 清除梯度累积，结尾 _ 表示这是原地操作</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练-2"><a href="#训练-2" class="headerlink" title="训练"></a>训练</h3><p>首先定义一个函数来训练一个迭代周期，<code>updater</code>是更新模型参数的常用函数，它接受批量大小作为参数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""训练模型一个迭代周期"""</span></span><br><span class="line">    <span class="comment"># 将模型设置为训练模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    <span class="comment"># 训练损失总和、训练准确度总和、样本数</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="comment"># 计算梯度并更新参数</span></span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            <span class="comment"># 使用PyTorch内置的优化器和损失函数</span></span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用定制的优化器和损失函数</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater(X.shape[<span class="number">0</span>])</span><br><span class="line">        metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="comment"># 返回平均训练损失和训练精度</span></span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br></pre></td></tr></tbody></table></figure>

<p>定义一个在动画中绘制数据的类<code>Animator</code></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Animator</span>:</span><br><span class="line">    <span class="string">"""在动画中绘制数据"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 xlim=<span class="literal">None</span>, ylim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 xscale=<span class="string">'linear'</span>, yscale=<span class="string">'linear'</span>,</span></span><br><span class="line"><span class="params">                 fmts=(<span class="params"><span class="string">'-'</span>, <span class="string">'m--'</span>, <span class="string">'g-.'</span>, <span class="string">'r:'</span></span>),</span></span><br><span class="line"><span class="params">                 nrows=<span class="number">1</span>, ncols=<span class="number">1</span>, figsize=(<span class="params"><span class="number">8</span>, <span class="number">6</span></span>)</span>):</span><br><span class="line">        <span class="comment"># 创建绘图窗口</span></span><br><span class="line">        <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            legend = []</span><br><span class="line">        <span class="variable language_">self</span>.fig, <span class="variable language_">self</span>.axes = plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 若只有一个子图，统一处理为列表</span></span><br><span class="line">        <span class="keyword">if</span> nrows * ncols == <span class="number">1</span>:</span><br><span class="line">            <span class="variable language_">self</span>.axes = [<span class="variable language_">self</span>.axes]</span><br><span class="line">        <span class="variable language_">self</span>.X, <span class="variable language_">self</span>.Y, <span class="variable language_">self</span>.fmts = <span class="literal">None</span>, <span class="literal">None</span>, fmts</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 坐标轴设置函数</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">set_axes</span>():</span><br><span class="line">            ax = <span class="variable language_">self</span>.axes[<span class="number">0</span>]</span><br><span class="line">            ax.set_xlabel(xlabel)</span><br><span class="line">            ax.set_ylabel(ylabel)</span><br><span class="line">            ax.set_xscale(xscale)</span><br><span class="line">            ax.set_yscale(yscale)</span><br><span class="line">            <span class="keyword">if</span> xlim: ax.set_xlim(xlim)</span><br><span class="line">            <span class="keyword">if</span> ylim: ax.set_ylim(ylim)</span><br><span class="line">            <span class="keyword">if</span> legend: ax.legend(legend)</span><br><span class="line">            ax.grid(<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.config_axes = set_axes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="string">"""增量绘制多条曲线"""</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(y, <span class="string">"__len__"</span>):</span><br><span class="line">            y = [y]</span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(x, <span class="string">"__len__"</span>):</span><br><span class="line">            x = [x] * n</span><br><span class="line">        <span class="comment"># 初始化 X, Y 容器</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.X:</span><br><span class="line">            <span class="variable language_">self</span>.X = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.Y:</span><br><span class="line">            <span class="variable language_">self</span>.Y = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="comment"># 添加数据点</span></span><br><span class="line">        <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x, y)):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="variable language_">self</span>.X[i].append(a)</span><br><span class="line">                <span class="variable language_">self</span>.Y[i].append(b)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">show</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""训练结束后绘制最终曲线"""</span></span><br><span class="line">        <span class="variable language_">self</span>.axes[<span class="number">0</span>].cla()</span><br><span class="line">        <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.X, <span class="variable language_">self</span>.Y, <span class="variable language_">self</span>.fmts):</span><br><span class="line">            <span class="variable language_">self</span>.axes[<span class="number">0</span>].plot(x, y, fmt)</span><br><span class="line">        <span class="variable language_">self</span>.config_axes()</span><br><span class="line">        plt.show()</span><br></pre></td></tr></tbody></table></figure>

<p>实现一个训练函数，它会在<code>train_iter</code>访问到的训练数据集上训练一个模型<code>net</code></p>
<p>该训练函数将会运行多个迭代周期，在每个迭代周期结束时，利用<code>test_iter</code>访问到的测试数据集对模型进行评估，最后利用<code>Animator</code>类来可视化训练过程</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""训练模型"""</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">'epoch'</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>],</span><br><span class="line">                        legend=[<span class="string">'train loss'</span>, <span class="string">'train acc'</span>, <span class="string">'test acc'</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        train_loss, train_acc = train_metrics</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>:&gt;2d}</span>/<span class="subst">{num_epochs}</span>: "</span></span><br><span class="line">              <span class="string">f"loss=<span class="subst">{train_loss:<span class="number">.4</span>f}</span>, train_acc=<span class="subst">{train_acc:<span class="number">.3</span>f}</span>, test_acc=<span class="subst">{test_acc:<span class="number">.3</span>f}</span>"</span>)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, train_metrics + (test_acc,))</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Final loss <span class="subst">{train_loss:<span class="number">.3</span>f}</span>, train acc <span class="subst">{train_acc:<span class="number">.3</span>f}</span>, test acc <span class="subst">{test_acc:<span class="number">.3</span>f}</span>"</span>)</span><br><span class="line">    animator.show()  <span class="comment"># 在最后绘图</span></span><br></pre></td></tr></tbody></table></figure>

<p>启动代码部分</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br></pre></td></tr></tbody></table></figure>

<p>输出结果</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Final loss 0.448, train acc 0.847, test acc 0.830</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510171327.webp" alt="202510171327" style="zoom:67%;">

<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>现在训练已经完成，模型已经准备好对图像进行分类预测</p>
<p>给定一系列图像将比较它们的实际标签(文本输出的第一行)和模型预测(文本输出的第二行)</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch3</span>(<span class="params">net, test_iter, n=<span class="number">6</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""预测标签"""</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    trues = get_fashion_mnist_labels(y)</span><br><span class="line">    preds = get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>))</span><br><span class="line">    titles = [true +<span class="string">'\n'</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(trues, preds)]</span><br><span class="line">    show_images(</span><br><span class="line">        X[<span class="number">0</span>:n].reshape((n, <span class="number">28</span>, <span class="number">28</span>)), <span class="number">1</span>, n, titles=titles[<span class="number">0</span>:n])</span><br><span class="line"></span><br><span class="line">predict_ch3(net, test_iter)  <span class="comment"># 刚刚已经训练好模型的w和b了，直接调用即可</span></span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510171334.webp" alt="202510171334" style="zoom: 80%;">

<p>训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型，大多数常见的深度学习模型都有类似的训练过程</p>
<h3 id="练习题-3"><a href="#练习题-3" class="headerlink" title="练习题"></a>练习题</h3><ol>
<li><p>返回概率最大的分类标签总是最优解吗？例如，医疗诊断场景下可以这样做吗？</p>
<p>softmax 只考虑“哪个更可能”，却不知道“这个决策带来的后果”，所以这些领域往往要引入<strong>阈值决策(decision threshold)</strong>，而不是简单地取 <code>argmax</code></p>
</li>
<li><p>假设使用softmax回归来预测下一个单词，可选取的单词数目过多可能会带来哪些问题?</p>
<table>
<thead>
<tr>
<th>问题类别</th>
<th>产生原因</th>
<th>后果</th>
<th>解决方法</th>
</tr>
</thead>
<tbody><tr>
<td>计算量爆炸</td>
<td>词表太大，每次都要算所有词的 exp</td>
<td>训练极慢</td>
<td>Hierarchical Softmax, Sampled Softmax</td>
</tr>
<tr>
<td>数值不稳定</td>
<td>exp(大数) 溢出, exp(小数) 下溢</td>
<td>NaN、梯度消失</td>
<td>减最大值、Log-Softmax</td>
</tr>
<tr>
<td>内存消耗高</td>
<td>最后一层参数量随词表线性增长</td>
<td>显存不足</td>
<td>参数分片、低秩分解、稀疏更新</td>
</tr>
<tr>
<td>模型不平衡</td>
<td>高频词梯度主导</td>
<td>泛化差</td>
<td>采样修正、词频平衡</td>
</tr>
</tbody></table>
</li>
</ol>
<h2 id="softmax回归的简洁实现"><a href="#softmax回归的简洁实现" class="headerlink" title="softmax回归的简洁实现"></a>softmax回归的简洁实现</h2><p>通过深度学习框架的高级API也能更方便地实现softmax回归模型，继续使用Fashion-MNIST数据集，并保持批量大小为256</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="初始化模型参数-3"><a href="#初始化模型参数-3" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>softmax回归的输出层是一个全连接层，为了实现模型，只需在<code>Sequential</code>中添加一个带有10个输出的全连接层，仍然以均值0和标准差0.01随机初始化权重</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PyTorch不会隐式地调整输入的形状</span></span><br><span class="line"><span class="comment"># 因此在线性层前定义了展平层(flatten)，来调整网络输入的形状</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="softmax的实现"><a href="#softmax的实现" class="headerlink" title="softmax的实现"></a>softmax的实现</h3><p>在底层实现时考虑到可以从所有$o_k$中减去$\max(o_k)$避免上溢，这是可行的，但是可能$o_j - \max(o_k)$具有较大的负值，$\exp(o_j - \max(o_k))$可能会出现下溢的情况，使得$\log(\hat y_j)$为-inf</p>
<p>通过将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰的数值稳定性问题<br>$$<br>\begin{split}\begin{aligned}<br>\log{(\hat y_j)} &amp; = \log\left( \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}\right) \\<br>&amp; = \log{(\exp(o_j - \max(o_k)))}-\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)} \\<br>&amp; = o_j - \max(o_k) -\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)}.<br>\end{aligned}\end{split}<br>$$<br>这样就没有将softmax概率传递到损失函数中，而是在交叉熵损失函数中传递未规范化的预测，在同一过程中同时得到 softmax 和它的对数，以避免数值溢出并提高计算效率</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">'none'</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="优化算法-1"><a href="#优化算法-1" class="headerlink" title="优化算法"></a>优化算法</h3><p>使用学习率为0.1的小批量随机梯度下降作为优化算法</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练-3"><a href="#训练-3" class="headerlink" title="训练"></a>训练</h3><p>训练函数仍用之前定义的</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></tbody></table></figure>

<p>和以前一样，这个算法使结果收敛到一个相当高的精度，而且这次的代码比之前更精简了</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Final loss 0.448, train acc 0.849, test acc 0.811</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510171427.webp" alt="202510171427" style="zoom:67%;">

<p>在图上会发现一个现象，第10轮的精度比第9轮的下降了，意味着模型已经开始拟合噪声，而非通用特征，也就是常说的<strong>过拟合(overfitting)</strong></p>
<p>在这个例子中Fashion-MNIST并不是很复杂，使用的模型参数却有784个，网络太过复杂化，就容易陷入过拟合</p>
<p>过拟合的解决在多层感知机部分讲解</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://yhblogs.cn">今天睡够了吗</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://yhblogs.cn/posts/31940.html">http://yhblogs.cn/posts/31940.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yhblogs.cn" target="_blank">がんばろう</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E2%8C%A8%EF%B8%8Fpython/">⌨️python</a></div><div class="post_share"><div class="social-share" data-image="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-1qpqrw_1280x720.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/9999.html" title="多层感知机"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-21yzzx_1280x720.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">多层感知机</div></div></a></div><div class="next-post pull-right"><a href="/posts/48513.html" title="深度学习预备知识"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-og3zqp_1280x720.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">深度学习预备知识</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/30698.html" title="BERT_Pytorch"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7jjyd9_2560x1440.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-09</div><div class="title">BERT_Pytorch</div></div></a></div><div><a href="/posts/31208.html" title="FunRec 推荐系统_精排模型"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7j931e_1280x720_(1) (1).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-18</div><div class="title">FunRec 推荐系统_精排模型</div></div></a></div><div><a href="/posts/24333.html" title="FunRec推荐系统_召回模型"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-vpp725_1280x720_(1).webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-14</div><div class="title">FunRec推荐系统_召回模型</div></div></a></div><div><a href="/posts/58676.html" title="Leetcode100记录"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-9ozdyx_1280x720.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-26</div><div class="title">Leetcode100记录</div></div></a></div><div><a href="/posts/22642.html" title="windows安装ROCm"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/ROCm_logo.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-10</div><div class="title">windows安装ROCm</div></div></a></div><div><a href="/posts/3865533702.html" title="pyqt5简单实践"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071521231.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-28</div><div class="title">pyqt5简单实践</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/b_2a1aef95f351a5f7ef72eb81e6838fd6.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">今天睡够了吗</div><div class="author-info__description">相遇是最小单位的奇迹</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071549233.webp" target="_blank" title="QQ"><i class="iconfont icon-QQ"></i></a><a class="social-icon" href="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071549234.webp" target="_blank" title="微信"><i class="iconfont icon-weixin"></i></a><a class="social-icon" href="https://space.bilibili.com/277953459?spm_id_from=333.1007.0.0" target="_blank" title="bilibili"><i class="iconfont icon-bilibili"></i></a><a class="social-icon" href="https://github.com/YaoHui-Wu06022" target="_blank" title="Github"><i class="iconfont icon-GitHub"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">保持理智，相信明天</div><div class="twopeople"><div class="twopeople"><div class="container" style="height:200px;"><canvas class="illo" width="800" height="800" style="max-width: 200px; max-height: 200px; touch-action: none; width: 640px; height: 640px;"></canvas></div> <script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople1.js"></script> <script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/zdog.dist.js"></script> <script id="rendered-js" src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople.js"></script> <style>.twopeople{margin:0;align-items:center;justify-content:center;text-align:center}canvas{display:block;margin:0 auto;cursor:move}</style></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E5%85%83%E7%B4%A0"><span class="toc-number">1.1.</span> <span class="toc-text">基本元素</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.1.</span> <span class="toc-text">线性模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.2.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90%E8%A7%A3"><span class="toc-number">1.1.3.</span> <span class="toc-text">解析解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8DSGD"><span class="toc-number">1.1.4.</span> <span class="toc-text">随机梯度下降SGD</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A2%E9%87%8F%E5%8C%96%E5%8A%A0%E9%80%9F"><span class="toc-number">1.2.</span> <span class="toc-text">矢量化加速</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E4%B8%8E%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.3.</span> <span class="toc-text">正态分布与平方损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%B0%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C"><span class="toc-number">1.4.</span> <span class="toc-text">从线性回归到深度网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0%E9%A2%98"><span class="toc-number">1.5.</span> <span class="toc-text">练习题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.</span> <span class="toc-text">线性回归的底层实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.1.</span> <span class="toc-text">生成数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.2.</span> <span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">2.3.</span> <span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.4.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.5.</span> <span class="toc-text">定义损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">2.6.</span> <span class="toc-text">定义优化算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">2.7.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0%E9%A2%98-1"><span class="toc-number">2.8.</span> <span class="toc-text">练习题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">线性回归的简洁实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86-1"><span class="toc-number">3.1.</span> <span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">3.2.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0-1"><span class="toc-number">3.3.</span> <span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-1"><span class="toc-number">3.4.</span> <span class="toc-text">定义损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-1"><span class="toc-number">3.5.</span> <span class="toc-text">定义优化算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="toc-number">3.6.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="toc-number">3.7.</span> <span class="toc-text">内置函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">3.8.</span> <span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">4.</span> <span class="toc-text">softmax回归的概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">4.1.</span> <span class="toc-text">分类问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-number">4.2.</span> <span class="toc-text">网络架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E7%9A%84%E5%8F%82%E6%95%B0%E5%BC%80%E9%94%80"><span class="toc-number">4.3.</span> <span class="toc-text">全连接层的参数开销</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax%E8%BF%90%E7%AE%97"><span class="toc-number">4.4.</span> <span class="toc-text">softmax运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A0%B7%E6%9C%AC%E7%9A%84%E7%9F%A2%E9%87%8F%E5%8C%96"><span class="toc-number">4.5.</span> <span class="toc-text">小批量样本的矢量化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-1"><span class="toc-number">4.6.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6"><span class="toc-number">4.6.1.</span> <span class="toc-text">对数似然</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#softmax%E5%8F%8A%E5%85%B6%E5%AF%BC%E6%95%B0"><span class="toc-number">4.6.2.</span> <span class="toc-text">softmax及其导数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1"><span class="toc-number">4.6.3.</span> <span class="toc-text">交叉熵损失</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E4%BF%A1%E6%81%AF%E8%AE%BA%E7%9C%8B%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="toc-number">4.7.</span> <span class="toc-text">从信息论看交叉熵</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%86%B5"><span class="toc-number">4.7.1.</span> <span class="toc-text">熵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E9%87%8F"><span class="toc-number">4.7.2.</span> <span class="toc-text">信息量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%8D%E7%9C%8B%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="toc-number">4.7.3.</span> <span class="toc-text">再看交叉熵</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0%E9%A2%98-2"><span class="toc-number">4.8.</span> <span class="toc-text">练习题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">5.</span> <span class="toc-text">图像分类数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">5.1.</span> <span class="toc-text">常用数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86-2"><span class="toc-number">5.2.</span> <span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E5%B0%8F%E6%89%B9%E9%87%8F"><span class="toc-number">5.3.</span> <span class="toc-text">读取小批量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88%E6%89%80%E6%9C%89%E7%BB%84%E4%BB%B6"><span class="toc-number">5.4.</span> <span class="toc-text">整合所有组件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax%E5%9B%9E%E5%BD%92%E7%9A%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.</span> <span class="toc-text">softmax回归的底层实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0-2"><span class="toc-number">6.1.</span> <span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89softmax"><span class="toc-number">6.2.</span> <span class="toc-text">定义softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B-2"><span class="toc-number">6.3.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-2"><span class="toc-number">6.4.</span> <span class="toc-text">定义损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E7%B2%BE%E5%BA%A6"><span class="toc-number">6.5.</span> <span class="toc-text">分类精度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">6.6.</span> <span class="toc-text">优化算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-2"><span class="toc-number">6.7.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B"><span class="toc-number">6.8.</span> <span class="toc-text">预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0%E9%A2%98-3"><span class="toc-number">6.9.</span> <span class="toc-text">练习题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.</span> <span class="toc-text">softmax回归的简洁实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0-3"><span class="toc-number">7.1.</span> <span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#softmax%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.2.</span> <span class="toc-text">softmax的实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95-1"><span class="toc-number">7.3.</span> <span class="toc-text">优化算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-3"><span class="toc-number">7.4.</span> <span class="toc-text">训练</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">©2022 - 2026 By 今天睡够了吗</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">You must always have faith in who you are！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="false"></script><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>