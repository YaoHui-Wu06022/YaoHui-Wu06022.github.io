<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>注意力机制 | がんばろう</title><meta name="author" content="今天睡够了吗"><meta name="copyright" content="今天睡够了吗"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="注意力提示固定随机种子 123456789101112131415def set_seed(seed: int = 42):    # Python 内置随机    random.seed(seed)    # NumPy 随机    np.random.seed(seed)    # PyTorch CPU 随机    torch.manual_seed(seed)    # PyTorch G">
<meta property="og:type" content="article">
<meta property="og:title" content="注意力机制">
<meta property="og:url" content="http://yhblogs.cn/posts/3339.html">
<meta property="og:site_name" content="がんばろう">
<meta property="og:description" content="注意力提示固定随机种子 123456789101112131415def set_seed(seed: int = 42):    # Python 内置随机    random.seed(seed)    # NumPy 随机    np.random.seed(seed)    # PyTorch CPU 随机    torch.manual_seed(seed)    # PyTorch G">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-lyyx5q_2560x1440.webp">
<meta property="article:published_time" content="2026-01-04T20:53:57.000Z">
<meta property="article:modified_time" content="2026-01-31T12:00:30.726Z">
<meta property="article:author" content="今天睡够了吗">
<meta property="article:tag" content="⌨️python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-lyyx5q_2560x1440.webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yhblogs.cn/posts/3339.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '注意力机制',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-01-31 12:00:30'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_3319458_ks437t3n4r.css"><link rel="stylesheet" href="/css/modify.css"><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/b_2a1aef95f351a5f7ef72eb81e6838fd6.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouye"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw iconfont icon-rili"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw iconfont icon-biaoqian"></i><span> 标签</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="がんばろう"><img class="site-icon" src="/img/favicon.png"><span class="site-name">がんばろう</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouye"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw iconfont icon-rili"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw iconfont icon-biaoqian"></i><span> 标签</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">注意力机制</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2026-01-04T20:53:57.000Z" title="发表于 2026-01-04 20:53:57">2026-01-04</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">14.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>58分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="注意力机制"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h2 id="注意力提示"><a href="#注意力提示" class="headerlink" title="注意力提示"></a>注意力提示</h2><p>固定随机种子</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_seed</span>(<span class="params">seed: <span class="built_in">int</span> = <span class="number">42</span></span>):</span><br><span class="line">    <span class="comment"># Python 内置随机</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># NumPy 随机</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># PyTorch CPU 随机</span></span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># PyTorch GPU 随机(单卡 / 多卡都需要)</span></span><br><span class="line">    torch.cuda.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br><span class="line"></span><br><span class="line">set_seed(<span class="number">42</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="查询、键和值"><a href="#查询、键和值" class="headerlink" title="查询、键和值"></a>查询、键和值</h3><p>“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来</p>
<p>在注意力机制的背景下，自主性提示被称为<strong>查询(query)</strong></p>
<p>而非自主性提示作为**键(key)<strong>与感官输入(sensory inputs)的</strong>值(value)**构成一组 pair 作为输入</p>
<p>给定任何查询，注意力机制通过<strong>注意力汇聚(attention pooling)</strong> 将非自主性提示的 key 引导至感官输入</p>
<p>在注意力机制中，这些感官输入被称为<strong>值(value)</strong></p>
<p>每个值都与一个**键(key)**配对，这可以想象为感官输入的非自主提示</p>
<p>可以通过设计注意力汇聚的方式，便于给定的查询(自主性提示)与键(非自主性提示)进行匹配，这将引导得出最匹配的值(感官输入)</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/qkv.jpg" alt="qkv" style="zoom:80%;">

<h3 id="注意力的可视化"><a href="#注意力的可视化" class="headerlink" title="注意力的可视化"></a>注意力的可视化</h3><p>平均汇聚层可以被视为输入的加权平均值，其中各输入的权重是一样的</p>
<p>注意力汇聚得到的是加权平均的总和值，其中权重是在给定的查询和不同的键之间计算得出的</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></tbody></table></figure>

<p>为了可视化注意力权重，需要定义一个<code>show_heatmaps</code>函数</p>
<p>其输入<code>matrices</code>的形状是<code>(num_rows, num_cols, H, W)</code>，每个元素是一个二维矩阵</p>
<p><code>cmap</code>颜色映射，<code>Reds</code>常用于：</p>
<ul>
<li>权重</li>
<li>概率</li>
<li>能量 / 强度</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_heatmaps</span>(<span class="params">matrices, xlabel, ylabel, titles=<span class="literal">None</span>, figsize=(<span class="params"><span class="number">2.5</span>, <span class="number">2.5</span></span>),</span></span><br><span class="line"><span class="params">                  cmap=<span class="string">'Reds'</span></span>):</span><br><span class="line">    <span class="string">"""显示矩阵热图"""</span></span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    num_rows, num_cols = matrices.shape[<span class="number">0</span>], matrices.shape[<span class="number">1</span>]</span><br><span class="line">    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,</span><br><span class="line">                                 sharex=<span class="literal">True</span>, sharey=<span class="literal">True</span>, squeeze=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> i, (row_axes, row_matrices) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, matrices)):</span><br><span class="line">        <span class="keyword">for</span> j, (ax, matrix) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(row_axes, row_matrices)):</span><br><span class="line">            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)</span><br><span class="line">            <span class="keyword">if</span> i == num_rows - <span class="number">1</span>:</span><br><span class="line">                ax.set_xlabel(xlabel)</span><br><span class="line">            <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">                ax.set_ylabel(ylabel)</span><br><span class="line">            <span class="keyword">if</span> titles:</span><br><span class="line">                ax.set_title(titles[j])</span><br><span class="line">    fig.colorbar(pcm, ax=axes, shrink=<span class="number">0.6</span>);</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.eye(<span class="number">10</span>).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">show_heatmaps(attention_weights, xlabel=<span class="string">'Keys'</span>, ylabel=<span class="string">'Queries'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/output_attention-cues_054b1a_36_0.svg" alt="output_attention-cues_054b1a_36_0"></p>
<p>随机生成一个 10×10 矩阵，对每一行做 softmax，得到合法的注意力概率分布</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 随机生成一个 10×10 的原始注意力得分矩阵(logits)</span></span><br><span class="line">raw_scores = torch.randn(<span class="number">10</span>, <span class="number">10</span>)  <span class="comment"># 得分可为负</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每一行做 softmax，确保每一行是合法的概率分布</span></span><br><span class="line"><span class="comment"># 对每一个 Query，所有 Key 的注意力权重加起来等于 1</span></span><br><span class="line">attention_weights = F.softmax(raw_scores, dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 扩展维度以适配 show_heatmaps 的接口</span></span><br><span class="line"><span class="comment"># (1, 1, 10, 10)：表示 1 行 × 1 列 的热图布局</span></span><br><span class="line">attention_weights_vis = attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">show_heatmaps(</span><br><span class="line">    attention_weights_vis,</span><br><span class="line">    xlabel=<span class="string">'Key Index'</span>,</span><br><span class="line">    ylabel=<span class="string">'Query Index'</span>,</span><br><span class="line">    titles=[<span class="string">'Attention Weights'</span>],</span><br><span class="line">    figsize=(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260106_1321.svg" alt="20260106_1321" style="zoom:80%;">

<h2 id="注意力汇聚-Nadaraya-Watson核回归"><a href="#注意力汇聚-Nadaraya-Watson核回归" class="headerlink" title="注意力汇聚:Nadaraya-Watson核回归"></a>注意力汇聚:Nadaraya-Watson核回归</h2><blockquote>
<p>了解思想即可，在这里d_model默认为1，所以没有出现Q和K不同维度的问题</p>
</blockquote>
<p>查询(自主提示)和键(非自主提示)之间的交互形成了注意力汇聚</p>
<p>注意力汇聚有选择地聚合了值(感官输入)以生成最终的输出</p>
<h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><p>给定的成对的“输入－输出”数据集${(x_1, y_1), \ldots, (x_n, y_n)}$，如何学习$f$来预测任意新输入的输出$\hat{y} = f(x)$</p>
<p>根据下面的非线性函数生成一个人工数据集，其中加入的噪声项为$\epsilon$：<br>$$<br>y_i = 2\sin(x_i) + x_i^{0.8} + \epsilon,<br>$$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n_train = <span class="number">50</span>  <span class="comment"># 训练样本数</span></span><br><span class="line">x_train, _ = torch.sort(torch.rand(n_train) * <span class="number">5</span>)   <span class="comment"># 排序后的训练样本</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * torch.sin(x) + x**<span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">y_train = f(x_train) + torch.normal(<span class="number">0.0</span>, <span class="number">0.5</span>, (n_train,))  <span class="comment"># 训练样本的输出</span></span><br><span class="line">x_test = torch.arange(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0.1</span>)  <span class="comment"># 测试样本</span></span><br><span class="line">y_truth = f(x_test)  <span class="comment"># 测试样本的真实输出</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(x_test))</span><br></pre></td></tr></tbody></table></figure>

<p>下面的函数将绘制所有的训练样本(样本由圆圈表示)，不带噪声项的真实数据生成函数(标记为“Truth”)，以及学习得到的预测函数(标记为“Pred”)</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_kernel_reg</span>(<span class="params">y_hat</span>):</span><br><span class="line">    d2l.plot(x_test, [y_truth, y_hat], <span class="string">'x'</span>, <span class="string">'y'</span>, legend=[<span class="string">'Truth'</span>, <span class="string">'Pred'</span>],</span><br><span class="line">             xlim=[<span class="number">0</span>, <span class="number">5</span>], ylim=[-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">    d2l.plt.plot(x_train, y_train, <span class="string">'o'</span>, alpha=<span class="number">0.5</span>)</span><br></pre></td></tr></tbody></table></figure>

<blockquote>
<p>关于d2l.plot函数：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">has_one_axis</span>(<span class="params">X</span>):  <span class="comment"># True if X (tensor or list) has 1 axis</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="built_in">hasattr</span>(X, <span class="string">"ndim"</span>) <span class="keyword">and</span> X.ndim == <span class="number">1</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>)</span><br><span class="line">            <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(X[<span class="number">0</span>], <span class="string">"__len__"</span>))</span><br></pre></td></tr></tbody></table></figure>

<p><code>torch.Tensor</code>和<code>numpy.ndarray</code>有<code>ndim</code>这个属性，Python list 没有</p>
<p>想要list的值扩大不用<code>*</code>，例如<code>[x * 3 for x in X]</code>，用<code>*</code>实现的是复制效果</p>
</blockquote>
<h3 id="平均汇聚"><a href="#平均汇聚" class="headerlink" title="平均汇聚"></a>平均汇聚</h3><p>基于平均汇聚来计算所有训练样本输出值的平均值<br>$$<br>f(x) = \frac{1}{n}\sum_{i=1}^n y_i,<br>$$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_hat = torch.repeat_interleave(y_train.mean(), n_test)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260106_1354.svg" alt="20260106_1354">

<h3 id="非参数注意力汇聚"><a href="#非参数注意力汇聚" class="headerlink" title="非参数注意力汇聚"></a>非参数注意力汇聚</h3><p>根据输入的位置对输出进行加权<br>$$<br>f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i,<br>$$<br>其中是$K$是核(kernel)，公式所描述的估计器被称为<em>Nadaraya-Watson核回归</em></p>
<p>重写得到一个更加通用的**注意力汇聚(attention pooling)**公式<br>$$<br>f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i,<br>$$<br>其中$x$是查询，$(x_i, y_i)$是键值对，查询和键之间的关系建模为注意力权重$\alpha(x, x_i)$，这个权重将被分配给每一个对应值$y_i$</p>
<p>对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布：它们是非负的，并且总和为1</p>
<p>如果把核考虑为高斯核<br>$$<br>K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2}).<br>$$<br>代入后就能获得：<br>$$<br>\begin{split}\begin{aligned} f(x) &amp;=\sum_{i=1}^n \alpha(x, x_i) y_i<br>\\ &amp;= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i<br>\\&amp;= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}\end{split}<br>$$<br>如果一个键越是接近给定的查询, 那么分配给这个键对应值$y_i$的注意力权重就会越大， 也就“获得了更多的注意力”</p>
<p>Nadaraya-Watson核回归是一个非参数模型，将基于这个非参数的注意力汇聚模型来绘制预测结果</p>
<p>从绘制的结果会发现新的模型预测线是平滑的，并且比平均汇聚的预测更接近真实</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_repeat的形状:(n_test,n_train),</span></span><br><span class="line"><span class="comment"># 每一行是一个查询(Query)，复制来和所有 Key 对齐</span></span><br><span class="line">X_repeat = x_test.repeat_interleave(n_train).reshape((-<span class="number">1</span>, n_train))</span><br><span class="line"><span class="comment"># x_train包含着键，attention_weights的形状：(n_test,n_train),</span></span><br><span class="line"><span class="comment"># 每一行都包含着要在给定的每个查询的值(y_train)之间分配的注意力权重</span></span><br><span class="line">attention_weights = F.softmax(-(X_repeat-x_train)**<span class="number">2</span>/<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重</span></span><br><span class="line">y_hat = torch.matmul(attention_weights, y_train)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260106_1409.svg" alt="20260106_1409">

<p>这里测试数据的输入相当于查询，而训练数据的输入相当于键</p>
<p>因此由观察可知“查询-键”对越接近，注意力汇聚的注意力权重就越高</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">'Sorted training inputs'</span>,</span><br><span class="line">                  ylabel=<span class="string">'Sorted testing inputs'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260106_1414.svg" alt="20260106_1414"></p>
<h3 id="带参数注意力汇聚"><a href="#带参数注意力汇聚" class="headerlink" title="带参数注意力汇聚"></a>带参数注意力汇聚</h3><p>非参数的Nadaraya-Watson核回归具有一致性的优点：如果有足够的数据，此模型会收敛到最优结果</p>
<p>在下面的查询和键之间的距离乘以可学习参数$w$<br>$$<br>\begin{split}\begin{aligned}f(x) &amp;= \sum_{i=1}^n \alpha(x, x_i) y_i \\<br>&amp;= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x - x_j)w)^2\right)} y_i \\<br>&amp;= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i.\end{aligned}\end{split}<br>$$</p>
<h4 id="批量矩阵乘法"><a href="#批量矩阵乘法" class="headerlink" title="批量矩阵乘法"></a>批量矩阵乘法</h4><p>为了更有效地计算小批量数据的注意力，可以利用深度学习开发框架中提供的批量矩阵乘法</p>
<p>假设第一个小批量数据包含$n$个矩阵$\mathbf{X}_1,\ldots, \mathbf{X}_n$，形状为$a\times b$，第二个小批量包含$n$个矩阵，$\mathbf{Y}_1, \ldots, \mathbf{Y}_n$，形状为$b\times c$，</p>
<p>它们的批量矩阵乘法得到$n$个矩阵$\mathbf{X}_1\mathbf{Y}_1, \ldots, \mathbf{X}_n\mathbf{Y}_n$，形状为$a\times c$</p>
<p>假定两个张量的形状分别是$(n,a,b)$和$(n,b,c)$，它们的批量矩阵乘法输出的形状为$(n,a,c)$</p>
<blockquote>
<p><strong>为什么要引入“批量矩阵乘法”？</strong></p>
<p>深度学习里几乎所有东西都是“成批”的，例如在 Attention 中：</p>
<ul>
<li><code>attention_weights</code>：<code>(batch, query_len, key_len)</code></li>
<li><code>values</code>：<code>(batch, key_len, d)</code></li>
</ul>
<p>输出是：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(batch, query_len, d)</span><br></pre></td></tr></tbody></table></figure>

<p>本质上就是批量矩阵乘法</p>
</blockquote>
<p>在注意力机制的背景中，可以使用小批量矩阵乘法来计算小批量数据中的加权平均值</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">weights = torch.ones((<span class="number">2</span>, <span class="number">10</span>)) * <span class="number">0.1</span> <span class="comment"># 2个batch，均匀注意力</span></span><br><span class="line">values = torch.arange(<span class="number">20.0</span>).reshape((<span class="number">2</span>, <span class="number">10</span>)) <span class="comment"># 2个batch，每个batch有10个value</span></span><br><span class="line">torch.bmm(weights.unsqueeze(<span class="number">1</span>), values.unsqueeze(-<span class="number">1</span>))</span><br><span class="line"><span class="comment"># weights.unsqueeze(1)把(2, 10)变为(batch×1×key_len)，1对应query数量</span></span><br><span class="line"><span class="comment"># values.unsqueeze(-1)把(2, 10)变为(batch×key_len×1)，1对应value_dim</span></span><br><span class="line"><span class="comment"># 输出结果为(2,1,1)</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 4.5000]],</span><br><span class="line"></span><br><span class="line">        [[14.5000]]])</span><br></pre></td></tr></tbody></table></figure>

<h4 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h4><p><code>repeat_interleave</code> 只能把数据“拉平成一维重复”，真正需要的是一个二维的<br> <code>(查询个数, 键的个数)</code> 矩阵</p>
<p>使用小批量矩阵乘法，定义Nadaraya-Watson核回归的带参数版本为</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NWKernelRegression</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.w = nn.Parameter(torch.rand((<span class="number">1</span>,), requires_grad=<span class="literal">True</span>)) <span class="comment"># 这里w是一个标量，到transformer成为二维</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values</span>):</span><br><span class="line">        <span class="comment"># queries和attention_weights的形状为(查询个数，“键－值”对个数)</span></span><br><span class="line">        queries = queries.repeat_interleave(keys.shape[<span class="number">1</span>]).reshape((-<span class="number">1</span>, keys.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="variable language_">self</span>.attention_weights = F.softmax(</span><br><span class="line">            -((queries - keys) * <span class="variable language_">self</span>.w)**<span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># values的形状为(查询个数，“键－值”对个数)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(<span class="variable language_">self</span>.attention_weights.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                         values.unsqueeze(-<span class="number">1</span>)).reshape(-<span class="number">1</span>) <span class="comment"># 去掉“人为保留的 1×1 维度”，把结果还原成一维向量</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>将训练数据集变换为键和值用于训练注意力模型</p>
<p>在带参数的注意力汇聚模型中，任何一个训练样本的输入都会和除自己以外的所有训练样本的“键－值”对进行计算，从而得到其对应的预测输出</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输入</span></span><br><span class="line">X_tile = x_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># Y_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输出</span></span><br><span class="line">Y_tile = y_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># keys的形状:('n_train'，'n_train'-1)，每个 query 对应 n_train-1 个 key</span></span><br><span class="line"><span class="comment"># 构造“掩码”，去掉对角线</span></span><br><span class="line">keys = X_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br><span class="line"><span class="comment"># values的形状:('n_train'，'n_train'-1)，每个 query 对应 n_train-1 个 value</span></span><br><span class="line">values = Y_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 第 i 行 = 用来预测第 i 个样本的“所有其他输入”</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>训练带参数的注意力汇聚模型时，使用平方损失函数和随机梯度下降</p>
<blockquote>
<p>反向传播算法Adam和SGD有什么区别?</p>
<p>SGD：只看“当前梯度”，走得朴素但稳定</p>
<p>Adam：记住“历史梯度的方向和大小”，走得快但更激进</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>SGD</th>
<th>Adam</th>
</tr>
</thead>
<tbody><tr>
<td>是否记历史</td>
<td>否</td>
<td>是</td>
</tr>
<tr>
<td>学习率</td>
<td>全局一个</td>
<td>参数自适应</td>
</tr>
<tr>
<td>对尺度敏感</td>
<td>很敏感</td>
<td>不敏感</td>
</tr>
<tr>
<td>收敛速度</td>
<td>慢</td>
<td>快</td>
</tr>
<tr>
<td>稳定性</td>
<td>高</td>
<td>初期可能不稳</td>
</tr>
<tr>
<td>泛化(经验)</td>
<td>常更好</td>
<td>有时略差</td>
</tr>
<tr>
<td>超参数</td>
<td>少</td>
<td>多</td>
</tr>
</tbody></table>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = NWKernelRegression()</span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">'none'</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">'epoch'</span>, ylabel=<span class="string">'loss'</span>, xlim=[<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    trainer.zero_grad()</span><br><span class="line">    l = loss(net(x_train, keys, values), y_train)</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    trainer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'epoch <span class="subst">{epoch + <span class="number">1</span>}</span>, loss <span class="subst">{<span class="built_in">float</span>(l.<span class="built_in">sum</span>()):<span class="number">.6</span>f}</span>'</span>)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, <span class="built_in">float</span>(l.<span class="built_in">sum</span>()))</span><br></pre></td></tr></tbody></table></figure>

<p>举个例子：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x_train = [1 2 3 4]  # 也就是q</span><br><span class="line">keys = </span><br><span class="line">	[</span><br><span class="line">		[2 3 4]</span><br><span class="line">		[1 3 4]</span><br><span class="line">		[1 2 4]</span><br><span class="line">		[1 2 3]</span><br><span class="line">	]</span><br><span class="line">queries = </span><br><span class="line">	[</span><br><span class="line">		[1 1 1]</span><br><span class="line">		[2 2 2]</span><br><span class="line">		[3 3 3]</span><br><span class="line">		[4 4 4]</span><br><span class="line">	]</span><br><span class="line"># 这样才能让queries - keys获得距离</span><br><span class="line"># 行数始终为n_train，但列数会因为避免和自己计算而减1</span><br></pre></td></tr></tbody></table></figure>

<p>训练完带参数的注意力汇聚模型后可以发现：在尝试拟合带噪声的训练数据时，预测结果绘制的线不如之前非参数模型的平滑</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># keys的形状:(n_test，n_train)，每一行包含着相同的训练输入(例如，相同的键)</span></span><br><span class="line">keys = x_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># value的形状:(n_test，n_train)</span></span><br><span class="line">values = y_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line">y_hat = net(x_test, keys, values).unsqueeze(<span class="number">1</span>).detach()</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></tbody></table></figure>

<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260106_1535.svg" alt="20260106_1535"></p>
<p>为什么新的模型更不平滑了呢？看一下输出结果的绘制图：与非参数的注意力汇聚模型相比，带参数的模型加入可学习的参数后，曲线在注意力权重较大的区域变得更不平滑</p>
<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260106_1536.svg" alt="20260106_1536"></p>
<h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><ul>
<li><p>在带参数的注意力汇聚的实验中学习得到的参数的价值是什么？为什么在可视化注意力权重时，它会使加权区域更加尖锐？</p>
<p>通过学习参数，自动学会了“相似性的尺度与判别标准”</p>
<p><code>w</code> 小：核函数宽；注意力分布平；很多点一起平均 → 低方差，高偏差</p>
<p><code>w</code> 大：核函数窄；权重集中在少数点 → 低偏差，高方差</p>
<p>因为可学习参数放大了相似度差异，而 softmax 会对这种差异做指数级放大，两者叠加的结果就是——权重集中到极少数位置，在可视化中表现为加权区域更加尖锐，所以transformer里加入了$\sqrt d$控制 score 的尺度，防止过度尖锐</p>
</li>
<li><p>为本节的核回归设计一个新的带参数的注意力汇聚模型。训练这个新模型并可视化其注意力权重</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ParametricKernelAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># log_sigma 保证 sigma &gt; 0(数值稳定)</span></span><br><span class="line">        <span class="variable language_">self</span>.log_sigma = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># 可学习偏置</span></span><br><span class="line">        <span class="variable language_">self</span>.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        queries: (n_query,)</span></span><br><span class="line"><span class="string">        keys:    (n_query, n_kv)</span></span><br><span class="line"><span class="string">        values:  (n_query, n_kv)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n_kv = keys.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># (n_query, n_kv)</span></span><br><span class="line">        queries = queries.repeat_interleave(n_kv).reshape(-<span class="number">1</span>, n_kv)</span><br><span class="line"></span><br><span class="line">        sigma = torch.exp(<span class="variable language_">self</span>.log_sigma)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 注意力打分</span></span><br><span class="line">        scores = -(queries - keys) ** <span class="number">2</span> / (<span class="number">2</span> * sigma ** <span class="number">2</span>) + <span class="variable language_">self</span>.bias</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 注意力权重</span></span><br><span class="line">        <span class="variable language_">self</span>.attention_weights = F.softmax(scores, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加权求和</span></span><br><span class="line">        out = torch.bmm(</span><br><span class="line">            <span class="variable language_">self</span>.attention_weights.unsqueeze(<span class="number">1</span>),</span><br><span class="line">            values.unsqueeze(-<span class="number">1</span>)</span><br><span class="line">        ).reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">net = ParametricKernelAttention()</span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">'none'</span>)</span><br><span class="line">trainer = torch.optim.Adam(net.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">20</span></span><br><span class="line">losses = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    trainer.zero_grad()</span><br><span class="line">    y_hat = net(x_train, keys, values)</span><br><span class="line">    l = loss(y_hat, y_train)</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    trainer.step()</span><br><span class="line"></span><br><span class="line">    losses.append(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'epoch <span class="subst">{epoch+<span class="number">1</span>}</span>, loss <span class="subst">{<span class="built_in">float</span>(l.<span class="built_in">sum</span>()):<span class="number">.6</span>f}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>训练出来的更稳一些，优化空间更“正交”，梯度更稳定，也更容易找到好解</p>
<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260106_1558.svg" alt="20260106_1558"></p>
</li>
</ul>
<h2 id="注意力评分函数"><a href="#注意力评分函数" class="headerlink" title="注意力评分函数"></a>注意力评分函数</h2><p>Nadaraya-Watson使用高斯核来对查询和键之间的关系建模，高斯核指数部分可以视为注意力评分函数，简称评分函数</p>
<p>然后把这个函数的输出结果输入到softmax函数中进行运算，注意力汇聚的输出就是基于这些注意力权重的值的加权和</p>
<p>由于注意力权重是概率分布，因此加权和其本质上是加权平均值</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/attention-output.webp" alt="attention-output" style="zoom:80%;">

<p>假设有一个查询$\mathbf{q} \in \mathbb{R}^q$和$m$个“键－值”对$(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)$，其中$\mathbf{k}_i \in \mathbb{R}^k$，$\mathbf{v}_i \in \mathbb{R}^v$</p>
<p>注意力汇聚函数就被表示成值的加权和<br>$$<br>f(\mathbf{q}, (\mathbf k_1, \mathbf v_1 ), \ldots, (\mathbf k_m, \mathbf v_m)) = \sum_{i=1}^m \alpha(\mathbf q, \mathbf k_i) \mathbf v_i \in \mathbb{R}^v<br>$$<br>其中查询和键的注意力权重(标量)是通过注意力评分函数将两个向量映射成标量，再经过softmax运算得到的<br>$$<br>\alpha(\mathbf{q}, \mathbf k_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf k_i)) = \frac{\exp(a(\mathbf{q}, \mathbf k_i))}{\sum_{j=1}^m \exp(a(\mathbf{q}, \mathbf k_j))} \in \mathbb{R}.<br>$$</p>
<h3 id="掩蔽softmax操作"><a href="#掩蔽softmax操作" class="headerlink" title="掩蔽softmax操作"></a>掩蔽softmax操作</h3><p>softmax操作用于输出一个概率分布作为注意力权重，在某些情况下，并非所有的值都应该被纳入到注意力汇聚中</p>
<p>比如某些文本序列被填充了没有意义的特殊词元，为了仅将有意义的词元作为值来获取注意力汇聚，可以指定一个有效序列长度，以便在计算softmax时过滤掉超出指定范围的位置</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span><br><span class="line">    <span class="string">"""通过在最后一个轴上掩蔽元素来执行softmax操作"""</span></span><br><span class="line">    <span class="comment"># X:3D张量，valid_lens:1D或2D张量</span></span><br><span class="line">    <span class="comment"># X.shape == (batch_size, num_queries, num_keys)</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>: <span class="comment"># 每个 batch 一个 有效 key 长度</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>]) <span class="comment"># 实现拉成(B×Q,)</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 每个 batch 的每个 query 有自己的有效 key 长度</span></span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>) <span class="comment"># 拉成(B×Q,)，匹配后面的X.reshape(-1, shape[-1])</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0</span></span><br><span class="line">        <span class="comment"># 因为sequence_mask输入(N, L)，所以要把batch和Q合并，K不变</span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens,</span><br><span class="line">                              value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="comment"># 从(B×Q, K) → (B, Q, K)</span></span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>考虑由两个矩阵表示的样本，这两个样本的有效长度分别为2和3，经过掩蔽softmax操作，超出有效长度的值都被掩蔽为0</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([<span class="number">2</span>, <span class="number">3</span>]))</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[0.5396, 0.4604, 0.0000, 0.0000],</span><br><span class="line">         [0.5120, 0.4880, 0.0000, 0.0000]],</span><br><span class="line"></span><br><span class="line">        [[0.3211, 0.3711, 0.3078, 0.0000],</span><br><span class="line">         [0.2700, 0.2769, 0.4531, 0.0000]]])</span><br></pre></td></tr></tbody></table></figure>

<p>同样，也可以使用二维张量，为矩阵样本中的每一行指定有效长度</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([[<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>]]))</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[1.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="line">         [0.3466, 0.3790, 0.2744, 0.0000]],</span><br><span class="line"></span><br><span class="line">        [[0.3545, 0.6455, 0.0000, 0.0000],</span><br><span class="line">         [0.1659, 0.3488, 0.1922, 0.2931]]])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="加性注意力"><a href="#加性注意力" class="headerlink" title="加性注意力"></a>加性注意力</h3><p><font color="DarkViolet">当查询Q和键K是不同维度的矢量时，可以使用加性注意力作为评分函数</font></p>
<p>Q和K维度相同 指的是 $d_q = d_k$</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Q.shape = (batch_size, num_queries, d_q)</span><br><span class="line">K.shape = (batch_size, num_keys,    d_k)</span><br><span class="line">V.shape = (batch_size, num_keys,    d_v)</span><br></pre></td></tr></tbody></table></figure>

<p><font color="DarkViolet">值的特征维度和键/查询的特征维度可以不一样，但值的序列长度必须和键的相同</font></p>
<p>给定查询$\mathbf{q} \in \mathbb{R}^q$和键$\mathbf{k} \in \mathbb{R}^k$，加性注意力的评分函数为<br>$$<br>a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},<br>$$<br>将查询和键连结起来后输入到一个多层感知机(MLP)中，感知机包含一个隐藏层，通过使用$\tanh$作为激活函数，并且禁用偏置项</p>
<blockquote>
<p>禁用偏置项的情况：</p>
<ul>
<li><p>紧跟 BatchNorm / LayerNorm 的线性层或卷积层，前一层的 bias 会被完全抵消，属于冗余参数</p>
<p>Conv + BN：Conv 通常 <code>bias=False</code></p>
<p>Linear + LN：Linear 通常 <code>bias=False</code></p>
</li>
<li><p>注意力机制中的打分网络</p>
<p>打分函数关注的是 <strong>相对相似度</strong>，加 bias 会引入与$q,k$无关的常数偏好</p>
</li>
<li><p>点积注意力中的线性映射</p>
<p>在 Transformer 中，<code>W_q, W_k, W_v</code> 通常不带 bias，因为后面跟 LayerNorm</p>
</li>
</ul>
<p>一般在最终分类 / 回归输出层以及中间的 MLP 表示学习层(无 BN/LN)时加入bias</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AdditiveAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">"""加性注意力"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 把Q和K映射到相同维度，这是“加性注意力”能处理不同维度 Q/K 的关键</span></span><br><span class="line">        <span class="variable language_">self</span>.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 把“特征向量”压成“相似度分数”</span></span><br><span class="line">        <span class="variable language_">self</span>.W_v = nn.Linear(num_hiddens, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        queries, keys = <span class="variable language_">self</span>.W_q(queries), <span class="variable language_">self</span>.W_k(keys)</span><br><span class="line">        <span class="comment"># queries.shape = (batch_size, num_queries, num_hiddens)</span></span><br><span class="line">        <span class="comment"># keys.shape    = (batch_size, num_kv, num_hiddens)</span></span><br><span class="line">        <span class="comment"># 在维度扩展后，</span></span><br><span class="line">        <span class="comment"># queries.shape = (batch_size, num_queries, 1, num_hiddens)</span></span><br><span class="line">        <span class="comment"># keys.shape    = (batch_size, 1, num_kv, num_hiddens)</span></span><br><span class="line">        <span class="comment"># 使用广播方式进行求和</span></span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># features.shape = (batch_size, num_queries, num_kv, num_hiddens)</span></span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        <span class="comment"># self.w_v仅有一个输出，因此从形状中移除最后那个维度</span></span><br><span class="line">        <span class="comment"># scores的形状：(batch_size， num_queries, num_kv)</span></span><br><span class="line">        scores = <span class="variable language_">self</span>.W_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="comment"># values的形状：(batch_size, num_kv, value_size)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(<span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.attention_weights), values)</span><br></pre></td></tr></tbody></table></figure>

<p><font color="Violetred">输出张量为(batch_size, num_queries, value_size)</font></p>
<p>点积注意力要求查询和键在特征维度上完全一致，否则内积无法定义</p>
<p>而加性注意力通过对查询Q和键K分别做线性映射，把它们投影到同一隐藏空间，从而在原始维度不同的情况下仍然可以计算相似度</p>
<p>查询、键和值的形状为**[批量大小，步数或词元序列长度，特征大小]→(batch_size, num_len, d_k)**</p>
<p>注意力汇聚输出的形状为**[批量大小，查询的步数，值的维度]→(batch_size, num_queries, value_size)**</p>
<p><font color="DarkViolet">对比Transformer</font></p>
<table>
<thead>
<tr>
<th>维度</th>
<th>AdditiveAttention</th>
<th>Transformer Attention</th>
</tr>
</thead>
<tbody><tr>
<td>相似度</td>
<td>MLP + tanh (加性)</td>
<td>点积(极致并行和效率)</td>
</tr>
<tr>
<td>Q/K 原始维度</td>
<td>可不同</td>
<td>必须相同</td>
</tr>
<tr>
<td>是否用 MLP</td>
<td>✅</td>
<td>❌</td>
</tr>
<tr>
<td>是否需要 √d 缩放</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>多头</td>
<td>❌</td>
<td>✅</td>
</tr>
<tr>
<td>表达灵活性</td>
<td>高</td>
<td>中</td>
</tr>
<tr>
<td>计算效率</td>
<td>低</td>
<td>极高</td>
</tr>
<tr>
<td>并行能力</td>
<td>一般</td>
<td>极强</td>
</tr>
<tr>
<td>适合时代</td>
<td>RNN 时代</td>
<td>大模型时代</td>
</tr>
</tbody></table>
<p>用一个小例子来演示上面的<code>AdditiveAttention</code>类</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">queries, keys = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">20</span>)), torch.ones((<span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># values的小批量，两个值矩阵是相同的</span></span><br><span class="line">values = torch.arange(<span class="number">40</span>, dtype=torch.float32).reshape(<span class="number">1</span>, <span class="number">10</span>, <span class="number">4</span>).repeat(</span><br><span class="line">    <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">valid_lens = torch.tensor([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">attention = AdditiveAttention(key_size=<span class="number">2</span>, query_size=<span class="number">20</span>, num_hiddens=<span class="number">8</span>,</span><br><span class="line">                              dropout=<span class="number">0.1</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],</span><br><span class="line"></span><br><span class="line">        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=&lt;BmmBackward0&gt;)</span><br></pre></td></tr></tbody></table></figure>

<p>加性注意力并没有“选择”任何 key，因为所有 key 是等价的，只剩下 mask 在起作用，注意力机制退化成了对前 <code>valid_lens[i]</code> 个 value 做平均</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">values =</span><br><span class="line">[[[ 0,  1,  2,  3],</span><br><span class="line">  [ 4,  5,  6,  7],</span><br><span class="line">  [ 8,  9, 10, 11],</span><br><span class="line">  [12, 13, 14, 15],</span><br><span class="line">  [16, 17, 18, 19],</span><br><span class="line">  [20, 21, 22, 23],</span><br><span class="line">  [24, 25, 26, 27],</span><br><span class="line">  [28, 29, 30, 31],</span><br><span class="line">  [32, 33, 34, 35],</span><br><span class="line">  [36, 37, 38, 39]]]</span><br></pre></td></tr></tbody></table></figure>

<p>第 0 个 batch<code>valid_lens[0] = 2</code></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0]</span><br></pre></td></tr></tbody></table></figure>

<p>前 2 个 value 的平均</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[(0+4)/2, (1+5)/2, (2+6)/2, (3+7)/2]</span><br><span class="line">= [2, 3, 4, 5]</span><br></pre></td></tr></tbody></table></figure>

<p>第 1 个 batch<code>valid_lens[0] = 6</code></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1/6, 1/6, 1/6, 1/6, 1/6, 1/6, 0, 0, 0, 0]</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[(0+4+8+12+16+20)/6,</span><br><span class="line"> (1+5+9+13+17+21)/6,</span><br><span class="line"> (2+6+10+14+18+22)/6,</span><br><span class="line"> (3+7+11+15+19+23)/6]</span><br><span class="line">= [10, 11, 12, 13]</span><br></pre></td></tr></tbody></table></figure>

<p>尽管加性注意力包含了可学习的参数，但由于本例子中每个键都是相同的，所以注意力权重是均匀的，由指定的有效长度决定</p>
<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260106_1855.svg" alt="20260106_1855"></p>
<h3 id="缩放点积注意力"><a href="#缩放点积注意力" class="headerlink" title="缩放点积注意力"></a>缩放点积注意力</h3><p>使用点积可以得到计算效率更高的评分函数，但是<font color="DarkViolet">点积操作要求查询和键具有相同的特征维度</font></p>
<blockquote>
<p>为了方便表述，$d_q=d_k$后面统一用$d_k$</p>
</blockquote>
<p>将点积除以$\sqrt{d}$，则**缩放点积注意力(scaled dot-product attention)**评分函数为<br>$$<br>a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}.<br>$$<br>这里和 Transformer 完全一致，是点积注意力在高维下必须引入的数值稳定性修正，没有这一项点积注意力在高维下会数值失控</p>
<p>从统计角度看<br>$$<br>\mathbf{q}^{\top} \mathbf{k}=\sum_{i=1}^{d} q_i k_i<br>$$<br>维度越大，点积的波动越大，会直接把 softmax 推进饱和区，softmax 变成近似 one-hot，梯度几乎为 0，训练会变得极其不稳定，除以$\sqrt{d}$ 本质上是在做方差归一化</p>
<p>换到小批量写法，基于$n$查询和$m$个键-值对计算注意力，其中查询和键的特征维度为$d_k$，值的特征维度为$d_v$</p>
<p>查询$\mathbf Q\in\mathbb R^{n\times d}$，键$\mathbf K\in\mathbb R^{m\times d}$，值$\mathbf V\in\mathbb R^{m\times v}$ 缩放点积注意力是<br>$$<br>\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}.<br>$$<br>缩放点积注意力的实现使用了暂退法进行模型正则化</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">"""缩放点积注意力"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">    <span class="comment"># queries：(batch_size，num_queries，d_k)</span></span><br><span class="line">    <span class="comment"># keys：   (batch_size，num_kv，d_k)</span></span><br><span class="line">    <span class="comment"># values： (batch_size，num_kv，d_v)</span></span><br><span class="line">    <span class="comment"># valid_lens :(batch_size，) or (batch_size，num_queries)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens=<span class="literal">None</span></span>):</span><br><span class="line">        d = queries.shape[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 设置transpose_b=True为了交换keys的最后两个维度</span></span><br><span class="line">        <span class="comment"># 因为之前写的时候都会专门加维度点积，现在是不加直接交换维度点积</span></span><br><span class="line">        scores = torch.bmm(queries, keys.transpose(<span class="number">1</span>,<span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        <span class="variable language_">self</span>.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="comment"># attention_weights: (batch_size, num_queries, num_kv)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(<span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.attention_weights), values)</span><br><span class="line">    	<span class="comment"># 输出: (batch_size, num_queries, d_v)</span></span><br></pre></td></tr></tbody></table></figure>

<p>演示上述的<code>DotProductAttention</code>类</p>
<p>使用与先前加性注意力例子中相同的键、值和有效长度</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">queries = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">keys =  torch.ones((<span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line">values = torch.arange(<span class="number">40</span>, dtype=torch.float32).reshape(<span class="number">1</span>, <span class="number">10</span>, <span class="number">4</span>).repeat(</span><br><span class="line">    <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">valid_lens = torch.tensor([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line">attention = DotProductAttention(dropout=<span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],</span><br><span class="line"></span><br><span class="line">        [[10.0000, 11.0000, 12.0000, 13.0000]]])</span><br></pre></td></tr></tbody></table></figure>

<p>输出结果应该是相同的</p>
<blockquote>
<p><strong>查询的维度和内容只有在键之间存在差异时才会影响注意力结果；当所有键在注意力空间中等价时，注意力机制退化为均匀加权，此时无论查询维度多大、取值多复杂，输出结果都不会发生变化</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th>情况</th>
<th>Query 是否影响输出</th>
</tr>
</thead>
<tbody><tr>
<td>所有 key 完全相同</td>
<td>❌ 不影响</td>
</tr>
<tr>
<td>key 不同，但 query=0</td>
<td>❌ 不影响</td>
</tr>
<tr>
<td>key 不同，query 有区分</td>
<td>✅ 强烈影响</td>
</tr>
<tr>
<td>query 维度↑，key 不变</td>
<td>❌ 仍不影响</td>
</tr>
<tr>
<td>query 维度↑，key 有结构</td>
<td>✅ 影响更丰富</td>
</tr>
</tbody></table>
<p>如果修改键，不再相同，可加性注意力和缩放的“点－积”注意力不再产生相同的结果</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keys = torch.arange(<span class="number">20</span>, dtype=torch.float32)\</span><br><span class="line">            .reshape(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>)\</span><br><span class="line">            .repeat(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>在加性注意力下，score 有差异，但差异不大，权重 ≈ “接近均匀、略有偏向”</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_weights ≈</span><br><span class="line">[0.476, 0.524, 0, 0, 0, 0, 0, 0, 0, 0]   # batch 0</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 2.0962,  3.0962,  4.0962,  5.0962]],</span><br><span class="line"></span><br><span class="line">        [[ 9.6081, 10.6081, 11.6081, 12.6081]]])</span><br></pre></td></tr></tbody></table></figure>

<p>在缩放点积注意力下，权重高度集中</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_weights ≈</span><br><span class="line">[0.85, 0.15, 0, 0, 0, 0, 0, 0, 0, 0]   # batch 0</span><br></pre></td></tr></tbody></table></figure>

<p>几乎就是第一个 value，加一点点第二个</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[0.2894, 1.2894, 2.2894, 3.2894]],</span><br><span class="line"></span><br><span class="line">        [[4.4147, 5.4147, 6.4147, 7.4147]]])</span><br></pre></td></tr></tbody></table></figure>

<table>
<thead>
<tr>
<th>项目</th>
<th>加性注意力</th>
<th>点积注意力</th>
</tr>
</thead>
<tbody><tr>
<td>打分函数</td>
<td>MLP + tanh</td>
<td>内积</td>
</tr>
<tr>
<td>数值压缩</td>
<td>有(tanh)</td>
<td>无</td>
</tr>
<tr>
<td>score 差异</td>
<td>小</td>
<td>大</td>
</tr>
<tr>
<td>softmax 后</td>
<td>平滑</td>
<td>尖锐</td>
</tr>
<tr>
<td>输出表现</td>
<td>接近均值</td>
<td>偏向少数 key</td>
</tr>
<tr>
<td>适用场景</td>
<td>适合大规模与精确对齐</td>
<td>适合大规模与精确对齐</td>
</tr>
<tr>
<td>图片</td>
<td><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260108_1535.svg" alt="20260108_1535" style="zoom:67%;"></td>
<td><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260108_1536.svg" alt="20260108_1536" style="zoom:67%;"></td>
</tr>
</tbody></table>
<p>所以点积注意力可能会过度关注某些位置，这也就是为什么transformer引入多头</p>
<h2 id="Bahdanau-注意力"><a href="#Bahdanau-注意力" class="headerlink" title="Bahdanau 注意力"></a>Bahdanau 注意力</h2><p>Bahdanau等人提出了一个没有严格单向对齐限制的 可微注意力模型，在预测词元时，如果不是所有输入词元都相关，模型将仅对齐(或参与)输入序列中与当前预测相关的部分</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260108_1532.webp" alt="20260108_1532"></p>
<h3 id="定义注意力解码器"><a href="#定义注意力解码器" class="headerlink" title="定义注意力解码器"></a>定义注意力解码器</h3><p><code>AttentionDecoder</code>类定义了带有注意力机制解码器的基本接口</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionDecoder</span>(d2l.Decoder):</span><br><span class="line">    <span class="string">"""带有注意力机制解码器的基本接口"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="comment"># 所有带注意力的 Decoder，都应该有一个 attention_weights 这个属性</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></tbody></table></figure>

<p>在接下来的<code>Seq2SeqAttentionDecoder</code>类中实现带有Bahdanau注意力的循环神经网络解码器</p>
<p>首先，初始化解码器的状态，需要下面的输入：</p>
<ol>
<li>编码器在所有时间步的最终层隐状态，将作为注意力的键和值；</li>
<li>上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态；</li>
<li>编码器有效长度(排除在注意力池中填充词元)</li>
</ol>
<p>在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询，注意力输出和输入嵌入都连结为循环神经网络解码器的输入</p>
<blockquote>
<p>num_steps：一句话有多长(时间轴)</p>
<p>num_hiddens：模型在每个位置能记住多少信息(空间轴)</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqAttentionDecoder</span>(<span class="title class_ inherited__">AttentionDecoder</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.attention = d2l.AdditiveAttention(num_hiddens, dropout)</span><br><span class="line">        <span class="comment"># 把 token id → embed_size 向量</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        <span class="comment"># GRU 输入维度是 embed_size + num_hiddens</span></span><br><span class="line">        <span class="comment"># decoder 每个时间步喂给 GRU 的输入不是单纯的 embedding</span></span><br><span class="line">        <span class="comment"># 当前目标词的 embedding + 当前注意力得到的 context</span></span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.GRU(</span><br><span class="line">            embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">            dropout=dropout)</span><br><span class="line">        <span class="comment"># 将 GRU 输出的 hidden → 词表维度 logits(未softmax)</span></span><br><span class="line">        <span class="variable language_">self</span>.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># state 里保存的 enc_outputs 的维度排列必须与 attention 的实现期望一致</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span><br><span class="line">        <span class="comment"># enc_outputs：encoder 每个时间步的输出(给 attention 当 key/value)</span></span><br><span class="line">        <span class="comment"># hidden_state：encoder 的最终隐藏状态(给 decoder 当初始 hidden_state)</span></span><br><span class="line">        <span class="comment"># enc_valid_lens：每个样本真实长度(mask padding，防止注意力看 padding)</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        <span class="comment"># outputs的形状为(batch_size，num_steps，num_hiddens).</span></span><br><span class="line">        <span class="comment"># decoder 的实现通常是“时间步优先”，所以将outputs的时间步放到最开头</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers，batch_size，num_hiddens)</span></span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state, enc_valid_lens)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="comment"># enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class="line">        <span class="comment"># 输出X的形状为(num_steps,batch_size,embed_size)</span></span><br><span class="line">        X = <span class="variable language_">self</span>.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        outputs, <span class="variable language_">self</span>._attention_weights = [], []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            <span class="comment"># 在 decoder 里每个时间步只算 一个 query</span></span><br><span class="line">            <span class="comment"># query的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            query = torch.unsqueeze(hidden_state[-<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># context的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            context = <span class="variable language_">self</span>.attention(</span><br><span class="line">                query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">            <span class="comment"># 在特征维度上连结</span></span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 将x变形为(1,batch_size,embed_size+num_hiddens)</span></span><br><span class="line">            out, hidden_state = <span class="variable language_">self</span>.rnn(x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            <span class="variable language_">self</span>._attention_weights.append(<span class="variable language_">self</span>.attention.attention_weights)</span><br><span class="line">        <span class="comment"># 全连接层变换后，outputs的形状为</span></span><br><span class="line">        <span class="comment"># (num_steps,batch_size,vocab_size)</span></span><br><span class="line">        outputs = <span class="variable language_">self</span>.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                          enc_valid_lens]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._attention_weights</span><br></pre></td></tr></tbody></table></figure>

<p>使用包含7个时间步的4个序列输入的小批量测试Bahdanau注意力解码器</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                             num_layers=<span class="number">2</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                                  num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)  <span class="comment"># (batch_size,num_steps)</span></span><br><span class="line">state = decoder.init_state(encoder(X), <span class="literal">None</span>)</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, <span class="built_in">len</span>(state), state[<span class="number">0</span>].shape, <span class="built_in">len</span>(state[<span class="number">1</span>]), state[<span class="number">1</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([4, 7, 10]), 3, torch.Size([4, 7, 16]), 2, torch.Size([4, 16]))</span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><p>指定超参数，实例化一个带有Bahdanau注意力的编码器和解码器，并对这个模型进行机器翻译训练</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoderCompat</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = encoder</span><br><span class="line">        <span class="variable language_">self</span>.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_X, dec_X, enc_valid_lens</span>):</span><br><span class="line">        <span class="comment"># Encoder</span></span><br><span class="line">        enc_outputs = <span class="variable language_">self</span>.encoder(enc_X, enc_valid_lens)</span><br><span class="line">        <span class="comment"># Decoder init</span></span><br><span class="line">        dec_state = <span class="variable language_">self</span>.decoder.init_state(enc_outputs, enc_valid_lens)</span><br><span class="line">        <span class="comment"># 必须返回 (Y_hat, state)</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decoder(dec_X, dec_state)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">250</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据</span></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encoder / Decoder</span></span><br><span class="line">encoder = d2l.Seq2SeqEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers, dropout</span><br><span class="line">)</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers, dropout</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">net = EncoderDecoderCompat(encoder, decoder)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">d2l.train_seq2seq(</span><br><span class="line">    net, train_iter, lr, num_epochs, tgt_vocab, device</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss 0.020, 10635.4 tokens/sec on cpu</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2026-01-08_16-20-13.webp" alt="Snipaste_2026-01-08_16-20-13" style="zoom:67%;">

<p>模型训练后，用它将几个英语句子翻译成法语并计算它们的BLEU分数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">'go .'</span>, <span class="string">"i lost ."</span>, <span class="string">'he\'s calm .'</span>, <span class="string">'i\'m home .'</span>]</span><br><span class="line">fras = [<span class="string">'va !'</span>, <span class="string">'j\'ai perdu .'</span>, <span class="string">'il est calme .'</span>, <span class="string">'je suis chez moi .'</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'<span class="subst">{eng}</span> =&gt; <span class="subst">{translation}</span>, '</span>,</span><br><span class="line">          <span class="string">f'bleu <span class="subst">{d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">go . =&gt; va !,  bleu 1.000</span><br><span class="line">i lost . =&gt; j'ai perdu .,  bleu 1.000</span><br><span class="line">he's calm . =&gt; il est paresseux .,  bleu 0.658</span><br><span class="line">i'm home . =&gt; je suis chez moi .,  bleu 1.000</span><br></pre></td></tr></tbody></table></figure>

<p>训练结束后，下面通过可视化注意力权重会发现，每个查询都会在键值对上分配不同的权重，这说明在每个解码步中，输入序列的不同部分被选择性地聚集在注意力池中</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.cat([step[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>] <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq], <span class="number">0</span>).reshape((</span><br><span class="line">    <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, num_steps))</span><br><span class="line">    <span class="comment"># 加上一个包含序列结束词元</span></span><br><span class="line">d2l.show_heatmaps(</span><br><span class="line">    attention_weights[:, :, :, :<span class="built_in">len</span>(engs[-<span class="number">1</span>].split()) + <span class="number">1</span>].cpu(),</span><br><span class="line">    xlabel=<span class="string">'Key positions'</span>, ylabel=<span class="string">'Query positions'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260108_1623.svg" alt="20260108_1623"></p>
<h2 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h2><p>给定相同的查询、键和值的集合时，希望模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖关系</p>
<p>可以用独立学习得到的$h$组不同的线性投影(linear projections)来变换查询、键和值</p>
<p>然后，这$h$组变换后的查询、键和值将并行地送到注意力汇聚中，最后将这$h$个注意力汇聚的输出拼接在一起并且通过另一个可以学习的线性投影进行变换以产生最终输出</p>
<p>这种设计被称为多头注意力(multihead attention) (Vaswani <em>et al.</em>, 2017)</p>
<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/multi-head-attention.webp" alt="multi-head-attention"></p>
<h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><p>给定查询$\mathbf{q} \in \mathbb{R}^{d_q}$、键$\mathbf{k} \in \mathbb{R}^{d_k}$和值$\mathbf{v} \in \mathbb{R}^{d_v}$，每个注意力头$\mathbf{h}_i(i = 1, \ldots, h)$的计算方法为：<br>$$<br>\mathbf h_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v},<br>$$<br>可学习的参数包括$\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}$，$\mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}$，$\mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}$以及代表注意力汇聚的函数$f$</p>
<p>$f$可以是加性注意力和缩放点积注意力</p>
<p>多头注意力的输出需要经过另一个线性转换，它对应着$h$个头连结后的结果，因此其可学习参数是$\mathbf W_o\in\mathbb R^{p_o\times h p_v}$</p>
<p>每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数</p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>在实现过程中通常选择缩放点积注意力作为每一个注意力头，为了避免计算代价和参数代价的大幅增长设定$p_q = p_k = p_v = p_o / h$ </p>
<p>如果将查询、键和值的线性变换的输出数量设置为$p_q h = p_k h = p_v h = p_o$则可以并行计算$h$个头</p>
<p>$p_o$是通过参数<code>num_hiddens</code>指定的</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_qkv</span>(<span class="params">X, num_heads</span>):</span><br><span class="line">    <span class="string">"""为了多注意力头的并行计算而变换形状"""</span></span><br><span class="line">    <span class="comment"># 输入X的形状:(batch_size，seq_len，num_hiddens)</span></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size, seq_len, num_heads, head_dim)</span></span><br><span class="line">    X = X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size, num_heads, seq_len, head_dim)</span></span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 因为接下来想把每个 head 当成一个“独立的注意力”在 batch 维度上并行计算</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终输出的形状:(batch_size * num_heads, seq_len, head_dim)</span></span><br><span class="line">    <span class="keyword">return</span> X.reshape(-<span class="number">1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line">    <span class="comment"># 把“多头注意力”转化成“一个 batch 里有很多个普通注意力”,就可以用同一个 attention 函数并行算所有头</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_output</span>(<span class="params">X, num_heads</span>):</span><br><span class="line">    <span class="string">"""逆转transpose_qkv函数的操作"""</span></span><br><span class="line">    <span class="comment"># X.shape: (batch_size * num_heads, seq_len, head_dim)</span></span><br><span class="line">    X = X.reshape(-<span class="number">1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    <span class="comment"># 把 seq_len 放回中间</span></span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 拼回 num_hiddens  (batch_size, seq_len, num_hiddens)</span></span><br><span class="line">    <span class="keyword">return</span> X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">"""多头注意力"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, dropout, bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        <span class="variable language_">self</span>.attention = d2l.DotProductAttention(dropout)</span><br><span class="line">        <span class="comment"># Transformer 的 Q / K / V 不是直接用输入，而是通过线性映射得到的</span></span><br><span class="line">        <span class="variable language_">self</span>.W_q = nn.Linear(query_size, num_hiddens, bias=bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_k = nn.Linear(key_size, num_hiddens, bias=bias)</span><br><span class="line">        <span class="variable language_">self</span>.W_v = nn.Linear(value_size, num_hiddens, bias=bias)</span><br><span class="line">        <span class="comment"># 把拼接后的多头结果，再线性变换一次</span></span><br><span class="line">        <span class="variable language_">self</span>.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        <span class="comment"># queries，keys，values: (batch_size, seq_len, num_hiddens)</span></span><br><span class="line">        <span class="comment"># valid_lens:  # (batch_size，)或(batch_size，seq_len)</span></span><br><span class="line">        <span class="comment"># 经过变换后queries，keys，values形状:</span></span><br><span class="line">        <span class="comment"># (batch_size * num_heads, seq_len, head_dim)</span></span><br><span class="line">        queries = transpose_qkv(<span class="variable language_">self</span>.W_q(queries), <span class="variable language_">self</span>.num_heads)</span><br><span class="line">        keys = transpose_qkv(<span class="variable language_">self</span>.W_k(keys), <span class="variable language_">self</span>.num_heads)</span><br><span class="line">        values = transpose_qkv(<span class="variable language_">self</span>.W_v(values), <span class="variable language_">self</span>.num_heads)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 在轴0，将第一项(标量或者矢量)复制num_heads次，</span></span><br><span class="line">            <span class="comment"># 然后如此复制第二项，然后诸如此类。</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(</span><br><span class="line">                valid_lens, repeats=<span class="variable language_">self</span>.num_heads, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 多个 head 的点积注意力并行计算</span></span><br><span class="line">        <span class="comment"># output的形状:(batch_size*num_heads，查询的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        output = <span class="variable language_">self</span>.attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 合并多头</span></span><br><span class="line">        <span class="comment"># output_concat: (batch_size, seq_len, num_hiddens)</span></span><br><span class="line">        output_concat = transpose_output(output, <span class="variable language_">self</span>.num_heads)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.W_o(output_concat)</span><br></pre></td></tr></tbody></table></figure>

<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/0c016e6fbd3c65402be3536d7eedbbea2c739f1c_2_567x499.png" alt="Tensor Shape in Multihead Attention"></p>
<p>使用键和值相同的小例子来测试编写的<code>MultiHeadAttention</code>类</p>
<p>多头注意力输出的形状是(<code>batch_size</code>，<code>num_queries</code>，<code>num_hiddens</code>)</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">64</span>, <span class="number">8</span></span><br><span class="line">attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                               num_hiddens, num_heads, <span class="number">0.2</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=0.2, inplace=False)</span><br><span class="line">  )</span><br><span class="line">  (W_q): Linear(in_features=64, out_features=64, bias=False)</span><br><span class="line">  (W_k): Linear(in_features=64, out_features=64, bias=False)</span><br><span class="line">  (W_v): Linear(in_features=64, out_features=64, bias=False)</span><br><span class="line">  (W_o): Linear(in_features=64, out_features=64, bias=False)</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">num_kvpairs, valid_lens =  <span class="number">6</span>, torch.tensor([<span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="comment"># X = torch.ones((batch_size, num_queries, num_hiddens))</span></span><br><span class="line"><span class="comment"># Y = torch.ones((batch_size, num_kvpairs, num_hiddens))</span></span><br><span class="line">X = torch.randn((batch_size, num_queries, num_hiddens))</span><br><span class="line">Y = torch.randn((batch_size, num_kvpairs, num_hiddens))</span><br><span class="line">attention(X, Y, Y, valid_lens).shape</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 64])</span><br></pre></td></tr></tbody></table></figure>

<p>分别可视化这个实验中的多个头的注意力权重</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">attn_weights = attention.attention.attention_weights  </span><br><span class="line"><span class="comment"># reshape之前: (batch_size * num_heads, num_queries, num_kvpairs) torch.Size([16, 4, 6])</span></span><br><span class="line"></span><br><span class="line">attn_weights = attn_weights.reshape(</span><br><span class="line">    batch_size,</span><br><span class="line">    num_heads,</span><br><span class="line">    num_queries,</span><br><span class="line">    num_kvpairs</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>

<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260108_1706.webp" alt="20260108_1706"></p>
<h2 id="自注意力和位置编码"><a href="#自注意力和位置编码" class="headerlink" title="自注意力和位置编码"></a>自注意力和位置编码</h2><p>在深度学习中，经常使用卷积神经网络(CNN)或循环神经网络(RNN)对序列进行编码</p>
<p>有了注意力机制之后，将词元序列输入注意力池化中，以便同一组词元同时充当查询、键和值</p>
<p>每个查询都会关注所有的键－值对并生成一个注意力输出，由于查询、键和值来自同一组输入，因此被称为<strong>自注意力(self-attention)</strong> ([Lin <em>et al.</em>, 2017], [Vaswani <em>et al.</em>, 2017])， 也被称为内部注意力(intra-attention) ([Cheng <em>et al.</em>, 2016], [Parikh <em>et al.</em>, 2016], [Paulus <em>et al.</em>, 2017])</p>
<h3 id="自注意力"><a href="#自注意力" class="headerlink" title="自注意力"></a>自注意力</h3><p>给定一个由词元组成的输入序列$\mathbf{x}_1, \ldots, \mathbf{x}_n$，其中任意$\mathbf{x}_i \in \mathbb{R}^d$，该序列的自注意力输出为一个长度相同的序列$\mathbf{y}_1, \ldots, \mathbf{y}_n$<br>$$<br>\mathbf y_i = f(\mathbf x_i, (\mathbf x_1, \mathbf x_1), \ldots, (\mathbf x_n, \mathbf x_n)) \in \mathbb{R}^d<br>$$<br>输出与输入的张量形状相同</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">128</span>, <span class="number">8</span></span><br><span class="line">attention = d2l.MultiHeadAttention(num_hiddens, num_heads, <span class="number">0.2</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=0.2, inplace=False)</span><br><span class="line">  )</span><br><span class="line">  (W_q): LazyLinear(in_features=0, out_features=128, bias=False)</span><br><span class="line">  (W_k): LazyLinear(in_features=0, out_features=128, bias=False)</span><br><span class="line">  (W_v): LazyLinear(in_features=0, out_features=128, bias=False)</span><br><span class="line">  (W_o): LazyLinear(in_features=0, out_features=128, bias=False)</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries, valid_lens = <span class="number">2</span>, <span class="number">4</span>, torch.tensor([<span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">attention(X, X, X, valid_lens).shape</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 128])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="比较卷积神经网络、循环神经网络和自注意力"><a href="#比较卷积神经网络、循环神经网络和自注意力" class="headerlink" title="比较卷积神经网络、循环神经网络和自注意力"></a>比较卷积神经网络、循环神经网络和自注意力</h3><p>比较下面几个架构，目标都是将由$n$个词元组成的序列映射到另一个长度相等的序列，其中的每个输入词元或输出词元都由$d$维向量表示</p>
<p>比较的是卷积神经网络(填充词元被忽略)、循环神经网络和自注意力这几个架构的计算复杂性、顺序操作和最大路径长度</p>
<p>顺序操作会妨碍并行计算，而任意的序列位置组合之间的路径越短，则能更轻松地学习序列中的远距离依赖关系</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/cnn-rnn-self-attention.webp" alt="cnn-rnn-self-attention" style="zoom:80%;">

<p>考虑一个卷积核大小为$k$的卷积层，由于序列长度是$n$，输入和输出的通道数量都是$d$，所以卷积层的计算复杂度为$\mathcal{O}(knd^2)$，卷积神经网络是分层的，因此为有$\mathcal{O}(1)$个顺序操作，最大路径长度为$\mathcal{O}(n/k)$</p>
<p>例如$\mathbf{x}_1$和$\mathbf{x}_5$都在卷积核大小为3的双层卷积神经网络的感受野内</p>
<p>当更新循环神经网络的隐状态时$d \times d$权重矩阵和$d$维隐状态的乘法计算复杂度为$\mathcal{O}(d^2)$，由于序列长度是$n$，因此循环神经网络层的计算复杂度为$\mathcal{O}(nd^2)$，有$\mathcal{O}(n)$个顺序操作无法并行化，最大路径长度也是$\mathcal{O}(n)$</p>
<p>在自注意力中，查询、键和值都是$n \times d$矩阵，考虑缩放的”点－积“注意力，其中$n \times d$乘以$d \times n$矩阵，之后输出的$n \times n$矩阵乘以$n \times d$矩阵，因此，自注意力具有$\mathcal{O}(n^2d)$计算复杂性</p>
<p>每个词元都通过自注意力直接连接到任何其他词元，因此有$\mathcal{O}(1)$个顺序操作可以并行计算，最大路径长度也是$\mathcal{O}(1)$</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>计算复杂度</th>
<th>顺序操作</th>
<th>最大路径</th>
</tr>
</thead>
<tbody><tr>
<td>CNN</td>
<td>O(knd²)</td>
<td>O(1)</td>
<td>O(n/k)</td>
</tr>
<tr>
<td>RNN</td>
<td>O(nd²)</td>
<td>O(n)</td>
<td>O(n)</td>
</tr>
<tr>
<td>Self-Attention</td>
<td>O(n²d)</td>
<td>O(1)</td>
<td>O(1)</td>
</tr>
</tbody></table>
<p>总而言之，卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短</p>
<p>但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢</p>
<h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的，而自注意力则因为并行计算而放弃了顺序操作</p>
<p>为了使用序列的顺序信息，通过在输入表示中添加**位置编码(positional encoding)**来注入绝对的或相对的位置信息</p>
<p>位置编码可以通过学习得到也可以直接固定得到，接下来描述的是基于正弦函数和余弦函数的固定位置编码(Vaswani <em>et al.</em>, 2017)</p>
<p>假设输入表示$\mathbf{X} \in \mathbb{R}^{n \times d}$包含一个序列中$n$个词元的$d$维嵌入表示，位置编码使用相同形状的位置嵌入矩阵$\mathbf{P} \in \mathbb{R}^{n \times d}$输出$\mathbf{X} + \mathbf{P}$，矩阵第$i$行，第$2j$列和$2j+1$列上的元素为：<br>$$<br>\begin{split}\begin{aligned} p_{i, 2j} &amp;= \sin\left(\frac{i}{10000^{2j/d}}\right),\\<br>p_{i, 2j+1} &amp;= \cos\left(\frac{i}{10000^{2j/d}}\right).\end{aligned}\end{split}<br>$$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">"""位置编码"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_hiddens, dropout, max_len=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 创建一个足够长的P</span></span><br><span class="line">        <span class="comment"># P[0, pos, :] = 第 pos 个位置的编码向量</span></span><br><span class="line">        <span class="variable language_">self</span>.P = torch.zeros((<span class="number">1</span>, max_len, num_hiddens))</span><br><span class="line">        <span class="comment"># .reshape(-1, 1) 所有位置编号，纵着排</span></span><br><span class="line">        X = torch.arange(max_len, dtype=torch.float32).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        X = X / torch.<span class="built_in">pow</span>(</span><br><span class="line">            <span class="number">10000</span>,</span><br><span class="line">            torch.arange(<span class="number">0</span>, num_hiddens, <span class="number">2</span>) / num_hiddens</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 偶数维 → 用 sin</span></span><br><span class="line">        <span class="comment"># 奇数维 → 用 cos</span></span><br><span class="line">        <span class="variable language_">self</span>.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(X)</span><br><span class="line">        <span class="variable language_">self</span>.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = X + <span class="variable language_">self</span>.P[:, :X.shape[<span class="number">1</span>], :].to(X.device)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(X)</span><br></pre></td></tr></tbody></table></figure>

<p><strong>为什么要用 sin / cos？</strong></p>
<p>1️⃣ 连续 &amp; 平滑</p>
<ul>
<li>相邻位置 → 编码差异小</li>
<li>远位置 → 编码差异大</li>
</ul>
<p>2️⃣ 不同维度 = 不同频率</p>
<ul>
<li>低维：变化慢(捕捉长距离)</li>
<li>高维：变化快(捕捉局部顺序)</li>
</ul>
<p>3️⃣ 相对位置信息可线性表示：对于任意偏移 k，<code>PE(pos + k)</code> 可以用 <code>PE(pos)</code> 的线性变换表示</p>
<p>不同batch相同位置+相同的位置编码，位置编码不会盖住词向量，只是一个bias，模型后续看到的是“词 + 位置信息的混合体”</p>
<p><strong>顺序信息是怎么“浮现”的？</strong></p>
<p>不同位置有不同<code>PE</code>，即使词相同，只要位置不同$q_i ≠ q_{i+1}$，于是注意力矩阵里得到的 score 不同，相当于间接看见了顺序</p>
<p>在位置嵌入矩阵$\mathbf{P}$中，行代表词元在序列中的位置，列代表位置编码的不同维度</p>
<p>从下面的例子中可以看到位置嵌入矩阵的第6列和第7列的频率高于第8列和第9列，第6列和第7列之间的偏移量(第8列和第9列相同)是由于正弦函数和余弦函数的交替</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoding_dim, num_steps = <span class="number">32</span>, <span class="number">60</span></span><br><span class="line">pos_encoding = PositionalEncoding(encoding_dim, <span class="number">0</span>)</span><br><span class="line">pos_encoding.<span class="built_in">eval</span>()</span><br><span class="line">X = pos_encoding(torch.zeros((<span class="number">1</span>, num_steps, encoding_dim)))</span><br><span class="line">P = pos_encoding.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">d2l.plot(torch.arange(num_steps), P[<span class="number">0</span>, :, <span class="number">6</span>:<span class="number">10</span>].T, xlabel=<span class="string">'Row (position)'</span>,</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">2.5</span>), legend=[<span class="string">"Col %d"</span> % d <span class="keyword">for</span> d <span class="keyword">in</span> torch.arange(<span class="number">6</span>, <span class="number">10</span>)])</span><br></pre></td></tr></tbody></table></figure>

<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260108_1730.svg" alt="20260108_1730"></p>
<h4 id="绝对位置信息"><a href="#绝对位置信息" class="headerlink" title="绝对位置信息"></a>绝对位置信息</h4><p>在二进制表示中，较高比特位的交替频率低于较低比特位</p>
<p>正余弦位置编码，用“不同频率的连续信号”，模拟了二进制中“高位慢变、低位快变”的层级结构，维度越靠后，对应的 sin/cos 频率越低</p>
<p>由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">P = P[<span class="number">0</span>, :, :].unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">d2l.show_heatmaps(P, xlabel=<span class="string">'Column (encoding dimension)'</span>,</span><br><span class="line">                  ylabel=<span class="string">'Row (position)'</span>, figsize=(<span class="number">3.5</span>, <span class="number">4</span>), cmap=<span class="string">'Blues'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260108_1941.svg" alt="20260108_1941"></p>
<hr>
<p>用 d=4 的位置编码，手算 pos=0,1,2 的完整行，这里假设max_len为10000<br>$$<br>\begin{aligned}<br>P E(p o s, 2 i) &amp; =\sin \left(\frac{p o s}{10000^{2 i / d}}\right) \\<br>P E(p o s, 2 i+1) &amp; =\cos \left(\frac{p o s}{10000^{2 i / d}}\right)<br>\end{aligned}<br>$$<br>$d = 4$，那么维度索引：0, 1, 2, 3，所以 $i$ 只能取 0 和 1</p>
<p>对于$i=0$，分母为1，对于$i=1$，分母为100</p>
<p>pos = 0:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PE(0) ≈</span><br><span class="line">[</span><br><span class="line">  0, # sin(0)</span><br><span class="line">  1, # cos(0)</span><br><span class="line">  0, # sin(1)</span><br><span class="line">  1  # cos(1)</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>

<p>pos = 1：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PE(1) ≈</span><br><span class="line">[</span><br><span class="line">  0.8415, # sin(1)</span><br><span class="line">  0.5403, # cos(1)</span><br><span class="line">  0.0100, # sin(1/100)</span><br><span class="line">  0.9999  # cos(1/100)</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>

<p>pos = 2：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PE(2) ≈</span><br><span class="line">[</span><br><span class="line">  0.9093,  # sin(2)</span><br><span class="line"> -0.4161,  # cos(2)</span><br><span class="line">  0.0200,  # sin(2/100)</span><br><span class="line">  0.9998   # cos(2/100)</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>

<h4 id="相对位置信息"><a href="#相对位置信息" class="headerlink" title="相对位置信息"></a>相对位置信息</h4><p>除了捕获绝对位置信息之外，上述的位置编码还允许模型学习得到输入序列中相对位置信息</p>
<p>这是因为对于任何确定的位置偏移$\delta$，位置$i + \delta$处的位置编码可以线性投影位置$i$处的位置编码来表示</p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>Transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络层</p>
<p>尽管Transformer最初是应用于在文本数据上的序列到序列学习，但现在已经推广到各种现代的深度学习中，例如语言、视觉、语音和强化学习领域</p>
<h3 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h3><p>Transformer是由编码器和解码器组成的，与基于Bahdanau注意力实现的序列到序列的学习相比，Transformer的编码器和解码器是基于自注意力的模块叠加而成的，源(输入)序列和目标(输出)序列的<strong>嵌入(embedding)<strong>表示将加上</strong>位置编码(positional encoding)</strong>，再分别输入到编码器和解码器中</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/transformer.webp" alt="transformer" style="zoom:80%;">

<p>Transformer 的编码器由多个结构完全相同的层堆叠而成，每一层包含两个子层</p>
<ol>
<li><p>多头自注意力(Multi-Head Self-Attention)</p>
<p>查询(Query)、键(Key)、值(Value)均来自前一编码器层的输出，用于建模序列中不同位置之间的全局依赖关系</p>
</li>
<li><p>基于位置的前馈网络(Position-wise Feed-Forward Network)</p>
<p>对每个位置的向量独立进行非线性变换，不同位置共享参数</p>
</li>
</ol>
<p>每个子层都采用<strong>残差连接(residual connection)<strong>和</strong>层规范化(layer normalization)</strong></p>
<p>对于序列中任何位置的任何输入$\mathbf{x} \in \mathbb{R}^d$，都要求满足$\mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$，以便满足残差连接</p>
<p>输入序列中每个位置，编码器最终都会输出一个$d$维的上下文相关表示向量</p>
<p>解码器同样由多个相同的层堆叠而成，并且同样使用残差连接和层规范化</p>
<ol>
<li><p>掩蔽多头自注意力(Masked Multi-Head Self-Attention)</p>
<p>只允许当前位置关注之前的位置，通过 mask 阻止“看到未来信息，保证模型具有自回归(auto-regressive)属性</p>
</li>
<li><p>编码器－解码器注意力(Encoder-Decoder Attention)</p>
<p>Query：来自前一解码器层的输出；Key / Value：来自整个编码器的输出</p>
<p>用于在生成过程中选择性关注输入序列的相关部分</p>
</li>
<li><p>FFN结构和编码器相同</p>
</li>
</ol>
<table>
<thead>
<tr>
<th>对比维度</th>
<th>编码器（Encoder）</th>
<th>解码器（Decoder）</th>
</tr>
</thead>
<tbody><tr>
<td>主要功能</td>
<td>理解输入序列</td>
<td>生成输出序列</td>
</tr>
<tr>
<td>是否自回归</td>
<td>否</td>
<td>是</td>
</tr>
<tr>
<td>自注意力</td>
<td>无 mask，可看全序列</td>
<td>有 mask，只看过去</td>
</tr>
<tr>
<td>输入来源</td>
<td>原始输入序列</td>
<td>已生成的输出 + 编码器输出</td>
</tr>
<tr>
<td>输出作用</td>
<td>提供语义表示</td>
<td>逐步预测下一个词元</td>
</tr>
</tbody></table>
<h3 id="基于位置的前馈网络"><a href="#基于位置的前馈网络" class="headerlink" title="基于位置的前馈网络"></a>基于位置的前馈网络</h3><p>基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机(MLP)，这就是称前馈网络是基于位置的(positionwise)的原因</p>
<p>输入<code>X</code>的形状<code>(batch_size，seq_len，d_model)</code>将被一个两层的感知机转换成形状为<code>(batch_size，seq_len，ffn_num_outputs)</code>的输出张量</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFFN</span>(nn.Module):</span><br><span class="line">    <span class="string">"""基于位置的前馈网络"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span></span><br><span class="line"><span class="params">                 **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dense2(<span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.dense1(X)))</span><br></pre></td></tr></tbody></table></figure>

<p>因为用同一个多层感知机对所有位置上的输入进行变换，所以当所有这些位置的输入相同时，它们的输出也是相同的</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ffn = PositionWiseFFN(<span class="number">4</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">ffn.<span class="built_in">eval</span>()</span><br><span class="line">ffn(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))[<span class="number">0</span>]</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.5157,  0.2434,  0.4663, -0.0975,  0.1334, -0.3209, -0.1155, -0.5577],</span><br><span class="line">        [ 0.5157,  0.2434,  0.4663, -0.0975,  0.1334, -0.3209, -0.1155, -0.5577],</span><br><span class="line">        [ 0.5157,  0.2434,  0.4663, -0.0975,  0.1334, -0.3209, -0.1155, -0.5577]],</span><br><span class="line">       grad_fn=&lt;SelectBackward0&gt;)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="残差连接和层规范化"><a href="#残差连接和层规范化" class="headerlink" title="残差连接和层规范化"></a>残差连接和层规范化</h3><p>层规范化和批量规范化的目标相同，但层规范化是基于特征维度进行规范化</p>
<p>尽管批量规范化在计算机视觉中被广泛应用，但在自然语言处理任务中(输入通常是变长序列)批量规范化通常不如层规范化的效果好</p>
<p>BN：跨 batch 统计的规范化</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[B, C, L]  或  [B, C, H, W]</span><br></pre></td></tr></tbody></table></figure>

<p>对每一个通道 C，在整个 batch(以及空间维度)上计算均值和方差</p>
<p><font color="DarkViolet">BN假设：同一个 batch 中，不同样本在统计意义上是“可比的”</font></p>
<p>LN：单个样本内部的规范化</p>
<p>只在当前样本内部，对所有特征维度计算均值和方差</p>
<p><font color="DarkViolet">LN假设：一个 token / 一个时间步的各个特征维度之间是可比的</font></p>
<table>
<thead>
<tr>
<th></th>
<th>BatchNorm</th>
<th>LayerNorm</th>
</tr>
</thead>
<tbody><tr>
<td>统计范围</td>
<td>batch 内多个样本</td>
<td>单个样本</td>
</tr>
<tr>
<td>是否依赖 batch size</td>
<td>强烈依赖</td>
<td>完全不依赖</td>
</tr>
<tr>
<td>是否受序列长度影响</td>
<td>会</td>
<td>不会</td>
</tr>
<tr>
<td>适合任务</td>
<td>图像</td>
<td>NLP / Transformer</td>
</tr>
</tbody></table>
<p>可以使用残差连接和层规范化来实现<code>AddNorm</code>类，暂退法也被作为正则化方法使用</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AddNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">"""残差连接后进行层规范化"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, normalized_shape, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, Y</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.ln(<span class="variable language_">self</span>.dropout(Y) + X)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>有了组成Transformer编码器的基础组件，现在可以先实现编码器中的一个层</p>
<p><code>EncoderBlock</code>类包含两个子层：多头自注意力和基于位置的前馈网络，这两个子层都使用了残差连接和紧随的层规范化</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">"""Transformer编码器块"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_hiddens, norm_shape, ffn_num_input,</span></span><br><span class="line"><span class="params">                 ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.attention = d2l.MultiHeadAttention(</span><br><span class="line">            num_hiddens, num_heads, dropout, use_bias)</span><br><span class="line">        <span class="variable language_">self</span>.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.ffn = PositionWiseFFN(</span><br><span class="line">            ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        <span class="variable language_">self</span>.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, valid_lens</span>):</span><br><span class="line">        Y = <span class="variable language_">self</span>.addnorm1(X, <span class="variable language_">self</span>.attention(X, X, X, valid_lens))</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.addnorm2(Y, <span class="variable language_">self</span>.ffn(Y))</span><br></pre></td></tr></tbody></table></figure>

<p>Transformer编码器中的任何层都不会改变其输入的形状</p>
<p>下面实现的Transformer编码器的代码中，堆叠了<code>num_layers</code>个<code>EncoderBlock</code>类的实例</p>
<p>这里使用的是值范围在-1和1之间的固定位置编码，因此通过学习得到的输入的嵌入表示的值需要先乘以嵌入维度的平方根进行重新缩放，然后再与位置编码相加</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoder</span>(d2l.Encoder):</span><br><span class="line">    <span class="string">"""Transformer编码器"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, norm_shape,</span></span><br><span class="line"><span class="params">                 ffn_num_input, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, num_layers, dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.num_hiddens = num_hiddens</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        <span class="variable language_">self</span>.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            <span class="variable language_">self</span>.blks.add_module(<span class="string">"block"</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                EncoderBlock(num_hiddens, norm_shape, ffn_num_input,</span><br><span class="line">                             ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, use_bias))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, valid_lens, *args</span>):</span><br><span class="line">        <span class="comment"># 因为位置编码值在-1和1之间，</span></span><br><span class="line">        <span class="comment"># 因此嵌入值乘以嵌入维度的平方根进行缩放，</span></span><br><span class="line">        <span class="comment"># 然后再与位置编码相加。</span></span><br><span class="line">        X = <span class="variable language_">self</span>.pos_encoding(<span class="variable language_">self</span>.embedding(X) * math.sqrt(<span class="variable language_">self</span>.num_hiddens))</span><br><span class="line">        <span class="variable language_">self</span>.attention_weights = [<span class="literal">None</span>] * <span class="built_in">len</span>(<span class="variable language_">self</span>.blks)</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.blks):</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">            <span class="variable language_">self</span>.attention_weights[</span><br><span class="line">                i] = blk.attention.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></tbody></table></figure>

<p>指定了超参数来创建一个两层的Transformer编码器，输出的形状是(批量大小，时间步数目，<code>num_hiddens</code>)</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    vocab_size=<span class="number">100</span>,</span><br><span class="line">    num_hiddens=<span class="number">24</span>,</span><br><span class="line">    norm_shape=<span class="number">24</span>,</span><br><span class="line">    ffn_num_input=<span class="number">24</span>,</span><br><span class="line">    ffn_num_hiddens=<span class="number">48</span>,</span><br><span class="line">    num_heads=<span class="number">4</span>,</span><br><span class="line">    num_layers=<span class="number">2</span>,</span><br><span class="line">    dropout=<span class="number">0.5</span></span><br><span class="line">)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">encoder(torch.ones((<span class="number">2</span>, <span class="number">100</span>), dtype=torch.long), valid_lens).shape</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 100, 24])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>Transformer解码器也是由多个相同的层组成，在<code>DecoderBlock</code>类中实现的每个层包含了三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络</p>
<p>为了在解码器中保留自回归的属性，其掩蔽自注意力设定了参数<code>dec_valid_lens</code>，以便任何查询都只会与解码器中所有已经生成词元的位置(即直到该查询位置为止)进行注意力计算</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">"""解码器中第i个块"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_hiddens, norm_shape, ffn_num_input,</span></span><br><span class="line"><span class="params">                 ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, i, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.i = i</span><br><span class="line">        <span class="variable language_">self</span>.attention1 = d2l.MultiHeadAttention(</span><br><span class="line">            num_hiddens, num_heads, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.attention2 = d2l.MultiHeadAttention(</span><br><span class="line">            num_hiddens, num_heads, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                                   num_hiddens)</span><br><span class="line">        <span class="variable language_">self</span>.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        enc_outputs, enc_valid_lens = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 训练阶段，输出序列的所有词元都在同一时间处理，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]初始化为None。</span></span><br><span class="line">        <span class="comment"># 预测阶段，输出序列是通过词元一个接着一个解码的，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span></span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][<span class="variable language_">self</span>.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][<span class="variable language_">self</span>.i], X), axis=<span class="number">1</span>)</span><br><span class="line">        state[<span class="number">2</span>][<span class="variable language_">self</span>.i] = key_values</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.training:</span><br><span class="line">            batch_size, num_steps, _ = X.shape</span><br><span class="line">            <span class="comment"># dec_valid_lens的开头:(batch_size,num_steps),</span></span><br><span class="line">            <span class="comment"># 其中每一行是[1,2,...,num_steps]</span></span><br><span class="line">            dec_valid_lens = torch.arange(</span><br><span class="line">                <span class="number">1</span>, num_steps + <span class="number">1</span>, device=X.device).repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dec_valid_lens = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 自注意力</span></span><br><span class="line">        X2 = <span class="variable language_">self</span>.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = <span class="variable language_">self</span>.addnorm1(X, X2)</span><br><span class="line">        <span class="comment"># 编码器－解码器注意力。</span></span><br><span class="line">        <span class="comment"># enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span></span><br><span class="line">        Y2 = <span class="variable language_">self</span>.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = <span class="variable language_">self</span>.addnorm2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.addnorm3(Z, <span class="variable language_">self</span>.ffn(Z)), state</span><br></pre></td></tr></tbody></table></figure>

<p>假设：生成一句话 y1 y2 y3</p>
<p>t = 1</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入</span></span><br><span class="line">X = [y1]</span><br><span class="line">state[<span class="number">2</span>][i] = <span class="literal">None</span></span><br><span class="line"><span class="comment"># 执行</span></span><br><span class="line">key_values = X  <span class="comment"># [y1]</span></span><br><span class="line">state[<span class="number">2</span>][i] = [y1]</span><br></pre></td></tr></tbody></table></figure>

<p>t = 2</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入</span></span><br><span class="line">X = [y2]</span><br><span class="line">state[<span class="number">2</span>][i] = [y1]</span><br><span class="line"><span class="comment"># 执行</span></span><br><span class="line">key_values = torch.cat(([y1], [y2]), dim=<span class="number">1</span>)</span><br><span class="line">state[<span class="number">2</span>][i] = [y1, y2]</span><br></pre></td></tr></tbody></table></figure>

<p>t = 3</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入</span></span><br><span class="line">X = [y3]</span><br><span class="line">state[<span class="number">2</span>][i] = [y1, y2]</span><br><span class="line"><span class="comment"># 执行</span></span><br><span class="line">key_values = [y1, y2, y3]</span><br><span class="line">state[<span class="number">2</span>][i] = [y1, y2, y3]</span><br></pre></td></tr></tbody></table></figure>

<p><code>state[2][self.i]</code> = 当前 DecoderBlock 在“到目前为止”已经生成的所有 token 表示</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">decoder_blk = DecoderBlock(</span><br><span class="line">    num_hiddens=<span class="number">24</span>,</span><br><span class="line">    norm_shape=<span class="number">24</span>,</span><br><span class="line">    ffn_num_input=<span class="number">24</span>,</span><br><span class="line">    ffn_num_hiddens=<span class="number">48</span>,</span><br><span class="line">    num_heads=<span class="number">8</span>,</span><br><span class="line">    dropout=<span class="number">0.5</span>,</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">)</span><br><span class="line">decoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">state = [encoder_blk(X, valid_lens), valid_lens, [<span class="literal">None</span>]]</span><br><span class="line">decoder_blk(X, state)[<span class="number">0</span>].shape</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 100, 24])</span><br></pre></td></tr></tbody></table></figure>

<p>现在构建了由<code>num_layers</code>个<code>DecoderBlock</code>实例组成的完整的Transformer解码器，最后通过一个全连接层计算所有<code>vocab_size</code>个可能的输出词元的预测值。解码器的自注意力权重和编码器解码器注意力权重都被存储下来，方便日后可视化的需要</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoder</span>(d2l.AttentionDecoder):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, norm_shape, </span></span><br><span class="line"><span class="params">                 ffn_num_input, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, num_layers, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.num_hiddens = num_hiddens</span><br><span class="line">        <span class="variable language_">self</span>.num_layers = num_layers</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        <span class="variable language_">self</span>.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            <span class="variable language_">self</span>.blks.add_module(<span class="string">"block"</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                DecoderBlock(num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, i))</span><br><span class="line">        <span class="variable language_">self</span>.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="literal">None</span>] * <span class="variable language_">self</span>.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        X = <span class="variable language_">self</span>.pos_encoding(<span class="variable language_">self</span>.embedding(X) * math.sqrt(<span class="variable language_">self</span>.num_hiddens))</span><br><span class="line">        <span class="variable language_">self</span>._attention_weights = [[<span class="literal">None</span>] * <span class="built_in">len</span>(<span class="variable language_">self</span>.blks) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.blks):</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">            <span class="comment"># 解码器自注意力权重</span></span><br><span class="line">            <span class="variable language_">self</span>._attention_weights[<span class="number">0</span>][</span><br><span class="line">                i] = blk.attention1.attention.attention_weights</span><br><span class="line">            <span class="comment"># “编码器－解码器”自注意力权重</span></span><br><span class="line">            <span class="variable language_">self</span>._attention_weights[<span class="number">1</span>][</span><br><span class="line">                i] = blk.attention2.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dense(X), state</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._attention_weights</span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练-2"><a href="#训练-2" class="headerlink" title="训练"></a>训练</h3><p>指定Transformer的编码器和解码器都是2层，都使用4头注意力</p>
<p>为了进行序列到序列的学习，下面在“英语－法语”机器翻译数据集上训练Transformer模型</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_layers, dropout, batch_size, num_steps = <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span>, <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">200</span>, d2l.try_gpu()</span><br><span class="line">ffn_num_input, ffn_num_hiddens, num_heads = <span class="number">32</span>, <span class="number">64</span>, <span class="number">4</span></span><br><span class="line">norm_shape = [<span class="number">32</span>]</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab), num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">decoder = TransformerDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab), num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">net = EncoderDecoderCompat(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss 0.029, 8776.3 tokens/sec on cpu</span><br></pre></td></tr></tbody></table></figure>

<p>训练结束后，使用Transformer模型将一些英语句子翻译成法语，并且计算它们的BLEU分数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">'go .'</span>, <span class="string">"i lost ."</span>, <span class="string">'he\'s calm .'</span>, <span class="string">'i\'m home .'</span>]</span><br><span class="line">fras = [<span class="string">'va !'</span>, <span class="string">'j\'ai perdu .'</span>, <span class="string">'il est calme .'</span>, <span class="string">'je suis chez moi .'</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'<span class="subst">{eng}</span> =&gt; <span class="subst">{translation}</span>, '</span>,</span><br><span class="line">          <span class="string">f'bleu <span class="subst">{d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">go . =&gt; va !,  bleu 1.000</span><br><span class="line">i lost . =&gt; j'ai perdu .,  bleu 1.000</span><br><span class="line">he's calm . =&gt; il est calme .,  bleu 1.000</span><br><span class="line">i'm home . =&gt; je suis chez moi .,  bleu 1.000</span><br></pre></td></tr></tbody></table></figure>

<p>当进行最后一个英语到法语的句子翻译工作时，可视化Transformer的注意力权重</p>
<p>编码器自注意力权重的形状为(编码器层数，注意力头数，<code>num_steps</code>或查询的数目，<code>num_steps</code>或“键－值”对的数目)</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    enc_attention_weights.cpu(), xlabel=<span class="string">'Key positions'</span>,</span><br><span class="line">    ylabel=<span class="string">'Query positions'</span>, titles=[<span class="string">'Head %d'</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)],</span><br><span class="line">    figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></tbody></table></figure>

<p>逐行呈现两层多头注意力的权重，每个注意力头都根据查询、键和值的不同的表示子空间来表示不同的注意力</p>
<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/20260108_2154.webp" alt="20260108_2154"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://yhblogs.cn">今天睡够了吗</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://yhblogs.cn/posts/3339.html">http://yhblogs.cn/posts/3339.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yhblogs.cn" target="_blank">がんばろう</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E2%8C%A8%EF%B8%8Fpython/">⌨️python</a></div><div class="post_share"><div class="social-share" data-image="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-lyyx5q_2560x1440.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/30698.html" title="BERT_Pytorch"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7jjyd9_2560x1440.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">BERT_Pytorch</div></div></a></div><div class="next-post pull-right"><a href="/posts/62963.html" title="计算机视觉"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-po97l3_1280x720.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">计算机视觉</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/30698.html" title="BERT_Pytorch"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7jjyd9_2560x1440.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-09</div><div class="title">BERT_Pytorch</div></div></a></div><div><a href="/posts/31208.html" title="FunRec 推荐系统_精排模型"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7j931e_1280x720_(1) (1).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-18</div><div class="title">FunRec 推荐系统_精排模型</div></div></a></div><div><a href="/posts/24333.html" title="FunRec推荐系统_召回模型"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-vpp725_1280x720_(1).webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-14</div><div class="title">FunRec推荐系统_召回模型</div></div></a></div><div><a href="/posts/58676.html" title="Leetcode100记录"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-9ozdyx_1280x720.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-26</div><div class="title">Leetcode100记录</div></div></a></div><div><a href="/posts/22642.html" title="windows安装ROCm"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/ROCm_logo.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-10</div><div class="title">windows安装ROCm</div></div></a></div><div><a href="/posts/3865533702.html" title="pyqt5简单实践"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071521231.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-28</div><div class="title">pyqt5简单实践</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/b_2a1aef95f351a5f7ef72eb81e6838fd6.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">今天睡够了吗</div><div class="author-info__description">相遇是最小单位的奇迹</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071549233.webp" target="_blank" title="QQ"><i class="iconfont icon-QQ"></i></a><a class="social-icon" href="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071549234.webp" target="_blank" title="微信"><i class="iconfont icon-weixin"></i></a><a class="social-icon" href="https://space.bilibili.com/277953459?spm_id_from=333.1007.0.0" target="_blank" title="bilibili"><i class="iconfont icon-bilibili"></i></a><a class="social-icon" href="https://github.com/YaoHui-Wu06022" target="_blank" title="Github"><i class="iconfont icon-GitHub"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">保持理智，相信明天</div><div class="twopeople"><div class="twopeople"><div class="container" style="height:200px;"><canvas class="illo" width="800" height="800" style="max-width: 200px; max-height: 200px; touch-action: none; width: 640px; height: 640px;"></canvas></div> <script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople1.js"></script> <script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/zdog.dist.js"></script> <script id="rendered-js" src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople.js"></script> <style>.twopeople{margin:0;align-items:center;justify-content:center;text-align:center}canvas{display:block;margin:0 auto;cursor:move}</style></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8F%90%E7%A4%BA"><span class="toc-number">1.</span> <span class="toc-text">注意力提示</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E3%80%81%E9%94%AE%E5%92%8C%E5%80%BC"><span class="toc-number">1.1.</span> <span class="toc-text">查询、键和值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.2.</span> <span class="toc-text">注意力的可视化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B1%87%E8%81%9A-Nadaraya-Watson%E6%A0%B8%E5%9B%9E%E5%BD%92"><span class="toc-number">2.</span> <span class="toc-text">注意力汇聚:Nadaraya-Watson核回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.1.</span> <span class="toc-text">生成数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E6%B1%87%E8%81%9A"><span class="toc-number">2.2.</span> <span class="toc-text">平均汇聚</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E5%8F%82%E6%95%B0%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B1%87%E8%81%9A"><span class="toc-number">2.3.</span> <span class="toc-text">非参数注意力汇聚</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%A6%E5%8F%82%E6%95%B0%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B1%87%E8%81%9A"><span class="toc-number">2.4.</span> <span class="toc-text">带参数注意力汇聚</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-number">2.4.1.</span> <span class="toc-text">批量矩阵乘法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.4.2.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">2.4.3.</span> <span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%9D%E8%80%83%E9%A2%98"><span class="toc-number">2.5.</span> <span class="toc-text">思考题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%AF%84%E5%88%86%E5%87%BD%E6%95%B0"><span class="toc-number">3.</span> <span class="toc-text">注意力评分函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A9%E8%94%BDsoftmax%E6%93%8D%E4%BD%9C"><span class="toc-number">3.1.</span> <span class="toc-text">掩蔽softmax操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">3.2.</span> <span class="toc-text">加性注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%A9%E6%94%BE%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">3.3.</span> <span class="toc-text">缩放点积注意力</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bahdanau-%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">4.</span> <span class="toc-text">Bahdanau 注意力</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.1.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">4.2.</span> <span class="toc-text">定义注意力解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="toc-number">4.3.</span> <span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">5.</span> <span class="toc-text">多头注意力</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">5.1.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.2.</span> <span class="toc-text">实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8C%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">6.</span> <span class="toc-text">自注意力和位置编码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">6.1.</span> <span class="toc-text">自注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AF%94%E8%BE%83%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">6.2.</span> <span class="toc-text">比较卷积神经网络、循环神经网络和自注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">6.3.</span> <span class="toc-text">位置编码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF"><span class="toc-number">6.3.1.</span> <span class="toc-text">绝对位置信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF"><span class="toc-number">6.3.2.</span> <span class="toc-text">相对位置信息</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer"><span class="toc-number">7.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B-2"><span class="toc-number">7.1.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E4%BD%8D%E7%BD%AE%E7%9A%84%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C"><span class="toc-number">7.2.</span> <span class="toc-text">基于位置的前馈网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%B1%82%E8%A7%84%E8%8C%83%E5%8C%96"><span class="toc-number">7.3.</span> <span class="toc-text">残差连接和层规范化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">7.4.</span> <span class="toc-text">编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">7.5.</span> <span class="toc-text">解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-2"><span class="toc-number">7.6.</span> <span class="toc-text">训练</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">©2022 - 2026 By 今天睡够了吗</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">You must always have faith in who you are！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="false"></script><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>