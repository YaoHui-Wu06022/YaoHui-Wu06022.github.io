<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深度学习预备知识 | がんばろう</title><meta name="author" content="今天睡够了吗"><meta name="copyright" content="今天睡够了吗"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="学习路径按照 《动手学深度学习》 符号数字  $x$ ：标量 $\mathbf{x}$ ：向量 $\mathbf{X}$ ：矩阵 $\mathsf{X}$：张量 $\mathrm{I}$：单位矩阵 $x_i,[\mathbf{x}]_i$ ：向量 $\mathbf{x}$ 第 $i$ 个元素 $x_{i j},[\mathbf{X}]_{i j}$ ：矩阵 $\mathbf{X}$ 第 $i$ 行">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习预备知识">
<meta property="og:url" content="http://yhblogs.cn/posts/48513.html">
<meta property="og:site_name" content="がんばろう">
<meta property="og:description" content="学习路径按照 《动手学深度学习》 符号数字  $x$ ：标量 $\mathbf{x}$ ：向量 $\mathbf{X}$ ：矩阵 $\mathsf{X}$：张量 $\mathrm{I}$：单位矩阵 $x_i,[\mathbf{x}]_i$ ：向量 $\mathbf{x}$ 第 $i$ 个元素 $x_{i j},[\mathbf{X}]_{i j}$ ：矩阵 $\mathbf{X}$ 第 $i$ 行">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-og3zqp_1280x720.webp">
<meta property="article:published_time" content="2025-10-10T16:31:57.000Z">
<meta property="article:modified_time" content="2026-01-31T12:00:30.727Z">
<meta property="article:author" content="今天睡够了吗">
<meta property="article:tag" content="⌨️python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-og3zqp_1280x720.webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yhblogs.cn/posts/48513.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习预备知识',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-01-31 12:00:30'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_3319458_ks437t3n4r.css"><link rel="stylesheet" href="/css/modify.css"><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/b_2a1aef95f351a5f7ef72eb81e6838fd6.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouye"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw iconfont icon-rili"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw iconfont icon-biaoqian"></i><span> 标签</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="がんばろう"><img class="site-icon" src="/img/favicon.png"><span class="site-name">がんばろう</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouye"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw iconfont icon-rili"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw iconfont icon-biaoqian"></i><span> 标签</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">深度学习预备知识</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-10-10T16:31:57.000Z" title="发表于 2025-10-10 16:31:57">2025-10-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">10.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>42分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="深度学习预备知识"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p>学习路径按照 <a target="_blank" rel="noopener" href="https://zh.d2l.ai/index.html">《动手学深度学习》</a></p>
<h2 id="符号"><a href="#符号" class="headerlink" title="符号"></a>符号</h2><p><strong>数字</strong></p>
<ul>
<li>$x$ ：标量</li>
<li>$\mathbf{x}$ ：向量</li>
<li>$\mathbf{X}$ ：矩阵</li>
<li>$\mathsf{X}$：张量</li>
<li>$\mathrm{I}$：单位矩阵</li>
<li>$x_i,[\mathbf{x}]_i$ ：向量 $\mathbf{x}$ 第 $i$ 个元素</li>
<li>$x_{i j},[\mathbf{X}]_{i j}$ ：矩阵 $\mathbf{X}$ 第 $i$ 行第 $j$ 列的元素</li>
</ul>
<p><strong>集合论</strong></p>
<ul>
<li>$\mathcal{X}$ ：集合</li>
<li>$\mathbb{Z}$ ：整数集合</li>
<li>$\mathbb{R}$ ：实数集合</li>
<li>$\mathbb{R}^n: n$ 维实数向量集合</li>
<li>$\mathbb{R}^{a \times b}$ ：包含 $a$ 行和 $b$ 列的实数矩阵集合</li>
<li>$\mathcal{A} \cup \mathcal{B}$ ：集合 $\mathcal{A}$ 和 $\mathcal{B}$ 的并集</li>
<li>$\mathcal{A} \cap \mathcal{B}$ ：集合 $\mathcal{A}$ 和 $\mathcal{B}$ 的交集</li>
<li>$\mathcal{A} \backslash \mathcal{B}$ ：集合 $\mathcal{A}$ 与集合 $\mathcal{B}$ 相减， $\mathcal{B}$ 关于 $\mathcal{A}$ 的相对补集</li>
</ul>
<p><strong>函数和运算符</strong></p>
<ul>
<li>$f(\cdot)$ ：函数</li>
<li>$\log (\cdot)$ ：自然对数</li>
<li>$\exp (\cdot)$ ：指数函数</li>
<li>$\mathbf{1}_{\boldsymbol{\chi}}$ ：指示函数</li>
<li>$(\cdot)^{\top}$ ：向量或矩阵的转置</li>
<li>$\mathbf{X}^{-1}$ ：矩阵的逆</li>
<li>$\odot$ ：按元素相乘</li>
<li>$[\cdot, \cdot]$ ：连结</li>
<li>$|\mathcal{X}|$ ：集合的基数</li>
<li>$|\cdot|_p: L_p$ 正则</li>
<li>$|\cdot|: L_2$ 正则</li>
<li>$\langle\mathbf{x}, \mathbf{y}\rangle$ ：向量 $\mathbf{x}$ 和 $\mathbf{y}$ 的点积</li>
<li>$\sum$ ：连加</li>
<li>$\Pi $：连乘 </li>
<li>$\stackrel{\mathrm{def}}{=}$：定义</li>
</ul>
<p><strong>微积分</strong></p>
<ul>
<li>$\frac{d y}{d x}$ ：$y$ 关于 $x$ 的导数</li>
<li>$\frac{\partial y}{\partial x}$ ：$y$ 关于 $x$ 的偏导数</li>
<li>$\nabla_{\mathbf{x}} y$ ：$y$ 关于向量 $\mathbf{x}$ 的梯度</li>
<li>$\int_a^b f(x) d x$ ：$f$ 在 $a$ 到 $b$ 区间上关于 $x$ 的定积分</li>
<li>$\int f(x) d x$ ：$f$ 关于 $x$ 的不定积分</li>
</ul>
<p><strong>概率与信息论</strong></p>
<ul>
<li>$P(\cdot)$ ：概率分布</li>
<li>$z \sim P$ ：随机变量 $z$ 具有概率分布 $P$</li>
<li>$P(X \mid Y): X \mid Y$ 的条件概率</li>
<li>$p(x)$ ：概率密度函数</li>
<li>$E_x[f(x)]$ ：函数 $f$ 对 $x$ 的数学期望</li>
<li>$X \perp Y$ ：随机变量 $X$ 和 $Y$ 是独立的</li>
<li>$X \perp Y \mid Z$ ：随机变量 $X$ 和 $Y$ 在给定随机变量 $Z$ 的条件下是独立的</li>
<li>$\operatorname{Var}(X)$ ：随机变量 $X$ 的方差</li>
<li>$\sigma_X$ ：随机变量 $X$ 的标准差</li>
<li>$\operatorname{Cov}(X, Y)$ ：随机变量 $X$ 和 $Y$ 的协方差</li>
<li>$\rho(X, Y)$ ：随机变量 $X$ 和 $Y$ 的相关性</li>
<li>$H(X)$ ：随机变量 $X$ 的熵</li>
<li>$D_{\mathrm{KL}}(P | Q): P$ 和 $Q$ 的KL－散度</li>
</ul>
<p><strong>复杂度</strong></p>
<ul>
<li>$\mathcal{O}$ ：大 O 标记</li>
</ul>
<h2 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h2><p>这一部分和numpy很相像，但不同的是用torch来实现</p>
<p>张量表示一个由数值组成的数组，这个数组可能有多个维度</p>
<ul>
<li><p>具有一个轴的张量对应数学上的<strong>向量(vector)</strong></p>
</li>
<li><p>具有两个轴的张量对应数学上的<strong>矩阵(matrix)</strong></p>
</li>
<li><p>具有两个轴以上的张量没有特殊的数学名称</p>
</li>
</ul>
<p>和numpy的写法基本一致，具体的回头看<a href="/posts/60233.html" title="python基础">python基础</a></p>
<p>创建一个行向量</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">12</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span><br></pre></td></tr></tbody></table></figure>

<p>可以通过<code>shape</code>来访问张量(沿每个轴的长度)的形状，如果想要知道张量中元素的总数，需要通过<code>numel()</code>来访问</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br><span class="line">x.numel()</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([12])</span><br><span class="line">12</span><br></pre></td></tr></tbody></table></figure>

<p>可以通过<code>cat</code>来实现多个张量**连结(concatenate)**在一起，只需要提供张量列表，并给出沿哪个轴连结(0为列，1为行)</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.<span class="built_in">float</span>).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]])</span><br><span class="line">torch.cat((X,Y),dim=<span class="number">0</span>),torch.cat((X,Y),dim=<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">         [ 4.,  5.,  6.,  7.],</span><br><span class="line">         [ 8.,  9., 10., 11.],</span><br><span class="line">         [ 2.,  1.,  4.,  3.],</span><br><span class="line">         [ 1.,  2.,  3.,  4.],</span><br><span class="line">         [ 4.,  3.,  2.,  1.]]),</span><br><span class="line"> tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],</span><br><span class="line">         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],</span><br><span class="line">         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))</span><br></pre></td></tr></tbody></table></figure>

<h3 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h3><p>即使形状不同，仍然可以通过调用**广播机制(broadcasting mechanism)**来执行按元素操作</p>
<p>工作方式如下：</p>
<ol>
<li>通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；</li>
<li>对生成的数组执行按元素操作</li>
</ol>
<p>在大多数情况下将沿着数组中长度为1的轴进行广播</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">a + b</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0, 1],</span><br><span class="line">        [1, 2],</span><br><span class="line">        [2, 3]])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="节省内存"><a href="#节省内存" class="headerlink" title="节省内存"></a>节省内存</h3><p>运行一些操作可能会导致为新结果分配内存</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before = <span class="built_in">id</span>(Y)</span><br><span class="line">Y = Y + X</span><br><span class="line"><span class="built_in">id</span>(Y) == before <span class="comment"># 输出False</span></span><br></pre></td></tr></tbody></table></figure>

<p>主要是因为py自发会创造新地址</p>
<p>在机器学习中可能有数百兆的参数，并且在一秒内多次更新所有参数，通常情况下希望原地执行这些更新；如果不原地更新，其他引用仍然会指向旧的内存位置，可以导致旧参数的引用</p>
<p><font color="Violetred">可以使用切片表示/复合赋值将操作的结果分配给先前分配的数组</font></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z = torch.zeros_like(Y)  <span class="comment"># 假设Z之前已经被创造</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'id(Z):'</span>, <span class="built_in">id</span>(Z))</span><br><span class="line">Z[:] = X + Y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'id(Z):'</span>, <span class="built_in">id</span>(Z))</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">id(Z): 1655337054096</span><br><span class="line">id(Z): 1655337054096</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before = <span class="built_in">id</span>(Y)</span><br><span class="line">Y += X</span><br><span class="line"><span class="built_in">id</span>(Y) == before <span class="comment"># 输出True</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="转化为其它对象"><a href="#转化为其它对象" class="headerlink" title="转化为其它对象"></a>转化为其它对象</h3><p>PyTorch <code>tensor</code> 和 NumPy <code>ndarray</code> 相互转化</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">n = t.numpy()</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">t = torch.from_numpy(n)</span><br></pre></td></tr></tbody></table></figure>

<p>转化后torch张量和numpy数组将共享底层内存，就地操作更改一个张量也会同时更改另一个张量</p>
<p><code>tensor</code>可以调用<code>item</code>函数转为标量或利用Python内置转化函数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line">a, a.item(), <span class="built_in">float</span>(a), <span class="built_in">int</span>(a)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([3.5000]), 3.5, 3.5, 3)</span><br></pre></td></tr></tbody></table></figure>

<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>在Python中常用的数据分析工具中，通常使用<code>pandas</code>，可以与张量兼容</p>
<h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><p>首先创建一个人工数据集，并存储在CSV(逗号分隔值)文件 <code>./data/house_tiny.csv</code>中</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.makedirs(<span class="string">"data"</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">'data'</span>,<span class="string">'house_tiny.csv'</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">'NumRooms,Alley,Price\n'</span>)  <span class="comment"># 列名</span></span><br><span class="line">    f.write(<span class="string">'NA,Pave,127500\n'</span>)  <span class="comment"># 每行表示一个数据样本</span></span><br><span class="line">    f.write(<span class="string">'2,NA,106000\n'</span>)</span><br><span class="line">    f.write(<span class="string">'4,NA,178100\n'</span>)</span><br><span class="line">    f.write(<span class="string">'NA,NA,140000\n'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>要从创建的CSV文件中加载原始数据集，导入<code>pandas</code>包并调用<code>read_csv</code>函数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   NumRooms Alley   Price</span><br><span class="line">0       NaN  Pave  127500</span><br><span class="line">1       2.0   NaN  106000</span><br><span class="line">2       4.0   NaN  178100</span><br><span class="line">3       NaN   NaN  140000</span><br></pre></td></tr></tbody></table></figure>

<h3 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h3><p><font color="DarkViolet">“NaN”项代表缺失值</font>，为了处理缺失的数据，典型的方法包括插值法和删除法，其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值</p>
<p>这里使用插值法(不然没数据了都)，通过位置索引<code>iloc</code>，将<code>data</code>分成<code>inputs</code>和<code>outputs</code>，其中前者为<code>data</code>的前两列，而后者为<code>data</code>的最后一列</p>
<p>对于<code>inputs</code>中NumRooms缺少的数值，用同一列的均值替换“NaN”项</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inputs, outputs = data.iloc[:,<span class="number">0</span>:<span class="number">2</span>], data.iloc[:,<span class="number">2</span>]</span><br><span class="line">inputs = inputs.fillna(inputs.mean(numeric_only=<span class="literal">True</span>)) <span class="comment"># 要加上numeric_only，不然报错</span></span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></tbody></table></figure>

<p>对于<code>inputs</code>中“巷子类型”(“Alley”)列，只接受两种类型的类别值“Pave”和“NaN”，<code>pandas</code>可以把字符串换为独热编码(one-hot encoding)</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = pd.get_dummies(inputs,dummy_na=<span class="literal">True</span>) <span class="comment"># 把字符串换为独热编码(one-hot encoding)</span></span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   NumRooms  Alley_Pave  Alley_nan</span><br><span class="line">0       3.0        True      False</span><br><span class="line">1       2.0       False       True</span><br><span class="line">2       4.0       False       True</span><br><span class="line">3       3.0       False       True</span><br></pre></td></tr></tbody></table></figure>

<h3 id="转换为张量格式"><a href="#转换为张量格式" class="headerlink" title="转换为张量格式"></a>转换为张量格式</h3><p>现在<code>inputs</code>和<code>outputs</code>中的所有条目都是数值类型，它们可以转换为张量格式</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先从pandas对象转为numpy对象，保证浮点数，再转为tensor</span></span><br><span class="line">X = torch.tensor(inputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">Y = torch.tensor(outputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(Y)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[3., 1., 0.],</span><br><span class="line">        [2., 0., 1.],</span><br><span class="line">        [4., 0., 1.],</span><br><span class="line">        [3., 0., 1.]], dtype=torch.float64)</span><br><span class="line">tensor([127500., 106000., 178100., 140000.], dtype=torch.float64)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="删除缺失值最多的列"><a href="#删除缺失值最多的列" class="headerlink" title="删除缺失值最多的列"></a>删除缺失值最多的列</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">drop_col</span>(<span class="params">data</span>):</span><br><span class="line">    num = data.isna().<span class="built_in">sum</span>() <span class="comment"># 统计每列的缺失值数量</span></span><br><span class="line">    col_to_drop = num.idxmax() <span class="comment"># 找出缺失最多的列名</span></span><br><span class="line">    data = data.drop(columns=[col_to_drop]) <span class="comment"># 删除对应列</span></span><br><span class="line">    <span class="comment"># print(data)</span></span><br><span class="line">    <span class="comment"># return data</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><h3 id="标量"><a href="#标量" class="headerlink" title="标量"></a>标量</h3><p>仅包含一个数值被称为<strong>标量(scalar)</strong>，标量由只有一个元素的张量表示</p>
<p>采用了数学表示法，其中标量变量由普通小写字母表示(例如，$x,y$和$z$),用$\mathbb{R}$表示所有(连续)实数标量的空间</p>
<h3 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h3><p>向量可以被视为标量值组成的列表，标量值被称为向量的<strong>元素(element)<strong>或</strong>分量(component)</strong></p>
<p>在数学表示法中，<font color="DarkViolet">向量通常记为粗体、小写的符号</font>(例如，$\mathbf{x},\mathbf{y}$和$\mathbf{z}$)</p>
<p>通过一维张量表示向量，一般来说，张量可以具有任意长度，取决于机器的内存限制</p>
<p>类似于普通的Python数组</p>
<p>长度：<code>len(x)</code>，对于一个二阶及以上张量，<code>len</code>只会输出其第0个维度大小</p>
<p>形状：<code>x.shape</code></p>
<h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><p><font color="DarkViolet">矩阵通常用粗体、大写字母来表示</font>(例如$\mathbf{X},\mathbf{Y},\mathbf{Z}$)，在代码中表示为具有两个轴的张量</p>
<p>数学表示法使$\mathbf{A} \in \mathbb{R}^{m \times n}$来表示矩阵$\mathbf{A}$，其由$m$行和$n$列的实值标量组成<br>$$<br>\begin{split}\mathbf{A}^\top =<br>\begin{bmatrix}<br>    a_{11} &amp; a_{21} &amp; \dots  &amp; a_{m1} \\<br>    a_{12} &amp; a_{22} &amp; \dots  &amp; a_{m2} \\<br>    \vdots &amp; \vdots &amp; \ddots  &amp; \vdots \\<br>    a_{1n} &amp; a_{2n} &amp; \dots  &amp; a_{mn}<br>\end{bmatrix}\end{split}<br>$$<br>行列数相等的矩阵被称为<strong>方阵(square matrix)</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0,  1,  2,  3],</span><br><span class="line">        [ 4,  5,  6,  7],</span><br><span class="line">        [ 8,  9, 10, 11],</span><br><span class="line">        [12, 13, 14, 15],</span><br><span class="line">        [16, 17, 18, 19]])</span><br></pre></td></tr></tbody></table></figure>

<p>交换矩阵的行和列称为矩阵的<strong>转置(transpose)</strong>，通常用$\mathbf{a}^\top$来表示矩阵的转置</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.T</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0,  4,  8, 12, 16],</span><br><span class="line">        [ 1,  5,  9, 13, 17],</span><br><span class="line">        [ 2,  6, 10, 14, 18],</span><br><span class="line">        [ 3,  7, 11, 15, 19]])</span><br></pre></td></tr></tbody></table></figure>

<p>如果$\mathbf{B}=\mathbf{A}^\top$，则对于任意$i$和$j$，都有$b_{ij}=a_{ji}$</p>
<h3 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h3><p>张量是描述具有任意数量轴的$n$维数组的通用方法，向量是一阶张量，矩阵是二阶张量</p>
<p><font color="DarkViolet">张量用特殊字体的大写字母表示</font>(例如$\mathsf{X},\mathsf{Y},\mathsf{Z}$)，索引机制与矩阵类似</p>
<p>图像以$n$维数组形式出现，其中3个轴对应于高度、宽度，以及一个通道(channel)轴，用于表示颜色通道(红色、绿色和蓝色)</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 0,  1,  2,  3],</span><br><span class="line">         [ 4,  5,  6,  7],</span><br><span class="line">         [ 8,  9, 10, 11]],</span><br><span class="line"></span><br><span class="line">        [[12, 13, 14, 15],</span><br><span class="line">         [16, 17, 18, 19],</span><br><span class="line">         [20, 21, 22, 23]]])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="Hadamard积"><a href="#Hadamard积" class="headerlink" title="Hadamard积"></a>Hadamard积</h3><p>任何按元素的一元运算都不会改变其操作数的形状</p>
<p>给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量</p>
<p><font color="Violetred">两个矩阵的按元素乘法称为Hadamard积</font><strong>(Hadamard product)</strong>(数学符号$\odot$)</p>
<blockquote>
<p>要和后面的点积区分开来，点积更像是线性代数课程上讲的矩阵乘法，Hadamard积是矩阵元素的操作</p>
</blockquote>
<p>矩阵$\mathbf{A}$和$\mathbf{B}$的Hadamard积为<br>$$<br>\begin{split}\mathbf{A} \odot \mathbf{B} =<br>\begin{bmatrix}<br>    a_{11}  b_{11} &amp; a_{12}  b_{12} &amp; \dots  &amp; a_{1n}  b_{1n} \\<br>    a_{21}  b_{21} &amp; a_{22}  b_{22} &amp; \dots  &amp; a_{2n}  b_{2n} \\<br>    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>    a_{m1}  b_{m1} &amp; a_{m2}  b_{m2} &amp; \dots  &amp; a_{mn}  b_{mn}<br>\end{bmatrix}\end{split}<br>$$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B = A.clone()</span><br><span class="line">A*B</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[  0,   1,   4,   9],</span><br><span class="line">        [ 16,  25,  36,  49],</span><br><span class="line">        [ 64,  81, 100, 121],</span><br><span class="line">        [144, 169, 196, 225],</span><br><span class="line">        [256, 289, 324, 361]])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><p>对任意张量进行的一个有用的操作是计算其元素的和</p>
<p>数学表示法使用符号$\sum$表示求和</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.shape, A.<span class="built_in">sum</span>()</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([5, 4]), tensor(190.))</span><br></pre></td></tr></tbody></table></figure>

<p>默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量，还可以指定张量沿哪一个轴来通过求和降低维度</p>
<p>为了通过求和所有行的元素来降维(轴0)，可以在调用函数时指定<code>axis=0</code>， 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([40, 45, 50, 55]), torch.Size([4]))</span><br></pre></td></tr></tbody></table></figure>

<p>为什么是这个结果呢，因为0轴(行轴)降维，各行的元素被累加压缩，所以输出的每个元素是行元素相加的结果</p>
<p>同理，指定<code>axis=1</code>将通过汇总所有列的元素降维</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis1 = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">A_sum_axis1, A_sum_axis1.shape</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))</span><br></pre></td></tr></tbody></table></figure>

<blockquote>
<p>注意结果，因为降维了，没有行的概念，从想法上列压缩，剩下的是行，但其实最后的结果是一维张量</p>
</blockquote>
<p>axis可以输入多个轴，对于二阶向量输入<code>[0,1]</code>等价于对矩阵的所有元素进行求和</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># 结果和A.sum()相同</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(190.)</span><br></pre></td></tr></tbody></table></figure>

<p><strong>平均值(mean或average)</strong>，通过将总和除以元素总数来计算平均值，也可以指定轴</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A.mean(), A.<span class="built_in">sum</span>() / A.numel()  <span class="comment"># 所有元素平均值</span></span><br><span class="line">A.mean(axis=<span class="number">0</span>), A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) / A.shape[<span class="number">0</span>]  <span class="comment"># 行平均值</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensor(9.5000), tensor(9.5000))</span><br><span class="line">(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))</span><br></pre></td></tr></tbody></table></figure>

<h4 id="非降维求和"><a href="#非降维求和" class="headerlink" title="非降维求和"></a>非降维求和</h4><p>有时在调用函数来计算总和或均值时保持轴数不变会很有用，加入<code>keepdims</code>参数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">sum_A</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 6.],</span><br><span class="line">        [22.],</span><br><span class="line">        [38.],</span><br><span class="line">        [54.],</span><br><span class="line">        [70.]])</span><br></pre></td></tr></tbody></table></figure>

<p>由于<code>sum_A</code>在对每行进行求和后仍保持两个轴，可以通过广播将<code>A</code>除以<code>sum_A</code>，实现对每行元素除以行平均数，如果没有<code>keepdims</code>将报错</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A / sum_A</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.0000, 0.1667, 0.3333, 0.5000],</span><br><span class="line">        [0.1818, 0.2273, 0.2727, 0.3182],</span><br><span class="line">        [0.2105, 0.2368, 0.2632, 0.2895],</span><br><span class="line">        [0.2222, 0.2407, 0.2593, 0.2778],</span><br><span class="line">        [0.2286, 0.2429, 0.2571, 0.2714]])</span><br></pre></td></tr></tbody></table></figure>

<p>如果想沿某个轴计算<code>A</code>元素的累积总和，可以调用<code>cumsum</code>函数，不会沿任何轴降低输入张量的维度</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.cumsum(axis=<span class="number">0</span>) <span class="comment"># 按行计算</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">        [ 4.,  6.,  8., 10.],</span><br><span class="line">        [12., 15., 18., 21.],</span><br><span class="line">        [24., 28., 32., 36.],</span><br><span class="line">        [40., 45., 50., 55.]])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="点积"><a href="#点积" class="headerlink" title="点积"></a>点积</h3><p>给定两个向量$\mathbf{x},\mathbf{y}\in\mathbb{R}^d$，<strong>点积(dot product)</strong>$\mathbf{x}^\top\mathbf{y} $(或$\langle\mathbf{x},\mathbf{y}\rangle$) 是相同位置的按元素乘积的和<br>$$<br>\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i<br>$$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>,dtype=torch.float32)</span><br><span class="line">y = torch.ones(<span class="number">4</span>,dtype=torch.float32)</span><br><span class="line">x, y, x@y</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))</span><br></pre></td></tr></tbody></table></figure>

<p>点积在很多场合都很有用，给定一组由向量$\mathbf{x} \in \mathbb{R}^d$表示的值，和一组由$\mathbf{w} \in \mathbb{R}^d$表示的权重，$\mathbf{x}$中的值根据权重$\mathbf{w}$的加权和，可以表示为点积$\mathbf{x}^\top \mathbf{w}$</p>
<p>当权重为非负数且和为1$\left(\sum_{i=1}^{d}{w_i}=1\right)$，点积表示<strong>加权平均(weighted average)</strong></p>
<p>将两个向量规范化得到单位长度后，点积表示它们夹角的余弦</p>
<h3 id="矩阵-向量积"><a href="#矩阵-向量积" class="headerlink" title="矩阵-向量积"></a>矩阵-向量积</h3><p>之前定义的$\mathbf{A} \in \mathbb{R}^{m \times n}$和向量$\mathbf{x} \in \mathbb{R}^n$，将矩阵$\mathbf{A}$用它的行向量表示<br>$$<br>\begin{split}\mathbf{A}=<br>\begin{bmatrix}<br>\mathbf{a}^\top_{1} \\<br>\mathbf{a}^\top_{2} \\<br>\vdots \\<br>\mathbf{a}^\top_m \\<br>\end{bmatrix}\end{split}<br>$$<br>其中每个$\mathbf{a}^\top_{i} \in \mathbb{R}^n$都是行向量，表示矩阵的第$i$行，矩阵向量积是一个长度为$m$的列向量，其第$i$个元素是点积$\mathbf{a}^\top_i \mathbf{x}$</p>
<blockquote>
<p>在深度学习中一般把以列向量为准，转置符号表示这是一个行向量，这样就很容易思考了</p>
</blockquote>
<p>$$<br>\begin{split}\mathbf{A}\mathbf{x}<br>= \begin{bmatrix}<br>\mathbf{a}^\top_{1} \\<br>\mathbf{a}^\top_{2} \\<br>\vdots \\<br>\mathbf{a}^\top_m \<br>\end{bmatrix}\mathbf{x}<br>= \begin{bmatrix}<br> \mathbf{a}^\top_{1} \mathbf{x}  \\<br> \mathbf{a}^\top_{2} \mathbf{x} \\<br>\vdots\\<br> \mathbf{a}^\top_{m} \mathbf{x}\<br>\end{bmatrix}\end{split}<br>$$<br>可以把一个矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$乘法看作一个从$\mathbb{R}^{n}$到$\mathbb{R}^{m}$向量的转换，这些转换是非常有用的，例如可以用方阵的乘法来表示旋转</p>
<p>在代码中**矩阵-向量积(matrix–vector product)**使用<code>mv</code>函数表示</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mv(input, vec) → Tensor</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><p><code>input</code> 必须是二维张量(shape = <code>(m, n)</code>)</p>
</li>
<li><p><code>vec</code> 必须是一维张量(shape = <code>(n,)</code>)</p>
</li>
<li><p>返回结果是一个一维张量(shape = <code>(m,)</code>)</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">A = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">y = torch.mv(A, x) <span class="comment"># 要求输入的数据类型相同</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([14, 32])</span><br></pre></td></tr></tbody></table></figure>

<p>$$<br>\left[\begin{array}{lll}<br>1 &amp; 2 &amp; 3 \\<br>4 &amp; 5 &amp; 6<br>\end{array}\right]\left[\begin{array}{l}<br>1 \\<br>2 \\<br>3<br>\end{array}\right]=\left[\begin{array}{l}<br>1 \times 1+2 \times 2+3 \times 3 \\<br>4 \times 1+5 \times 2+6 \times 3<br>\end{array}\right]=\left[\begin{array}{l}<br>14 \\<br>32<br>\end{array}\right]<br>$$</p>
<h3 id="矩阵-矩阵乘法"><a href="#矩阵-矩阵乘法" class="headerlink" title="矩阵-矩阵乘法"></a>矩阵-矩阵乘法</h3><p>假设有两个矩阵$\mathbf{A} \in \mathbb{R}^{n \times k}$ 和 $\mathbf{B} \in \mathbb{R}^{k \times m}$<br>$$<br>\begin{split}\mathbf{A}=\begin{bmatrix}<br> a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1k} \\<br> a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2k} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br> a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nk} \\<br>\end{bmatrix},\quad<br>\mathbf{B}=\begin{bmatrix}<br> b_{11} &amp; b_{12} &amp; \cdots &amp; b_{1m} \\<br> b_{21} &amp; b_{22} &amp; \cdots &amp; b_{2m} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br> b_{k1} &amp; b_{k2} &amp; \cdots &amp; b_{km} \\<br>\end{bmatrix}.\end{split}<br>$$<br>用行向量$\mathbf{a}^\top_{i} \in \mathbb{R}^k$表示矩阵$\mathbf{A}$的第$i$行，并让列向量$\mathbf{b}_{j} \in \mathbb{R}^k$作为矩阵$\mathbf{B}$的第$j$列</p>
<p>要生成矩阵积$\mathbf{C} = \mathbf{A}\mathbf{B}$，最简单的方法是考虑$\mathbf{A}$的行向量和$\mathbf{B}$的列向量，$\mathbf{B}=<br>\begin{bmatrix}<br>\mathbf{b}_1 &amp; \mathbf{b}_2 &amp; \cdots &amp; \mathbf{b}_m<br>\end{bmatrix}$</p>
<p>将每个元素$c_{ij}$计算为点积$\mathbf{a}^\top_i \mathbf{b}_j$</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2025-10-12_15-00-44.webp" alt="Snipaste_2025-10-12_15-00-44" style="zoom: 67%;">

<p>将矩阵-矩阵乘法看作简单地执行$m$次矩阵-向量积，并将结果拼接在一起，形成一个$n \times m$矩阵</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>).reshape((<span class="number">5</span>,<span class="number">4</span>)).<span class="built_in">float</span>()</span><br><span class="line">B = torch.ones(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">torch.mm(A,B)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 6.,  6.,  6.],</span><br><span class="line">        [22., 22., 22.],</span><br><span class="line">        [38., 38., 38.],</span><br><span class="line">        [54., 54., 54.],</span><br><span class="line">        [70., 70., 70.]])</span><br></pre></td></tr></tbody></table></figure>

<p><font color="Violetred">要与Hadamard积区别开，矩阵乘法并不是按元素乘法</font></p>
<h4 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h4><p>非正式地说，向量的范数是表示一个向量有多大，不涉及维度，而是分量的大小</p>
<p>向量范数是将向量映射到标量的函数$f$</p>
<p>给定任意向量$\mathbf{x}$，向量范数要满足一些属性</p>
<ul>
<li><p>$$<br>f(\alpha \mathbf{x}) = |\alpha| f(\mathbf{x}).<br>$$</p>
</li>
<li><p>$$<br>f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y}).<br>$$</p>
</li>
<li><p>$$<br>f(\mathbf{x}) \geq 0.<br>$$</p>
</li>
</ul>
<p>范数听起来很像距离的度量</p>
<p>欧几里得距离是一个$L_2$范数<br>$$<br>\mid \mid \mathbf{x}\mid \mid_2 = \sqrt{\sum_{i=1}^n x_i^2},<br>$$<br>在$L_2$范数中常常省略下标2，也就是$\mid \mid \mathbf{x}\mid \mid$等价于$\mid \mid \mathbf{x}\mid \mid_2$</p>
<p>代码中可以直接用<code>norm</code>计算</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u = torch.tensor([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line">torch.norm(u)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(5.)</span><br></pre></td></tr></tbody></table></figure>

<p>深度学习中更经常地使用$L_2$范数的平方，也会经常遇到$L_1$范数，它表示为向量元素的绝对值之和<br>$$<br>\mid \mid \mathbf{x}\mid \mid_1 = \sum_{i=1}^n \left|x_i \right|.<br>$$<br>与$L_2$范数相比，$L_1$范数受异常值的影响较小，将绝对值函数和按元素求和组合起来</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>()</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(7.)</span><br></pre></td></tr></tbody></table></figure>

<p>$L_2$范数和$L_1$范数都是更一般的$L_p$范数的特例<br>$$<br>\mid \mid\mathbf{x}\mid \mid_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.<br>$$<br>类似于向量的$L_2$范数，<font color="DarkViolet">矩阵$\mathbf{X} \in \mathbb{R}^{m \times n}$的Frobenius范数(Frobenius norm)</font>是矩阵元素平方和的平方根：<br>$$<br>\mid \mid\mathbf{X}\mid \mid_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}.<br>$$<br>Frobenius范数满足向量范数的所有性质，就像是矩阵形向量的$L_2$范数</p>
<p>仍然调用<code>norm</code>函数计算矩阵的Frobenius范数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(torch.ones((<span class="number">4</span>, <span class="number">9</span>)))</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(6.)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="范数和目标"><a href="#范数和目标" class="headerlink" title="范数和目标"></a>范数和目标</h4><p>在深度学习中经常试图解决优化问题：最大化分配给观测数据的概率; 最小化预测和真实观测之间的距离，用向量表示物品(如单词、产品或新闻文章)，以便最小化相似项目之间的距离，最大化不同项目之间的距离</p>
<h2 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h2><h3 id="导数和微分"><a href="#导数和微分" class="headerlink" title="导数和微分"></a>导数和微分</h3><p>在深度学习中，通常选择对于模型参数可微的损失函数。简而言之，对于每个参数，如果把这个参数增加或减少一个无穷小的量，可以知道损失会以多快的速度增加或减少</p>
<p>假设有一个函数$f: \mathbb{R} \rightarrow \mathbb{R}$，其输入和输出都是标量。如果$f$的导数存在，这个极限被定义为<br>$$<br>f’(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}.<br>$$<br>如果$f’(a)$存在，则称在处是**可微(differentiable)**的，导数$f’(x)$解释为相对于$x$的瞬时变化率</p>
<p>给定$y=f(x)$，其中$x$和$y$分别是函数的自变量和因变量，以下表达式是等价的<br>$$<br>f’(x) = y’ = \frac{dy}{dx} = \frac{df}{dx} = \frac{d}{dx} f(x) = Df(x) = D_x f(x)<br>$$<br>其中符号$\frac{d}{dx}$和$D$是微分运算符，表示微分操作</p>
<ul>
<li>$DC = 0$($C$为常数)</li>
<li>$Dx^n = nx^{n-1}$</li>
<li>$De^x = e^x$</li>
<li>$D\ln(x) = 1/x$</li>
</ul>
<h3 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h3><p>设$y = f(x_1, x_2, \ldots, x_n)$是一个具有$n$个变量的函数，$y$关于第$i$个参数$x_i$的**偏导数(partial derivative)**为<br>$$<br>\frac{\partial y}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.<br>$$<br>对于偏导数的表示，以下是等价的：<br>$$<br>\frac{\partial y}{\partial x_i} = \frac{\partial f}{\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.<br>$$</p>
<h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>可以连结一个多元函数对其所有变量的偏导数，以得到该函数的**梯度(gradient)**向量</p>
<p>设函数$f:\mathbb{R}^n\rightarrow\mathbb{R}$的输入是一个$n$维向量$\mathbf{x}=[x_1,x_2,\ldots,x_n]^\top$，并且输出是一个标量</p>
<p>函数$f(\mathbf{x})$相对于$\mathbf{x}$的梯度是一个包含$n$个偏导数的向量<br>$$<br>\nabla_{\mathbf{x}} f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n}\bigg]^\top<br>$$<br>$\nabla_{\mathbf{x}} f(\mathbf{x})$通常在没有歧义时被$\nabla f(\mathbf{x})$取代</p>
<p>在微分多元函数时经常使用以下规则：(python默认分子布局，结论改写成分子布局的形式)</p>
<ul>
<li><p>对于所有$\mathbf{A} \in \mathbb{R}^{m \times n}$，都有$\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x} = \mathbf{A}$</p>
</li>
<li><p>对于所有$\mathbf{A} \in \mathbb{R}^{n \times m}$，都有$\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} = \mathbf{A}^\top$</p>
</li>
<li><p>对于所有$\mathbf{A} \in \mathbb{R}^{n \times n}$，都有$\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x} = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}$</p>
<p>如果$\mathbf{A}$对称，则导数更简化为$\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x} =2\mathbf{A}\mathbf{x}$，这是神经网络里常见的“平方损失”形式</p>
</li>
<li><p>$\nabla_{\mathbf{x}} \mid\mid\mathbf{x} \mid\mid^2 = \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2\mathbf{x}$</p>
</li>
</ul>
<p>对于任何矩阵$\mathbf{X}$，都有$\nabla_{\mathbf{X}} |\mathbf{X} |_F^2 = 2\mathbf{X}$</p>
<h3 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h3><p>假设函数$y=f(u)$和$u=g(x)$都是可微的<br>$$<br>\frac{dy}{dx} = \frac{dy}{du} \frac{du}{dx}<br>$$</p>
<h3 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h3><ol>
<li><p>绘制函数$y = f(x) = x^3 - \frac{1}{x}$和其在$x = 1$处切线的图像</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">3</span> - <span class="number">1</span>/x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fp</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">3</span>*x**<span class="number">2</span> + <span class="number">1</span>/x**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tangent line: y = m(x - x1) + y1</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tangent</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> m*(x - x1) + y1</span><br><span class="line"></span><br><span class="line">x1 = <span class="number">1.0</span></span><br><span class="line">y1 = f(x1)</span><br><span class="line">m = fp(x1)</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0.1</span>,<span class="number">2</span>,<span class="number">400</span>)</span><br><span class="line">plt.plot(x,f(x),label=<span class="string">r"$y=x^3-\frac{1}{x}$"</span>)</span><br><span class="line">plt.plot(x,tangent(x),label=<span class="string">"Tangent"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>)</span><br><span class="line">plt.title(<span class="string">r" $y=x^3-\frac{1}{x}$ Tangent in $x=1$ "</span>)</span><br><span class="line">plt.scatter([x1], [y1], s=<span class="number">40</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510121636.webp" alt="202510121636" style="zoom: 67%;">
</li>
<li><p>函数$f(\mathbf{x}) = \mid\mid\mathbf{x}\mid\mid$的梯度是什么?<br>$$<br>\mid\mid x\mid\mid=\left(x^{\top} x\right)^{1 / 2}\\<br>\nabla\mid\mid x\mid\mid=2 x\cdot\frac{1}{2}\left(x^{\top} x\right)^{-1 / 2}  =\frac{x}{\mid\mid x \mid\mid}<br>$$<br>矢量L2范数关于矢量自身的梯度是与矢量方向相同的单位矢量</p>
</li>
<li><p>尝试写出函数$u = f(x, y, z)$，其中$x = x(a, b)$，$y = y(a, b)$，$z = z(a, b)$的链式法则<br>$$<br>\begin{aligned}<br>&amp; \frac{\partial u}{\partial a}=f_x \frac{\partial x}{\partial a}+f_y \frac{\partial y}{\partial a}+f_z \frac{\partial z}{\partial a} \\<br>&amp; \frac{\partial u}{\partial b}=f_x \frac{\partial x}{\partial b}+f_y \frac{\partial y}{\partial b}+f_z \frac{\partial z}{\partial b}<br>\end{aligned}<br>$$</p>
<p>其中 $f_x, f_y, f_z$ 都在 $(x(a, b), y(a, b), z(a, b))$ 处取值</p>
</li>
</ol>
<h2 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h2><p>求导是几乎所有深度学习优化算法的关键步骤，但对于复杂的模型，手工进行更新是一件很痛苦的事情(而且经常容易出错)</p>
<p>深度学习框架通过自动计算导数，即**自动微分(automatic differentiation)**来加快求导</p>
<p>根据设计好的模型，系统会构建一个<strong>计算图(computational graph)</strong>，来跟踪计算是哪些数据通过哪些操作组合起来产生输出。自动微分使系统能够随后反向传播梯度</p>
<p>**反向传播(backpropagate)**意味着跟踪整个计算图，填充关于每个参数的偏导数</p>
<h3 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h3><p>求导之前需要一个地方来存储梯度，并且不会在每次对一个参数求导时都分配新的内存</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># requires_grad 启动用于追踪梯度，注意需要float类型</span></span><br><span class="line">x.grad  <span class="comment"># 默认值是None</span></span><br></pre></td></tr></tbody></table></figure>

<p>计算y</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = <span class="number">2</span> * x@x  <span class="comment"># tensor(28., grad_fn=&lt;DotBackward0&gt;)</span></span><br></pre></td></tr></tbody></table></figure>

<p>通过调用反向传播函数来自动计算<code>y</code>关于<code>x</code>每个分量的梯度，并打印这些梯度</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.,  4.,  8., 12.])</span><br></pre></td></tr></tbody></table></figure>

<p>函数$y=2\mathbf{x}^{\top}\mathbf{x}$关于$\mathbf{x}$的梯度应为$4\mathbf{x}$，符合</p>
<p>计算<code>x</code>的另一个函数的导数前，<font color="DarkViolet">要记得清除梯度</font>，不然梯度会累积</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_() <span class="comment"># 清除累积</span></span><br><span class="line">y = x.<span class="built_in">sum</span>()</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 1., 1., 1.])</span><br></pre></td></tr></tbody></table></figure>

<p><code>backward</code> 做的是向量–雅可比积，<font color="DarkViolet">根据链式法则乘上局部雅可比矩阵</font></p>
<p>若 <code>y</code> 是标量，数学上反向传播的起点只有一个数，默认就是 1，不需要专门输入</p>
<p>若 <code>y</code> 是张量，上游梯度是一个与 <code>y</code> 同形状的张量$v$，如果不告诉它$v$是什么无法计算</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.backward(torch.ones_like(y)) <span class="comment"># 选择全1方向</span></span><br></pre></td></tr></tbody></table></figure>

<p>因为<code>sum</code>的雅可比就是全1，所以常这么写</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="comment"># == y.backward( torch.ones_like(y) )</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="非标量变量的反向传播"><a href="#非标量变量的反向传播" class="headerlink" title="非标量变量的反向传播"></a>非标量变量的反向传播</h3><p>当<code>y</code>不是标量时，向量<code>y</code>关于向量<code>x</code>的导数的最自然解释是一个矩阵</p>
<p>对于高阶和高维的<code>y</code>和<code>x</code>，求导的结果可以是一个高阶张量</p>
<p>对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x * <span class="number">3</span></span><br><span class="line">z = y ** <span class="number">2</span></span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="comment"># z.backward() # 非标量输出反向传播时如果没有指定gradient将会报错</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[18., 18.],</span><br><span class="line">        [18., 18.]])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="分离计算"><a href="#分离计算" class="headerlink" title="分离计算"></a>分离计算</h3><p>有时希望将某些计算移动到记录的计算图之外</p>
<p>假设<code>y</code>是作为<code>x</code>的函数计算的，而<code>z</code>则是作为<code>y</code>和<code>x</code>的函数计算的</p>
<p>想计算<code>z</code>关于<code>x</code>的梯度，但由于某种原因，希望将<code>y</code>视为一个常数，并且只考虑到<code>x</code>在<code>y</code>被计算后发挥的作用</p>
<p>可以分离<code>y</code>来返回一个新变量<code>u</code>，该变量与<code>y</code>具有相同的值，但丢弃计算图中如何计算<code>y</code>的任何信息，梯度不会向后流经<code>u</code>到<code>x</code></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x*x</span><br><span class="line">u = y.detach() <span class="comment"># 分离出一个新的张量，但共享相同的数据存储</span></span><br><span class="line">z = u * x</span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad == u)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[True, True],</span><br><span class="line">        [True, True]])</span><br></pre></td></tr></tbody></table></figure>

<p>同时也可以在<code>y</code>上调用反向传播，得到<code>y=x*x</code>关于的<code>x</code>的导数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad == <span class="number">2</span>*x)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[True, True],</span><br><span class="line">        [True, True]])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="控制流的梯度计算"><a href="#控制流的梯度计算" class="headerlink" title="控制流的梯度计算"></a>控制流的梯度计算</h3><p>使用自动微分的一个好处是，即使构建函数的计算图需要通过Python控制流(例如，条件、循环或任意函数调用)，仍然可以计算得到的变量的梯度</p>
<p>在下面的代码中，<code>while</code>循环的迭代次数和<code>if</code>语句的结果都取决于输入<code>a</code>的值</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">    b = a*<span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm()&lt;<span class="number">1000</span>:</span><br><span class="line">        b = b*<span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> *b</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line">a.grad.zero_()</span><br><span class="line">a = torch.randn(size=(), requires_grad=<span class="literal">True</span>) <span class="comment"># () 标量，没有维度</span></span><br><span class="line">d = f(a)</span><br><span class="line">d.backward() <span class="comment"># 标量无所谓</span></span><br><span class="line"><span class="built_in">print</span>(a.grad == d/a)</span><br><span class="line"><span class="comment"># tensor(True)</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="练习题-1"><a href="#练习题-1" class="headerlink" title="练习题"></a>练习题</h3><ol>
<li><p>为什么计算二阶导数比一阶导数的开销要更大？</p>
<p>设有函数$f:\mathbb{R}^{n}\rightarrow \mathbb{R}$</p>
<p>一阶导数是一个长度为$n$的向量，PyTorch的默认方式是使用反向模式自动微分(reverse-mode AD)，只要做一次反向传播，就能得到整个梯度向量</p>
<p>二阶导数是一个$n\times n$的矩阵(Hessian(海森矩阵))，要完整求出Hessian，就得重复计算一阶梯度$n$次，每次对不同的分量求导</p>
<p>复杂度从$O(n)$变成了$O(n^2)$</p>
</li>
<li><p>在控制流的例子中，计算<code>d</code>关于<code>a</code>的导数，如果将变量<code>a</code>更改为随机向量或矩阵，会发生什么？</p>
<p>将会报错，因为没有此时不是标量了，没有指定gradient将会报错，改成这个比较稳定，标量或者张量都不会出现报错</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d.<span class="built_in">sum</span>().backward()</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>使$f(x)=\sin(x)$，绘制$f(x)$和$\frac{df(x)}{dx}$的图像，其中后者不使用$f’(x)=\cos(x)$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = torch.linspace(<span class="number">0</span>,<span class="number">2</span>*np.pi,<span class="number">100</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">fx = torch.sin(x)</span><br><span class="line">fx.<span class="built_in">sum</span>().backward()</span><br><span class="line">dfx = x.grad</span><br><span class="line">plt.plot(x.detach().numpy(),fx.detach().numpy(),label=<span class="string">"f(x)=sin(x)"</span>)</span><br><span class="line">plt.plot(x.detach().numpy(), dfx.detach().numpy(), label=<span class="string">"dfx"</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510122256.webp" alt="202510122256" style="zoom: 67%;"></li>
</ol>
<h2 id="概率"><a href="#概率" class="headerlink" title="概率"></a>概率</h2><p>简单地说，机器学习就是做出预测</p>
<h3 id="基本概率论"><a href="#基本概率论" class="headerlink" title="基本概率论"></a>基本概率论</h3><p>检查骰子的唯一方法是多次投掷并记录结果</p>
<p>对于每个骰子将观察到${1, \ldots, 6}$中的一个值，对于每个值，一种自然的方法是将它出现的次数除以投掷的总次数，即此**事件(event)**概率的估计值</p>
<p>**大数定律(law of large numbers)**指出：随着投掷次数的增加，这个估计值会越来越接近真实的潜在概率</p>
<p>在统计学中，把从概率分布中抽取样本的过程称为<strong>抽样(sampling)</strong>，笼统来说可以把**分布(distribution)**看作对事件的概率分配</p>
<p>将概率分配给一些离散选择的分布称为<strong>多项分布(multinomial distribution)</strong></p>
<p>为了抽取一个样本，即掷骰子，只需传入一个概率向量，输出是另一个相同长度的向量，它在索引处的值是采样结果中出现的次数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fair_probs = torch.ones([<span class="number">6</span>]) / <span class="number">6</span> <span class="comment"># 分配概率</span></span><br><span class="line">multinomial.Multinomial(<span class="number">1</span>, fair_probs).sample() <span class="comment"># 进行采样</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 0., 0., 1., 0., 0.])</span><br></pre></td></tr></tbody></table></figure>

<p>希望从同一分布中生成多个样本，如果用Python的for循环来完成这个任务，速度会慢得惊人</p>
<p>因此使用深度学习框架的函数同时抽取多个样本，得到想要的任意形状的独立样本数组</p>
<p>可以统计1000次投掷后，每个数字被投中了多少次，计算相对频率，以作为真实概率的估计</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fair_probs = torch.ones([<span class="number">6</span>]) / <span class="number">6</span></span><br><span class="line">fre = <span class="number">1000</span></span><br><span class="line">counts = multinomial.Multinomial(fre, fair_probs).sample()</span><br><span class="line">counts/fre</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0.1750, 0.1640, 0.1520, 0.1710, 0.1610, 0.1770])</span><br></pre></td></tr></tbody></table></figure>

<p>也可以看到这些概率如何随着时间的推移收敛到真实概率，进行500组实验，每组抽取10个样品</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> multinomial <span class="comment"># 概率分布模块</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> ipywidgets <span class="keyword">import</span> interact, IntSlider</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simulate_dice</span>(<span class="params">n=<span class="number">10</span>, m=<span class="number">500</span></span>):</span><br><span class="line">    fair_probs = torch.ones([<span class="number">6</span>]) / <span class="number">6</span></span><br><span class="line">    counts = multinomial.Multinomial(n, fair_probs).sample((m,))</span><br><span class="line">    cum_counts = counts.cumsum(dim=<span class="number">0</span>)</span><br><span class="line">    estimates = cum_counts/cum_counts.<span class="built_in">sum</span>(dim=<span class="number">1</span>,keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">        plt.plot(estimates[:,i].numpy(), label=<span class="string">f"P(die=<span class="subst">{i+<span class="number">1</span>}</span>)"</span>)</span><br><span class="line">    plt.axhline(y=<span class="number">0.167</span>, color=<span class="string">"black"</span>, linestyle=<span class="string">"dashed"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Groups of experiments'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Estimated probability'</span>)</span><br><span class="line">    plt.title(<span class="string">f"n=<span class="subst">{n}</span>, m=<span class="subst">{m}</span>"</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="comment"># 定义交互</span></span><br><span class="line">interact(</span><br><span class="line">    simulate_dice,</span><br><span class="line">    n = IntSlider(value = <span class="number">10</span>, <span class="built_in">min</span>=<span class="number">1</span>, <span class="built_in">max</span>=<span class="number">30</span>, step=<span class="number">1</span>, description=<span class="string">"n(per group num)"</span>),</span><br><span class="line">    m = IntSlider(value = <span class="number">500</span>, <span class="built_in">min</span>=<span class="number">100</span>, <span class="built_in">max</span>=<span class="number">1000</span>, step=<span class="number">1</span>, description=<span class="string">"m(group num)"</span>)</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2025-10-14_10-24-30.webp" alt="Snipaste_2025-10-14_10-24-30" style="zoom:50%;">

<p>小n每组信息很少，每次抽样波动巨大；大n每组本身就是一个小样本集，波动明显减弱，样本方差减小，单次实验更接近期望</p>
<p>增大m结果会更接近理论值</p>
<h4 id="概率论公理"><a href="#概率论公理" class="headerlink" title="概率论公理"></a>概率论公理</h4><p>在给定的样本空间$\mathcal{S}$中，事件$\mathcal{A}$的概率，表示为$P(\mathcal{A})$，满足以下属性：</p>
<ul>
<li>对于任意事件$\mathcal{A}$，其概率从不会是负数，即$P(\mathcal{A}) \geq 0$</li>
<li>整个样本空间的概率为1，即$P(\mathcal{S}) = 1$</li>
<li>对于**互斥(mutually exclusive)**事件，任意一个可数序列$\mathcal{A}_1, \mathcal{A}<em>2, \ldots$，序列中任意一个事件发生的概率等于它们各自发生的概率之和，即$P(\bigcup</em>{i=1}^{\infty} \mathcal{A}<em>i) = \sum</em>{i=1}^{\infty} P(\mathcal{A}_i)$</li>
</ul>
<h4 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h4><p>随机变量几乎可以是任何数量，并且它可以在随机实验的一组可能性中取一个值</p>
<p>考虑一个随机变量$X$，其值在掷骰子的样本空间$\mathcal{S}={1,2,3,4,5,6}$中，可以将事件“看到一个5”表示为${X=5}$或$X=5$，其概率表示为$P({X=5})$或$P(X=5)$</p>
<p>**离散(discrete)<strong>随机变量(如骰子的每一面)和</strong>连续(continuous)**随机变量(如人的体重和身高)之间存在微妙的区别</p>
<p>将这个看到某个数值的可能性量化为**密度(density)**更为合适</p>
<h3 id="处理多个随机变量"><a href="#处理多个随机变量" class="headerlink" title="处理多个随机变量"></a>处理多个随机变量</h3><p>当处理多个随机变量时，会有若干个变量是我们感兴趣的</p>
<h4 id="联合概率"><a href="#联合概率" class="headerlink" title="联合概率"></a>联合概率</h4><p>对于$P(A=a,B=b)$回答：$A=a$和$B=b$同时满足的概率是多少？</p>
<p>对于任何$a$和$b$的取值，必然满足$P(A = a, B=b) \leq P(A=a)$，同时发生的概率一定不大于单独事件的发生概率</p>
<h4 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h4><p>联合概率的不等式带出<br>$$<br>0 \leq \frac{P(A=a, B=b)}{P(A=a)} \leq 1<br>$$<br>称这个比率为<strong>条件概率(conditional probability)</strong>，并用$P(B=b \mid A=a)$表示它</p>
<h4 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h4><p>使用条件概率的定义可以得出统计学中最有用的方程之一：Bayes定理(Bayes’ theorem)</p>
<p>根据乘法法则(multiplication rule)可得到<font color="DarkViolet">$P(A, B) = P(B \mid A) P(A)$</font>，根据对称性，可得到$P(A, B) = P(A \mid B) P(B)$</p>
<p>假设$P(B)&gt;0$，求解条件概率<br>$$<br>P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}<br>$$</p>
<h4 id="边际化"><a href="#边际化" class="headerlink" title="边际化"></a>边际化</h4><p>为了能进行事件概率求和，需要求和法则(sum rule)，即$B$的概率相当于计算的所有可能选择，并将所有选择的联合概率聚合在一起：<br>$$<br>P(B) = \sum_{A} P(A, B)<br>$$<br>这也称为<strong>边际化(marginalization)</strong>，边际化结果的概率或分布称为边际概率或边际分布</p>
<h4 id="独立性"><a href="#独立性" class="headerlink" title="独立性"></a>独立性</h4><p>如果两个随机变量$A$和$B$是独立的，意味着事件$A$的发生跟事件$B$的发生无关</p>
<p>在这种情况下，统计学家通常将这一点表述为$A \perp B$</p>
<p>根据贝叶斯定理，马上就能同样得到$P(A \mid B) = P(A)$，在所有其他情况下，称$A$和$B$依赖</p>
<p>根据条件分布$P(A \mid B) = \frac{P(A, B)}{P(B)} = P(A)$，等价于$P(A, B) = P(A)P(B)$，因此两个随机变量是独立的，当且仅当两个随机变量的联合分布是其各自分布的乘积</p>
<p>给定另一个随机变量$C$时，两个随机变量$A$和$B$是<strong>条件独立的(conditionally independent)</strong>，当且仅当$P(A, B \mid C) = P(A \mid C)P(B \mid C)$，这个情况表示为$A \perp B \mid C$</p>
<h3 id="概念应用"><a href="#概念应用" class="headerlink" title="概念应用"></a>概念应用</h3><p>假设一个医生对患者进行艾滋病病毒(HIV)测试，如果患者健康但测试显示他患病，这个概率只有1%；如果患者真正感染HIV，它永远不会检测不出</p>
<p>使用$D_1$来表示诊断结果(如果阳性为1，如果阴性为0)，$H$来表示感染艾滋病病毒的状态(如果阳性为1，如果阴性为0)，条件概率表中表示：</p>
<table>
<thead>
<tr>
<th>条件概率</th>
<th>$H=1$</th>
<th>$H=0$</th>
</tr>
</thead>
<tbody><tr>
<td>$P(D_1 = 1 \mid H)$</td>
<td>1</td>
<td>0.01</td>
</tr>
<tr>
<td>$P(D_1 = 0 \mid H)$</td>
<td>0</td>
<td>0.99</td>
</tr>
</tbody></table>
<p>每列的加和都是1(但每行的加和不是)，因为条件概率需要总和为1，就像概率一样</p>
<p>现在要计算如果测试出来呈阳性，患者感染HIV的概率，即$P(H = 1 \mid D_1 = 1)$</p>
<p>根据贝叶斯定理<br>$$<br>P(H = 1 \mid D_1 = 1)= \frac{P(D_1=1 \mid H=1) P(H=1)}{P(D_1=1)}<br>$$<br>假设人口总体是相当健康的$P(H=1) = 0.0015$，可以求出$P(D_1 = 1) $<br>$$<br>\begin{split}\begin{aligned}<br>P(D_1 = 1) =&amp; P(D_1=1, H=0) + P(D_1=1, H=1)  \\<br>=&amp; P(D_1=1 \mid H=0) P(H=0) + P(D_1=1 \mid H=1) P(H=1) \\<br>=&amp; 0.011485.<br>\end{aligned}\end{split}<br>$$<br>因此得到<br>$$<br>\begin{aligned}<br>P(H = 1 \mid D_1 = 1)= \frac{P(D_1=1 \mid H=1) P(H=1)}{P(D_1=1)} = 0.1306 \end{aligned}<br>$$<br>尽管使用了非常准确的测试，患者实际上患有艾滋病的几率只有13.06%，概率可能是违反直觉的</p>
<p>患者会要求医生进行另一次测试来确定病情，第二个测试具有不同的特性，它不如第一个测试那么精确</p>
<table>
<thead>
<tr>
<th>条件概率</th>
<th>H=1</th>
<th>H=0</th>
</tr>
</thead>
<tbody><tr>
<td>$P(D_2 = 1 \mid H)$</td>
<td>0.98</td>
<td>0.03</td>
</tr>
<tr>
<td>$P(D_2 = 0 \mid H)$</td>
<td>0.02</td>
<td>0.97</td>
</tr>
</tbody></table>
<p>不幸的是，第二次测试也显示阳性，通过假设条件独立性来计算出应用Bayes定理的必要概率</p>
<p>计算目标为<br>$$<br>\begin{split}\begin{aligned}<br>P(H = 1 \mid D_1 = 1, D_2 = 1)<br>= \frac{P(D_1 = 1, D_2 = 1 \mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)}<br>\end{aligned}\end{split}<br>$$<br>两次检测都为阳性实际上没感染的概率<br>$$<br>\begin{split}\begin{aligned}<br>P(D_1 = 1, D_2 = 1 \mid H = 0) = P(D_1 = 1 \mid H = 0) P(D_2 = 1 \mid H = 0)<br>= 0.0003<br>\end{aligned}\end{split}<br>$$<br>两次检测都为阳性实际上感染的概率<br>$$<br>\begin{split}\begin{aligned}<br>P(D_1 = 1, D_2 = 1 \mid H = 1) = P(D_1 = 1 \mid H = 1) P(D_2 = 1 \mid H = 1)<br>= 0.98<br>\end{aligned}\end{split}<br>$$<br>现在可以应用边际化和乘法规则<br>$$<br>\begin{split}\begin{aligned}<br>P(D_1 = 1, D_2 = 1)<br>=&amp; P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1)  \\<br>=&amp; P(D_1 = 1, D_2 = 1 \mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 \mid H = 1)P(H=1)\\<br>=&amp; 0.00176955<br>\end{aligned}\end{split}<br>$$<br>鉴于存在两次阳性检测，患者患有艾滋病的概率为<br>$$<br>\begin{split}\begin{aligned}<br>P(H = 1 \mid D_1 = 1, D_2 = 1)<br>= \frac{P(D_1 = 1, D_2 = 1 \mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)}<br>= 0.8307<br>\end{aligned}\end{split}<br>$$<br>第二次测试能够对患病的情况获得更高的信心，尽管第二次检验比第一次检验的准确性要低得多</p>
<h3 id="期望和方差"><a href="#期望和方差" class="headerlink" title="期望和方差"></a>期望和方差</h3><p>一个随机变量$X$的**期望(expectation)**表示为<br>$$<br>E[X] = \sum_{x} x P(X = x)<br>$$<br>当函数$f(x)$的输入是从分布$P$中抽取的随机变量时，$f(x)$的期望值为<br>$$<br>E_{x \sim P}[f(x)] = \sum_x f(x) P(x)<br>$$<br>在许多情况下，希望衡量随机变量$X$与其期望值的偏置，这可以通过方差来量化<br>$$<br>\mathrm{Var}[X] = E\left[(X - E[X])^2\right] =E[X^2] - E[X]^2<br>$$<br>随机变量函数的方差衡量当从该随机变量分布中采样不同值时，函数值偏离该函数的期望的程度<br>$$<br>\mathrm{Var}[f(x)] = E\left[\left(f(x) - E[f(x)]\right)^2\right]<br>$$</p>
<h3 id="练习题-2"><a href="#练习题-2" class="headerlink" title="练习题"></a>练习题</h3><ol>
<li><p>给定两个概率为$ P(\mathcal{A}) $和$P(\mathcal{B})$的事件，计算$P(\mathcal{A} \cup \mathcal{B})$和$P(\mathcal{A} \cap \mathcal{B})$的上限和下限(韦恩图)</p>
<p>假设$ P(\mathcal{A}) =p, P(\mathcal{B})=q $</p>
<p>$P(\mathcal{A} \cup \mathcal{B})=p+q-P(\mathcal{A} \cap \mathcal{B})$：</p>
<p>两个事件越独立越大(交集小)，下界在交集大(一个尽量包含另一个)时取得<br>$$<br>\mathrm{max}(p,q)\le P(\mathcal{A} \cup \mathcal{B})\le \mathrm{min}(1,p+q)<br>$$<br>$P(\mathcal{A} \cap \mathcal{B})=p+q-P(\mathcal{A} \cup \mathcal{B})$：</p>
<p>上界不能超过小的那个事件，下界由全集限制<br>$$<br>\mathrm{max}(0,p+q-1) \le P(\mathcal{A} \cap \mathcal{B}) \le \mathrm{min}(p,q)<br>$$</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/image-20251014105211207.png" alt="image-20251014105211207" style="zoom: 67%;">
</li>
<li><p>假设有一系列随机变量$A,B,C$，其中$B$只依赖于$A$，而$C$只依赖于$B$，能简化联合概率  $P(A,B,C)$吗?</p>
<p>马尔可夫链的关键性质是“无记忆性”，对于$A,B,C$它们形成了马尔可夫链<br>$$<br>A\rightarrow B \rightarrow C<br>$$<br>那么可以简化为<br>$$<br>P(A,B,C)=P(A)P(B\mid A)P(C\mid B)<br>$$<br>而不是一般情况下的<br>$$<br>P(A,B,C)=P(A)P(B\mid A)P(C\mid A,B)<br>$$<br>因为$C$不直接依赖$A$</p>
<p>未来的演化不依赖更早的历史，只依赖当下的状态</p>
</li>
</ol>
<h2 id="文档查询"><a href="#文档查询" class="headerlink" title="文档查询"></a>文档查询</h2><p><strong>查找模块中的所有函数和类</strong>：利用<code>dir</code>函数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">dir</span>(torch.distributions))</span><br></pre></td></tr></tbody></table></figure>

<p>通常可以忽略以<code>__</code>开始和结束的函数，它们是Python中的特殊对象，或以<code>_</code>开始的函数，它们通常是内部函数</p>
<p><strong>查找特定函数和类的用法</strong>：调用<code>help</code>函数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">help</span>(torch.ones)</span><br></pre></td></tr></tbody></table></figure>



</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://yhblogs.cn">今天睡够了吗</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://yhblogs.cn/posts/48513.html">http://yhblogs.cn/posts/48513.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yhblogs.cn" target="_blank">がんばろう</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E2%8C%A8%EF%B8%8Fpython/">⌨️python</a></div><div class="post_share"><div class="social-share" data-image="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-og3zqp_1280x720.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/31940.html" title="线性神经网络"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-1qpqrw_1280x720.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">线性神经网络</div></div></a></div><div class="next-post pull-right"><a href="/posts/22642.html" title="windows安装ROCm"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/ROCm_logo.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">windows安装ROCm</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/30698.html" title="BERT_Pytorch"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7jjyd9_2560x1440.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-09</div><div class="title">BERT_Pytorch</div></div></a></div><div><a href="/posts/31208.html" title="FunRec 推荐系统_精排模型"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7j931e_1280x720_(1) (1).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-18</div><div class="title">FunRec 推荐系统_精排模型</div></div></a></div><div><a href="/posts/24333.html" title="FunRec推荐系统_召回模型"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-vpp725_1280x720_(1).webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-14</div><div class="title">FunRec推荐系统_召回模型</div></div></a></div><div><a href="/posts/58676.html" title="Leetcode100记录"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-9ozdyx_1280x720.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-26</div><div class="title">Leetcode100记录</div></div></a></div><div><a href="/posts/22642.html" title="windows安装ROCm"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/ROCm_logo.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-10</div><div class="title">windows安装ROCm</div></div></a></div><div><a href="/posts/3865533702.html" title="pyqt5简单实践"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071521231.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-28</div><div class="title">pyqt5简单实践</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/b_2a1aef95f351a5f7ef72eb81e6838fd6.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">今天睡够了吗</div><div class="author-info__description">相遇是最小单位的奇迹</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071549233.webp" target="_blank" title="QQ"><i class="iconfont icon-QQ"></i></a><a class="social-icon" href="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071549234.webp" target="_blank" title="微信"><i class="iconfont icon-weixin"></i></a><a class="social-icon" href="https://space.bilibili.com/277953459?spm_id_from=333.1007.0.0" target="_blank" title="bilibili"><i class="iconfont icon-bilibili"></i></a><a class="social-icon" href="https://github.com/YaoHui-Wu06022" target="_blank" title="Github"><i class="iconfont icon-GitHub"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">保持理智，相信明天</div><div class="twopeople"><div class="twopeople"><div class="container" style="height:200px;"><canvas class="illo" width="800" height="800" style="max-width: 200px; max-height: 200px; touch-action: none; width: 640px; height: 640px;"></canvas></div> <script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople1.js"></script> <script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/zdog.dist.js"></script> <script id="rendered-js" src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople.js"></script> <style>.twopeople{margin:0;align-items:center;justify-content:center;text-align:center}canvas{display:block;margin:0 auto;cursor:move}</style></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7"><span class="toc-number">1.</span> <span class="toc-text">符号</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-number">2.</span> <span class="toc-text">数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-number">2.1.</span> <span class="toc-text">广播机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8A%82%E7%9C%81%E5%86%85%E5%AD%98"><span class="toc-number">2.2.</span> <span class="toc-text">节省内存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AC%E5%8C%96%E4%B8%BA%E5%85%B6%E5%AE%83%E5%AF%B9%E8%B1%A1"><span class="toc-number">2.3.</span> <span class="toc-text">转化为其它对象</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.1.</span> <span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="toc-number">3.2.</span> <span class="toc-text">处理缺失值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%BC%A0%E9%87%8F%E6%A0%BC%E5%BC%8F"><span class="toc-number">3.3.</span> <span class="toc-text">转换为张量格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A0%E9%99%A4%E7%BC%BA%E5%A4%B1%E5%80%BC%E6%9C%80%E5%A4%9A%E7%9A%84%E5%88%97"><span class="toc-number">3.4.</span> <span class="toc-text">删除缺失值最多的列</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-number">4.</span> <span class="toc-text">线性代数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E9%87%8F"><span class="toc-number">4.1.</span> <span class="toc-text">标量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F"><span class="toc-number">4.2.</span> <span class="toc-text">向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5"><span class="toc-number">4.3.</span> <span class="toc-text">矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F"><span class="toc-number">4.4.</span> <span class="toc-text">张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadamard%E7%A7%AF"><span class="toc-number">4.5.</span> <span class="toc-text">Hadamard积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4"><span class="toc-number">4.6.</span> <span class="toc-text">降维</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9D%9E%E9%99%8D%E7%BB%B4%E6%B1%82%E5%92%8C"><span class="toc-number">4.6.1.</span> <span class="toc-text">非降维求和</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%82%B9%E7%A7%AF"><span class="toc-number">4.7.</span> <span class="toc-text">点积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5-%E5%90%91%E9%87%8F%E7%A7%AF"><span class="toc-number">4.8.</span> <span class="toc-text">矩阵-向量积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5-%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-number">4.9.</span> <span class="toc-text">矩阵-矩阵乘法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8C%83%E6%95%B0"><span class="toc-number">4.9.1.</span> <span class="toc-text">范数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8C%83%E6%95%B0%E5%92%8C%E7%9B%AE%E6%A0%87"><span class="toc-number">4.9.2.</span> <span class="toc-text">范数和目标</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E7%A7%AF%E5%88%86"><span class="toc-number">5.</span> <span class="toc-text">微积分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E6%95%B0%E5%92%8C%E5%BE%AE%E5%88%86"><span class="toc-number">5.1.</span> <span class="toc-text">导数和微分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E5%AF%BC%E6%95%B0"><span class="toc-number">5.2.</span> <span class="toc-text">偏导数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6"><span class="toc-number">5.3.</span> <span class="toc-text">梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-number">5.4.</span> <span class="toc-text">链式法则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0%E9%A2%98"><span class="toc-number">5.5.</span> <span class="toc-text">练习题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">6.</span> <span class="toc-text">自动微分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E4%BE%8B%E5%AD%90"><span class="toc-number">6.1.</span> <span class="toc-text">简单例子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E6%A0%87%E9%87%8F%E5%8F%98%E9%87%8F%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">6.2.</span> <span class="toc-text">非标量变量的反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-number">6.3.</span> <span class="toc-text">分离计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A7%E5%88%B6%E6%B5%81%E7%9A%84%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">6.4.</span> <span class="toc-text">控制流的梯度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0%E9%A2%98-1"><span class="toc-number">6.5.</span> <span class="toc-text">练习题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E7%8E%87"><span class="toc-number">7.</span> <span class="toc-text">概率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E7%8E%87%E8%AE%BA"><span class="toc-number">7.1.</span> <span class="toc-text">基本概率论</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA%E5%85%AC%E7%90%86"><span class="toc-number">7.1.1.</span> <span class="toc-text">概率论公理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="toc-number">7.1.2.</span> <span class="toc-text">随机变量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E5%A4%9A%E4%B8%AA%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="toc-number">7.2.</span> <span class="toc-text">处理多个随机变量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87"><span class="toc-number">7.2.1.</span> <span class="toc-text">联合概率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87"><span class="toc-number">7.2.2.</span> <span class="toc-text">条件概率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86"><span class="toc-number">7.2.3.</span> <span class="toc-text">贝叶斯定理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%B9%E9%99%85%E5%8C%96"><span class="toc-number">7.2.4.</span> <span class="toc-text">边际化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="toc-number">7.2.5.</span> <span class="toc-text">独立性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5%E5%BA%94%E7%94%A8"><span class="toc-number">7.3.</span> <span class="toc-text">概念应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%9F%E6%9C%9B%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="toc-number">7.4.</span> <span class="toc-text">期望和方差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0%E9%A2%98-2"><span class="toc-number">7.5.</span> <span class="toc-text">练习题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E6%A1%A3%E6%9F%A5%E8%AF%A2"><span class="toc-number">8.</span> <span class="toc-text">文档查询</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">©2022 - 2026 By 今天睡够了吗</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">You must always have faith in who you are！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="false"></script><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>