<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>FunRec推荐系统_召回模型 | がんばろう</title><meta name="author" content="今天睡够了吗"><meta name="copyright" content="今天睡够了吗"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="datawhalechina/fun-rec：推荐系统入门 推荐系统的本质，是对关系的量化预测 预测过程需要系统深入理解三个关键要素  理解用户：历史记录，喜欢与不喜欢，搜索关键词 理解物品：对视频而已，内容属性：时长，制作质量；统计属性：多少人看过，平均评分 理解场景：比如一个在地铁上通勤的用户更可能对短视频感兴趣，而在家中休闲的用户则可能愿意观看较长的深度内容  将推荐系统的核心抽象">
<meta property="og:type" content="article">
<meta property="og:title" content="FunRec推荐系统_召回模型">
<meta property="og:url" content="http://yhblogs.cn/posts/24333.html">
<meta property="og:site_name" content="がんばろう">
<meta property="og:description" content="datawhalechina/fun-rec：推荐系统入门 推荐系统的本质，是对关系的量化预测 预测过程需要系统深入理解三个关键要素  理解用户：历史记录，喜欢与不喜欢，搜索关键词 理解物品：对视频而已，内容属性：时长，制作质量；统计属性：多少人看过，平均评分 理解场景：比如一个在地铁上通勤的用户更可能对短视频感兴趣，而在家中休闲的用户则可能愿意观看较长的深度内容  将推荐系统的核心抽象">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-vpp725_1280x720_(1).webp">
<meta property="article:published_time" content="2026-01-14T17:27:57.000Z">
<meta property="article:modified_time" content="2026-01-31T12:00:30.719Z">
<meta property="article:author" content="今天睡够了吗">
<meta property="article:tag" content="⌨️python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-vpp725_1280x720_(1).webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yhblogs.cn/posts/24333.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'FunRec推荐系统_召回模型',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-01-31 12:00:30'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_3319458_ks437t3n4r.css"><link rel="stylesheet" href="/css/modify.css"><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/b_2a1aef95f351a5f7ef72eb81e6838fd6.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouye"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw iconfont icon-rili"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw iconfont icon-biaoqian"></i><span> 标签</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="がんばろう"><img class="site-icon" src="/img/favicon.png"><span class="site-name">がんばろう</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouye"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw iconfont icon-rili"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw iconfont icon-biaoqian"></i><span> 标签</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">FunRec推荐系统_召回模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2026-01-14T17:27:57.000Z" title="发表于 2026-01-14 17:27:57">2026-01-14</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">29.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>106分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="FunRec推荐系统_召回模型"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p><a target="_blank" rel="noopener" href="https://github.com/datawhalechina/fun-rec">datawhalechina/fun-rec</a>：推荐系统入门</p>
<p>推荐系统的本质，是对关系的量化预测</p>
<p>预测过程需要系统深入理解三个关键要素</p>
<ul>
<li><strong>理解用户</strong>：历史记录，喜欢与不喜欢，搜索关键词</li>
<li><strong>理解物品</strong>：对视频而已，内容属性：时长，制作质量；统计属性：多少人看过，平均评分</li>
<li><strong>理解场景</strong>：比如一个在地铁上通勤的用户更可能对短视频感兴趣，而在家中休闲的用户则可能愿意观看较长的深度内容</li>
</ul>
<p>将推荐系统的核心抽象为一个数学函数：接收对用户、物品和场景的理解作为输入，输出一个代表连接可能性的分数</p>
<p>在理想情况下，可以为每个用户计算与所有物品的匹配分数，然后简单地选择分数最高的几个进行推荐，但在真实的工业环境中，这种做法完全不可行</p>
<p><strong>推荐系统工程化面临的核心矛盾：如何在极有限的时间内，从海量的候选中找到最优的推荐结果？</strong></p>
<p>工业界的解决方案是采用分阶段的漏斗式架构，通过“召回-排序-重排”的三层流水线来逐步缩小候选范围，在效率和效果之间找到平衡点</p>
<ul>
<li><strong>召回</strong>：不追求高精度而强调覆盖面，利用少量特征，通过协同过滤快速找到品味相似的用户并推荐其偏好内容；或基于内容相似度，召回与近期观看视频相似的内容</li>
<li><strong>排序</strong>：在召回后候选集极大缩小，这时使用预测函数，融合用户、物品、场景的所有可用特征，为每个候选物品计算精确的预测分数</li>
<li><strong>重排</strong>：预测分数最高的列表，不一定等于用户体验最佳的列表，考虑多样性、新颖性、公平性等因素，也有可能插入广告等</li>
</ul>
<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/recommendation_pipeline.webp" alt="recommendation_pipeline"></p>
<p><strong>召回系统</strong>：</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/chapter_02_outline.webp" alt="chapter_02_outline" style="zoom:80%;">

<p><strong>排序系统</strong>：</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/chapter_03_outline.webp" alt="chapter_03_outline" style="zoom: 80%;">

<p>指标：</p>
<ul>
<li><p><code>hit_rate@K</code>：对每个用户，真实物品出现在 Top-K 的比例</p>
<p>HitRate@K = 被命中的用户数/总用户数</p>
</li>
<li><p><code>precision@K</code>：对每个用户，Top‑K 推荐中属于真实正样本的比例</p>
<p>Precision@K = Top-K 中相关物品数/K</p>
</li>
<li><p><code>ndcg@K</code>：排得越靠前，得分越高</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>指标</th>
<th>关心点</th>
<th>是否看排序</th>
<th>适合评估什么</th>
</tr>
</thead>
<tbody><tr>
<td>HitRate@K</td>
<td>有没有命中</td>
<td>❌</td>
<td>召回覆盖能力</td>
</tr>
<tr>
<td>Precision@K</td>
<td>命中比例</td>
<td>❌</td>
<td>推荐列表纯度</td>
</tr>
<tr>
<td>NDCG@K</td>
<td>排名质量</td>
<td>✅</td>
<td>排序是否合理</td>
</tr>
</tbody></table>
<hr>
<p>召回模型的三个类别：</p>
<p><strong>协同过滤</strong>：基于用户或物品相似性进行建模，包括 ItemCF(含Swing)、UserCF、矩阵分解(向量化表示，含FunkSVD/BiasSVD)</p>
<p><strong>向量召回</strong>：把用户和物品映射到向量空间，用向量检索完成大规模召回，包括双塔(DSSM)、YouTube DNN、EGES</p>
<p><strong>序列召回</strong>：把用户看成随时间演化的状态，用最近行为预测下一步</p>
<h2 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h2><p>核心思想基于一个朴素的假设：相似的用户会喜欢相似的物品</p>
<p>首先收集用户的历史行为数据(如评分、点击、购买记录)，然后计算用户之间或物品之间的相似度，最后基于这些相似度为用户生成推荐</p>
<p>根据计算相似度的对象不同，协同过滤可以分为两种基本类型<font color="Violetred">(邻域法)</font>：</p>
<ul>
<li><p><strong>基于物品的协同过滤(Item-based CF)</strong>：推荐与用户历史偏好物品相似的其他物品</p>
<p>ItemCF在工业界应用广泛，因物品规模更小、属性更稳定，维护成本较低</p>
<p>Swing 算法利用用户-物品二部图结构提升相似度计算的鲁棒性，是 ItemCF 的改进方法</p>
</li>
<li><p><strong>基于用户的协同过滤(User-based CF)</strong>：寻找兴趣相似的用户并推荐其喜欢的物品</p>
<p>UserCF能够挖掘潜在兴趣、提升新颖性，但受限于用户规模大、兴趣变化快和行为稀疏等问题，在工业场景中应用受限</p>
</li>
</ul>
<p><strong>矩阵分解(Matrix Factorization, MF)</strong>：现代CF，直接建模用户-物品交互矩阵，通过学习用户和物品的低维向量表示来预测偏好，实现了U2I的直接召回，属于<font color="Violetred">模型法协同过滤</font>，为后续向量召回奠定基础</p>
<h3 id="ItemCF"><a href="#ItemCF" class="headerlink" title="ItemCF"></a>ItemCF</h3><p>ItemCF的思路建立在一个简单的假设上：用户的兴趣具有一定的连贯性，喜欢某个物品的用户往往也会对相似的物品感兴趣</p>
<p>实现流程主要包含两个步骤</p>
<p><font color="Violetred">步骤一：物品相似度计算</font></p>
<p>在大多数实际应用场景中通常只有用户是否对物品有过交互行为的数据(如点击、购买、收藏等)，而没有具体的评分信息</p>
<p>理论上可以将每个物品表示为一个向量，然后计算向量间的相似度，但当商品数量巨大时，计算所有物品对之间的相似度会变成一个巨大的工程，时间复杂度达到$O(|I|^2)$</p>
<p>因此从用户出发找物品组合，采用更高效的实现方式：</p>
<ol>
<li><p><strong>构建用户-物品倒排表</strong>：为每个用户维护一个交互过的物品列表</p>
</li>
<li><p><strong>计算物品共现矩阵</strong>：创建一个矩阵$C[i][j]$来记录物品$i$和$j$的共同用户数量</p>
</li>
<li><p><strong>计算最终相似度</strong>：使用余弦相似度公式计算物品相似度<br>$$<br>w_{ij} = \frac{C[i][j]}{\sqrt{|N(i)| \cdot |N(j)|}}<br>$$</p>
</li>
</ol>
<p>$|N(i)|$表示与物品$i$有交互的用户总数，分母是对共同用户数的标准化，防止热门商品占据绝对优势</p>
<p>$C[i][j]$是两个物品的共现次数</p>
<p><strong>算法复杂度分析</strong></p>
<p>相比暴力计算有显著的效率提升：</p>
<ul>
<li><strong>暴力方法</strong>：遍历$|U|$个用户，$O(|I|^2 \cdot |U|)$</li>
<li><strong>优化方法</strong>：每个用户交互$|N(u)|$个物品，只计算有共同用户的物品对，$O(\sum_{u} |N(u)|^2)$</li>
</ul>
<p>定义$\sum_{u} |N(u)| = R$(总交互数)，且用户平均交互物品数$\bar{m} = R/|U|$，那么$\sum_{u} |N(u)|^2\approx \sum_{u} \bar m^2 = |U|\cdot \bar m^2$</p>
<p><font color="DarkViolet">总复杂度可以表示为$O(R \cdot \bar{m})$</font></p>
<p>现实推荐数据的典型特征：</p>
<ul>
<li>$\bar m\ll|I|$(用户只看极少数物品)</li>
<li>交互矩阵极度稀疏</li>
</ul>
<p>优化方法效率远高于暴力计算，只计算真正有意义的物品对，避免了大量无效计算</p>
<p><font color="Violetred">步骤二：候选物品推荐</font></p>
<p>获得物品相似度矩阵后就可以预测用户对未接触物品的喜好程度了</p>
<p>选取用户最近交互的物品作为兴趣种子，为每个种子物品找到最相似的若干个候选物品，快速生成大量候选集合</p>
<p>用用户$u$过去看过的所有物品$j\in N(u)$，按它们和目标物品 $i$ 的相似度加权，得到用户对物品 $i$ 的兴趣评分<br>$$<br>\color{red} p(u, i) = \sum_{j \in N(u)} w_{ij} \cdot r_{uj}<br>$$<br>$w_{ij}$是物品之间的相似度，<font color="DarkViolet">$r_{uj}$表示用户对物品$j$的兴趣强度(可以是简单的1，也可以根据交互时间、类型等设置不同权重)</font></p>
<p>最终系统对所有候选物品按兴趣分数排序，选择Top-N物品作为ItemCF通道的推荐结果</p>
<p><strong>拓展：处理评分数据的相似度计算</strong></p>
<p>余弦相似度适用于隐式反馈场景(如点击、浏览)，但在某些应用中还有显式评分数据(如5星评分、点赞数等)，对于这类数据，可以使用更精细的相似度计算方法</p>
<p>当有评分数据时，<font color="DarkViolet">皮尔逊相关系数</font>能更好地捕获物品间的相似性模式：<br>$$<br>w_{ij} = \frac{\sum_{u \in U_{ij}}(r_{ui} - \bar r_i)(r_{uj} - \bar r_j)}{\sqrt{\sum_{u \in U_{ij}}(r_{ui} - \bar r_i)^2}\sqrt{\sum_{u \in U_{ij}}(r_{uj} - \bar r_j)^2}}<br>$$</p>
<ul>
<li>$U_{ij}$：同时给物品 $i$ 和 $j$ 打过分的用户</li>
<li>$r_{ui}$：用户 $u$ 对物品 $i$ 的评分</li>
<li>$\bar r_i$：物品 $i$ 的平均评分</li>
</ul>
<p>核心优势：皮尔逊相关系数通过中心化处理，能够</p>
<ul>
<li>消除不同物品评分分布的差异(有些物品普遍评分高，有些偏低)</li>
<li>关注用户评分的相对趋势而非绝对数值</li>
<li>更好地识别“用户对两个物品评分模式一致”的相似性</li>
</ul>
<p>基于皮尔逊相似度，可以预测用户对未接触物品的评分：<br>$$<br>\hat r_{u,j} = \bar r_j + \frac{\sum_{k \in S_j} w_{jk},\left( r_{u,k} - \bar r_{k} \right)}{\sum_{k \in S_j} w_{jk}}<br>$$<br>虽然皮尔逊在理论上更“优雅”，但在大规模系统中有问题：</p>
<ol>
<li>计算复杂，需要均值、平方、归一化，且对每个物品对都要算</li>
<li>对稀疏数据不稳定，$U_{ij}$很小，相关系数噪声大</li>
</ol>
<p>工业界更常见的是简化的余弦相似度，并通过其他方式(如加权、归一化)来处理评分差异</p>
<p><font color="Violetred">举例</font></p>
<p>预测用户1对物品5的评分</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">用户1</th>
<th align="center">用户2</th>
<th align="center">用户3</th>
<th align="center">用户4</th>
<th align="center">用户5</th>
</tr>
</thead>
<tbody><tr>
<td align="center">物品1</td>
<td align="center">5</td>
<td align="center">3</td>
<td align="center">4</td>
<td align="center">3</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">物品2</td>
<td align="center">3</td>
<td align="center">1</td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">物品3</td>
<td align="center">4</td>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">1</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">物品4</td>
<td align="center">4</td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">5</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">物品5</td>
<td align="center">?</td>
<td align="center">3</td>
<td align="center">5</td>
<td align="center">4</td>
<td align="center">1</td>
</tr>
</tbody></table>
<p>以计算物品5和1之间的相似度为例<br>$$<br>\bar r_{item5} = (3+5+4+1)/4=3.25 \\<br>\bar r_{item1} = (3+4+3+1)/4 = 2.75<br>$$<br>向量减去均值<br>$$<br>\text{item5}:(-0.25, 1.75, 0.75, -2.25) \quad \text{item1}: (0.25, 1.25, 0.25, -1.75)<br>$$<br>计算皮尔逊相似度<br>$$<br>\text{sim}(item5,item1)=\cos((-0.25,  1.75,  0.75, -2.25),(0.25,  1.25,  0.25, -1.75))=0.96946<br>$$</p>
<ol>
<li><p>计算物品间的相似度矩阵</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">item_data = {</span><br><span class="line">    <span class="string">'item1'</span>: {<span class="string">'user1'</span>: <span class="number">5</span>, <span class="string">'user2'</span>: <span class="number">3</span>, <span class="string">'user3'</span>: <span class="number">4</span>, <span class="string">'user4'</span>: <span class="number">3</span>, <span class="string">'user5'</span>: <span class="number">1</span>},</span><br><span class="line">    <span class="string">'item2'</span>: {<span class="string">'user1'</span>: <span class="number">3</span>, <span class="string">'user2'</span>: <span class="number">1</span>, <span class="string">'user3'</span>: <span class="number">3</span>, <span class="string">'user4'</span>: <span class="number">3</span>, <span class="string">'user5'</span>: <span class="number">5</span>},</span><br><span class="line">    <span class="string">'item3'</span>: {<span class="string">'user1'</span>: <span class="number">4</span>, <span class="string">'user2'</span>: <span class="number">2</span>, <span class="string">'user3'</span>: <span class="number">4</span>, <span class="string">'user4'</span>: <span class="number">1</span>, <span class="string">'user5'</span>: <span class="number">5</span>},</span><br><span class="line">    <span class="string">'item4'</span>: {<span class="string">'user1'</span>: <span class="number">4</span>, <span class="string">'user2'</span>: <span class="number">3</span>, <span class="string">'user3'</span>: <span class="number">3</span>, <span class="string">'user4'</span>: <span class="number">5</span>, <span class="string">'user5'</span>: <span class="number">2</span>},</span><br><span class="line">    <span class="string">'item5'</span>: {<span class="string">'user2'</span>: <span class="number">3</span>, <span class="string">'user3'</span>: <span class="number">5</span>, <span class="string">'user4'</span>: <span class="number">4</span>, <span class="string">'user5'</span>: <span class="number">1</span>},</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">similarity_matrix = pd.DataFrame(</span><br><span class="line">    np.identity(<span class="built_in">len</span>(item_data)), <span class="comment"># 生成一个 n × n 的单位矩阵</span></span><br><span class="line">    index=item_data.keys(),</span><br><span class="line">    columns=item_data.keys(),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历每条物品-用户评分数据</span></span><br><span class="line"><span class="keyword">for</span> i1, users1 <span class="keyword">in</span> item_data.items():</span><br><span class="line">    <span class="keyword">for</span> i2, users2 <span class="keyword">in</span> item_data.items():</span><br><span class="line">        <span class="keyword">if</span> i1 == i2: <span class="comment"># 跳过同一个物体</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        vec1, vec2 = [], []</span><br><span class="line">        <span class="keyword">for</span> user, rating1 <span class="keyword">in</span> users1.items():</span><br><span class="line">            rating2 = users2.get(user, -<span class="number">1</span>) <span class="comment"># 判断是否用户是否也打过分</span></span><br><span class="line">            <span class="keyword">if</span> rating2 == -<span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            vec1.append(rating1)</span><br><span class="line">            vec2.append(rating2)</span><br><span class="line">        similarity_matrix[i1][i2] = np.corrcoef(vec1, vec2)[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(similarity_matrix)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">          item1     item2     item3     item4     item5</span><br><span class="line">item1  1.000000 -0.476731 -0.123091  0.532181  0.969458</span><br><span class="line">item2 -0.476731  1.000000  0.645497 -0.310087 -0.478091</span><br><span class="line">item3 -0.123091  0.645497  1.000000 -0.720577 -0.427618</span><br><span class="line">item4  0.532181 -0.310087 -0.720577  1.000000  0.581675</span><br><span class="line">item5  0.969458 -0.478091 -0.427618  0.581675  1.000000</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>从user1交互过的物品中，找到与item5最相似的2个物品</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">target_user = <span class="string">'user1'</span></span><br><span class="line">target_item = <span class="string">'item5'</span></span><br><span class="line">num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">sim_items = similarity_matrix[target_item].sort_values(ascending=<span class="literal">False</span>)[<span class="number">1</span>:num+<span class="number">1</span>].index.tolist()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'与物品<span class="subst">{target_item}</span>最相似的<span class="subst">{num}</span>个物品为：<span class="subst">{sim_items}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">与物品item5最相似的2个物品为：['item1', 'item4']</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>预测用户1对物品5的评分</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">target_user_mean_rating = np.mean(<span class="built_in">list</span>(item_data[target_item].values()))</span><br><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">corr_values_sum = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> sim_items:</span><br><span class="line">    corr_value = similarity_matrix[target_item][item]</span><br><span class="line">    user_mean_rating = np.mean(<span class="built_in">list</span>(item_data[item].values()))</span><br><span class="line"></span><br><span class="line">    weighted_scores += corr_value * (item_data[item][target_user] - user_mean_rating)</span><br><span class="line">    corr_values_sum += corr_value</span><br><span class="line"></span><br><span class="line">target_item_pred = target_user_mean_rating + weighted_scores / corr_values_sum</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'用户<span class="subst">{target_user}</span>对物品<span class="subst">{target_item}</span>的预测评分为：<span class="subst">{target_item_pred}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户user1对物品item5的预测评分为：4.6</span><br></pre></td></tr></tbody></table></figure></li>
</ol>
<h4 id="Swing"><a href="#Swing" class="headerlink" title="Swing"></a>Swing</h4><p>ItemCF 虽然简单有效，但在工业场景中存在明显不足：热门物品因高频占据主导，随机点击等噪声行为会干扰相似度计算，且不同类型的用户行为被一视同仁，难以区分真实兴趣与偶然行为，也无法刻画物品的替代性和互补性关系</p>
<p>Swing 算法从用户–物品二部图结构出发，不再单纯依赖共现次数，而是通过挖掘更具鲁棒性的共同购买关系来过滤噪声，更准确地刻画物品之间的真实关联，实现可以分为两个主要步骤：</p>
<p><font color="Violetred">步骤一：用户-物品二部图构建</font></p>
<p>首先将用户与物品的交互数据转化为一个二部图，一边是所有用户节点，另一边是所有物品节点，如果用户对某个物品发生了点击、购买等行为，就在对应的用户节点与物品节点之间添加一条边，为后续的相似度计算提供了结构化的数据基础</p>
<p><font color="Violetred">步骤二：物品相似度计算</font></p>
<p>计算任意一对物品 $ i $ 与 $ j $ 的相似度</p>
<p>设 $ U_{i} $ 和 $ U_{j} $ 分别表示与物品 $ i $ 和 $ j $ 有过交互的用户集合，$I_{u}$表示用户$ u $交互过的所有物品集合</p>
<p>首先找到同时与物品 $ i $ 和 $ j $ 相连的用户集合 $ U_{i} \cap U_{j} $，然后对集合中的每一对用户$(u, v)$统计他们的其他共同购买行为，如果用户 $ u  $ 与 $ v $ 的其他交互行为重合较少(即 $ \left|I_{u} \cap I_{v}\right| $ 较小)，则认为他们在共同选择物品 $ i $ 和 $ j $ 上的行为更具特异性，应该为这对物品贡献更高的相似度得分</p>
<p>Swing score 的基础计算公式为：<br>$$<br>s(i, j) = \sum_{u \in U_i \cap U_j} \sum_{v \in U_i \cap U_j} \frac{1}{\alpha + |I_u \cap I_v|}<br>$$<br>其中$\alpha$是平滑系数，用来防止分母过小导致数值不稳定</p>
<p>一个具体例子：</p>
<p>用户$A,B,C$，物品$h,t,r,p$</p>
<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/swing_score.webp" alt="swing_score"></p>
<p>假设$\alpha = 1$，看用户与物品之间的连线计算数量</p>
<ul>
<li>用户对$[A, B]$的贡献分数为$\frac{1}{1 + 4} = \frac{1}{5}$</li>
<li>用户对$[B, C]$的贡献分数为$\frac{1}{1 + 2} = \frac{1}{3}$</li>
<li>用户对$[A, C]$的贡献分数为$\frac{1}{1 + 2} = \frac{1}{3}$</li>
</ul>
<p>物品$h,p$交集有$[A, B]$，$[A, C]$，$[B,C]$，所以swing score为 $s(h, p) = \frac{1}{5} + \frac{1}{3} + \frac{1}{3} = \frac{13}{15}$</p>
<p>物品$h,t(r)$的交集只有用户对$[A, B]$，所以swing score为 $s(h, t) = s(h, r) = \frac{1}{5}$</p>
<p><strong>用户权重调整</strong></p>
<p>为了降低活跃用户对计算结果的过度影响引入用户权重$w_u = \frac{1}{\sqrt{|I_u|}}$，控制单个节点(用户)在图结构中的能量输出规模</p>
<p>最终公式为<br>$$<br>\color{red} s(i, j) = \sum_{u \in U_i \cap U_j} \sum_{v \in U_i \cap U_j} w_u \cdot w_v \cdot \frac{1}{\alpha + |I_u \cap I_v|}<br>$$</p>
<p>Swing算法的核心在于Swing Score的计算，它通过分析用户-物品二部图中的swing结构来度量物品相似度</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_swing_similarity</span>(<span class="params">item_users, user_items, user_weights, item_i, item_j, alpha=<span class="number">1.0</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算两个物品之间的Swing Score相似度</span></span><br><span class="line"><span class="string">    参考funrec.models.swing.Swing._calculate_swing_similarity_optimized()的核心逻辑</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 找到同时与物品i和j交互的用户(共同用户)</span></span><br><span class="line">    common_users = item_users[item_i].intersection(item_users[item_j])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(common_users) &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.0</span>  <span class="comment"># 至少需要2个共同用户才能计算Swing score</span></span><br><span class="line"></span><br><span class="line">    swing_score = <span class="number">0.0</span></span><br><span class="line">    common_users_list = <span class="built_in">list</span>(common_users)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算所有共同用户对的贡献</span></span><br><span class="line">    <span class="keyword">for</span> u <span class="keyword">in</span> common_users_list:</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> common_users_list:</span><br><span class="line">            <span class="keyword">if</span> u == v:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 找到用户u和v的共同交互物品</span></span><br><span class="line">            common_items_uv = user_items[u].intersection(user_items[v])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用预计算的用户权重</span></span><br><span class="line">            user_weight_u = user_weights[u]  <span class="comment"># 1.0 / sqrt(|I_u|)</span></span><br><span class="line">            user_weight_v = user_weights[v]  <span class="comment"># 1.0 / sqrt(|I_v|)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Swing Score核心公式</span></span><br><span class="line">            contribution = (user_weight_u * user_weight_v) / (alpha + <span class="built_in">len</span>(common_items_uv))</span><br><span class="line">            swing_score += contribution</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> swing_score</span><br></pre></td></tr></tbody></table></figure>

<h3 id="UserCF"><a href="#UserCF" class="headerlink" title="UserCF"></a>UserCF</h3><p>UserCF基于一个简单假设：具有相似历史行为的用户，未来偏好也相似</p>
<p>尽管实践中更偏向 ItemCF，但 UserCF 提出的用户向量化思想为后续的矩阵分解模型奠定了基础</p>
<p>实现过程可以分解为两个核心步骤</p>
<p><font color="Violetred">步骤一：用户相似度计算</font></p>
<p>假设用户$u$和用户$v$分别对应物品集合$N(u)$和$N(v)$(即他们各自有过行为的物品)</p>
<p><strong>杰卡德相似系数</strong>：</p>
<p>如果系统只记录用户是否对物品有过行为(比如是否点击、购买)，而没有具体的评分，这个系数是个不错的选择<br>$$<br>w_{uv} = \frac{|N(u) \cap N(v)|}{|N(u) \cup N(v)|}<br>$$<br>分子是两人共同喜欢的物品数量，分母是两人喜欢的物品总数(去重后)</p>
<p><strong>余弦相似度</strong>：</p>
<p>将每个用户看成一个向量，计算向量间的夹角<br>$$<br>w_{uv} = \frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}}<br>$$<br><font color="DarkViolet">余弦相似度已经考虑了用户活跃度的差异</font></p>
<p><strong>皮尔逊相关系数</strong></p>
<p>有评分数据时<br>$$<br>w_{uv} = \frac{\sum_{i \in I}(r_{ui} - \bar r_u)(r_{vi} - \bar r_v)}{\sqrt{\sum_{i \in I}(r_{ui} - \bar r_u)^2}\sqrt{\sum_{i \in I}(r_{vi} - \bar r_v)^2}}<br>$$</p>
<blockquote>
<p>和之前ItemCF不一样的是，这里的平均数计算的是每个用户的</p>
</blockquote>
<p>皮尔逊系数通过中心化处理，有效消除了个人评分习惯的差异</p>
<p>通常选择相似度最高的K个用户作为“邻居”</p>
<p><font color="Violetred">步骤二：候选物品推荐</font></p>
<p>有了相似用户，下一步就是利用他们的偏好来预测目标用户对未交互过的物品的兴趣</p>
<p><strong>简单加权平均</strong><br>$$<br>\hat r_{u,p} = \frac{\sum_{v \in S_u} w_{uv} , r_{v,p}}{\sum_{v \in S_u} w_{uv}}<br>$$<br>$\hat r_{u,p}$是预测的用户$u$对物品$p$的评分，$S_u$是邻居用户集合，$w_{uv}$是相似度权重，$r_{v,p}$是邻居$v$对物品$p$的实际评分</p>
<p>如果考虑消除个人评分习惯影响可以加入偏置修正<br>$$<br>\hat r_{u,p} = \bar r_{u} + \frac{\sum_{v \in S_u} w_{uv} , (r_{v,p} - \bar r_{v})}{\sum_{v \in S_u} w_{uv}}<br>$$<br><strong>优化策略</strong></p>
<p>UserCF看起来很简单，但有个类似物品对的问题：当用户数量很大时，计算所有用户对之间的相似度会非常耗时，时间复杂度达到$O(|U|^2)$，同样的思路构建物品倒排表</p>
<ol>
<li><strong>构建倒排表</strong>：为每个物品维护一个用户列表，记录哪些用户对这个物品有过行为，这样就可以通过物品快速找到相关用户</li>
<li><strong>稀疏矩阵计算</strong>：创建一个矩阵$C[u][v]$来记录用户$u$和$v$的共同物品数量</li>
<li><strong>计算最终相似度</strong>：矩阵给出了余弦相似度公式的分子，再除以分母$\sqrt{|N(u)||N(v)|}$得到了用户相似度</li>
<li><strong>线上召回</strong></li>
</ol>
<p>召回的具体过程：系统为目标用户找到最相似的K个用户作为“相似用户集合”(通常K=50-200)，收集这些相似用户交互过的所有物品作为候选集合，并计算目标用户对候选物品的兴趣分数<br>$$<br>p(u, i) = \sum_{v \in S_u \cap N(i)} w_{uv} \cdot r_{vi}<br>$$<br>这里的计算和ItemCF一样，不再多赘述</p>
<p>优化后的时间复杂度约为$O(R \cdot \bar n)$，其中$R$是总的用户-物品交互记录数，$\bar n$是每个物品的平均用户数</p>
<p><strong>举例</strong></p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">物品1</th>
<th align="center">物品2</th>
<th align="center">物品3</th>
<th align="center">物品4</th>
<th align="center">物品5</th>
</tr>
</thead>
<tbody><tr>
<td align="center">用户1</td>
<td align="center">5</td>
<td align="center">3</td>
<td align="center">4</td>
<td align="center">4</td>
<td align="center">?</td>
</tr>
<tr>
<td align="center">用户2</td>
<td align="center">3</td>
<td align="center">1</td>
<td align="center">2</td>
<td align="center">3</td>
<td align="center">3</td>
</tr>
<tr>
<td align="center">用户3</td>
<td align="center">4</td>
<td align="center">3</td>
<td align="center">4</td>
<td align="center">3</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">用户4</td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">1</td>
<td align="center">5</td>
<td align="center">4</td>
</tr>
<tr>
<td align="center">用户5</td>
<td align="center">1</td>
<td align="center">5</td>
<td align="center">5</td>
<td align="center">2</td>
<td align="center">1</td>
</tr>
</tbody></table>
<p>基于皮尔逊相关系数，计算用户1与用户2的相似度<br>$$<br>\bar r_{user1}=4,\ \bar r_{user2}=2.25 \rightarrow  \text{user1}:(1,-1, 0,0) \quad \text{user2}: (0.75,-1.25,-0.25,0.75)<br>$$<br>计算这俩新向量的余弦相似度，得到皮尔逊相似度$0.852$</p>
<p>根据相似用户，用户2对物品5的评分是3，用户3对物品5的打分是5，可以计算出 用户1 对物品5的最终得分是<br>$$<br>P_{user1, item5}=\bar r_{user1}+\frac{\sum_{k=1}^{2}\left(w_{user1,user k}\left(r_{userk, item5}-\bar r_{userk}\right)\right)}{\sum_{k=1}^{2} w_{user1, userk}}=4+\frac{0.85*(3-2.4)+0.7*(5-3.8)}{0.85+0.7}=4.87<br>$$</p>
<ol>
<li><p>数据准备</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">user_data = {</span><br><span class="line">    <span class="string">'user1'</span>: {<span class="string">'item1'</span>: <span class="number">5</span>, <span class="string">'item2'</span>: <span class="number">3</span>, <span class="string">'item3'</span>: <span class="number">4</span>, <span class="string">'item4'</span>: <span class="number">4</span>},</span><br><span class="line">    <span class="string">'user2'</span>: {<span class="string">'item1'</span>: <span class="number">3</span>, <span class="string">'item2'</span>: <span class="number">1</span>, <span class="string">'item3'</span>: <span class="number">2</span>, <span class="string">'item4'</span>: <span class="number">3</span>, <span class="string">'item5'</span>: <span class="number">3</span>},</span><br><span class="line">    <span class="string">'user3'</span>: {<span class="string">'item1'</span>: <span class="number">4</span>, <span class="string">'item2'</span>: <span class="number">3</span>, <span class="string">'item3'</span>: <span class="number">4</span>, <span class="string">'item4'</span>: <span class="number">3</span>, <span class="string">'item5'</span>: <span class="number">5</span>},</span><br><span class="line">    <span class="string">'user4'</span>: {<span class="string">'item1'</span>: <span class="number">3</span>, <span class="string">'item2'</span>: <span class="number">3</span>, <span class="string">'item3'</span>: <span class="number">1</span>, <span class="string">'item4'</span>: <span class="number">5</span>, <span class="string">'item5'</span>: <span class="number">4</span>},</span><br><span class="line">    <span class="string">'user5'</span>: {<span class="string">'item1'</span>: <span class="number">1</span>, <span class="string">'item2'</span>: <span class="number">5</span>, <span class="string">'item3'</span>: <span class="number">5</span>, <span class="string">'item4'</span>: <span class="number">2</span>, <span class="string">'item5'</span>: <span class="number">1</span>},</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>使用字典来建立用户-物品的交互表</p>
<p><font color="DarkViolet">由于现实场景中，用户对物品的评分比较稀疏，如果直接使用矩阵进行存储，会存在大量空缺值，故此处使用了字典</font></p>
</li>
<li><p>基于皮尔逊相关系数，计算用户相似性矩阵</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化相似性矩阵</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">similarity_matrix = pd.DataFrame(</span><br><span class="line">    np.identity(<span class="built_in">len</span>(user_data)),</span><br><span class="line">    index=user_data.keys(),</span><br><span class="line">    columns=user_data.keys(),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历每条用户-物品评分数据</span></span><br><span class="line"><span class="keyword">for</span> u1, items1 <span class="keyword">in</span> user_data.items():</span><br><span class="line">    <span class="keyword">for</span> u2, items2 <span class="keyword">in</span> user_data.items():</span><br><span class="line">        <span class="keyword">if</span> u1 == u2:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        vec1, vec2 = [], []</span><br><span class="line">        <span class="keyword">for</span> item, rating1 <span class="keyword">in</span> items1.items():</span><br><span class="line">            rating2 = items2.get(item, -<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> rating2 == -<span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            vec1.append(rating1)</span><br><span class="line">            vec2.append(rating2)</span><br><span class="line">        <span class="comment"># 计算不同用户之间的皮尔逊相关系数</span></span><br><span class="line">        similarity_matrix[u1][u2] = np.corrcoef(vec1, vec2)[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(similarity_matrix)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">          user1     user2     user3     user4     user5</span><br><span class="line">user1  1.000000  0.852803  0.707107  0.000000 -0.792118</span><br><span class="line">user2  0.852803  1.000000  0.467707  0.489956 -0.900149</span><br><span class="line">user3  0.707107  0.467707  1.000000 -0.161165 -0.466569</span><br><span class="line">user4  0.000000  0.489956 -0.161165  1.000000 -0.641503</span><br><span class="line">user5 -0.792118 -0.900149 -0.466569 -0.641503  1.000000</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>计算用户1最相似的n个用户</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">target_user = <span class="string">'user1'</span></span><br><span class="line">num = <span class="number">2</span></span><br><span class="line"><span class="comment"># 由于最相似的用户为自己，去除本身</span></span><br><span class="line">sim_users = similarity_matrix[target_user].sort_values(ascending=<span class="literal">False</span>)[<span class="number">1</span>:num+<span class="number">1</span>].index.tolist()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'与用户<span class="subst">{target_user}</span>最相似的<span class="subst">{num}</span>个用户为：<span class="subst">{sim_users}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">与用户user1最相似的2个用户为：['user2', 'user3']</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>预测用户1对物品5的评分</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">corr_values_sum = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">target_item = <span class="string">'item5'</span></span><br><span class="line"><span class="comment"># 基于皮尔逊相关系数预测用户评分</span></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> sim_users:</span><br><span class="line">    corr_value = similarity_matrix[target_user][user]</span><br><span class="line">    user_mean_rating = np.mean(<span class="built_in">list</span>(user_data[user].values()))</span><br><span class="line"></span><br><span class="line">    weighted_scores += corr_value * (user_data[user][target_item] - user_mean_rating)</span><br><span class="line">    corr_values_sum += corr_value</span><br><span class="line"></span><br><span class="line">target_user_mean_rating = np.mean(<span class="built_in">list</span>(user_data[target_user].values()))</span><br><span class="line">target_item_pred = target_user_mean_rating + weighted_scores / corr_values_sum</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'用户<span class="subst">{target_user}</span>对物品<span class="subst">{target_item}</span>的预测评分为：<span class="subst">{target_item_pred:<span class="number">.4</span>f}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户user1对物品item5的预测评分为：4.8720</span><br></pre></td></tr></tbody></table></figure></li>
</ol>
<h3 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h3><p>UserCF 和 ItemCF 虽然直观易懂，但在真实场景中都会受到<font color="DarkViolet">数据稀疏性</font>的限制：大多数用户只与很少的物品发生过交互</p>
<p>这使得相似度既难以可靠计算，即便找到邻居，覆盖的物品也十分有限</p>
<p>邻域方法通常先计算相似度、再进行推荐，这种两阶段流程缺乏对整体交互数据的全局建模</p>
<p>转向另一种思路：不再显式计算相似度，而是通过学习<font color="DarkViolet">用户和物品的隐向量表示</font>，让相似性在向量空间中自然体现</p>
<p>矩阵分解正是这一思想的代表，也标志着<font color="Violetred">协同过滤从传统统计方法走向机器学习方法</font></p>
<p>矩阵分解的核心想法建立在两个关键假设上：</p>
<ol>
<li>低秩假设：看似复杂的评分矩阵实际上只由少数几个隐含因素主导，比如“面向男性vs面向女性”、“严肃vs逃避现实”等维度</li>
<li>隐向量假设：用户和物品都可以表示为这些隐含因素构成的向量，用户向量刻画偏好，物品向量刻画特征</li>
</ol>
<h4 id="基础模型FunkSVD"><a href="#基础模型FunkSVD" class="headerlink" title="基础模型FunkSVD"></a>基础模型FunkSVD</h4><p>基本思想：把复杂的用户-物品评分矩阵分解成两个简单的矩阵——用户特征矩阵和物品特征矩阵</p>
<p>假设有$m$个用户和$n$个物品，想要用$K$个隐含因子来描述它们，那么用户$u$可以用一个$K$维向量$p_u$来表示，物品$i$也可以用一个$K$维向量$q_i$来表示，预测用户$u$对物品$i$的评分就是这两个向量的内积：<br>$$<br>\hat r_{ui} = p_u^T q_i = \sum_{k=1}^{K} p_{u,k} \cdot q_{i,k}<br>$$<br>$p_{u,k}$表示用户$u$在第$k$个隐含因子上的偏好程度，$q_{i,k}$表示物品$i$在第$k$个隐含因子上的特征强度</p>
<p><font color="Violetred">现在问题变成了：如何找到这些隐含因子？</font></p>
<p>采用一个很自然的思路——让预测评分尽可能接近真实评分，也就是要最小化所有已知评分的预测误差<br>$$<br>\min_{P,Q} \frac{1}{2} \sum_{(u,i)\in \mathcal{K}} \left( r_{ui} - p_u^T q_i \right)^2<br>$$<br>$\mathcal{K}$表示所有已知评分的用户-物品对，$r_{ui}$是用户$u$对物品$i$的真实评分</p>
<p>使用梯度下降法，先计算预测误差$e_{ui} = r_{ui} - p_u^T q_i$，然后沿着误差减小的方向更新参数：<br>$$<br>p_{u,k} \leftarrow p_{u,k} + \eta \cdot e_{ui} \cdot q_{i,k}\\<br>q_{i,k} \leftarrow q_{i,k} + \eta \cdot e_{ui} \cdot p_{u,k}<br>$$<br>在实际应用中通常还会加入L2正则化来防止过拟合<br>$$<br>\min_{P,Q} \frac{1}{2} \sum_{(u,i)\in \mathcal{K}} \left( r_{ui} - p_u^T q_i \right)^2 + \lambda \left( |p_u|^2 + |q_i|^2 \right)<br>$$<br><strong>核心代码</strong></p>
<p>FunkSVD的核心在于学习用户和物品的隐向量表示，使用Embedding层来表示这些隐向量，并通过内积计算预测评分：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># === 用户塔 ===</span></span><br><span class="line"><span class="comment"># 用户潜在因子</span></span><br><span class="line">user_factors = Embedding(</span><br><span class="line">    user_vocab_size,</span><br><span class="line">    embedding_dim,</span><br><span class="line">    embeddings_initializer=<span class="string">"normal"</span>,</span><br><span class="line">    embeddings_regularizer=tf.keras.regularizers.l2(<span class="number">0.02</span>),</span><br><span class="line">    name=<span class="string">"user_factors"</span>,</span><br><span class="line">)(user_id_input)</span><br><span class="line">user_factors = Flatten()(user_factors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># === 物品塔 ===</span></span><br><span class="line"><span class="comment"># 物品潜在因子</span></span><br><span class="line">item_factors = Embedding(</span><br><span class="line">    item_vocab_size,</span><br><span class="line">    embedding_dim,</span><br><span class="line">    embeddings_initializer=<span class="string">"normal"</span>,</span><br><span class="line">    embeddings_regularizer=tf.keras.regularizers.l2(<span class="number">0.02</span>),</span><br><span class="line">    name=<span class="string">"item_factors"</span>,</span><br><span class="line">)(item_id_input)</span><br><span class="line">item_factors = Flatten()(item_factors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测：计算用户向量和物品向量的内积</span></span><br><span class="line">prediction = Dot(axes=<span class="number">1</span>)([user_factors, item_factors])</span><br></pre></td></tr></tbody></table></figure>

<p><code>embedding_dim</code>对应公式中的隐含因子数量$K$</p>
<p><code>user_factors</code>对应$p_u$，<code>item_factors</code>对应$q_i$</p>
<p>通过训练，模型会自动学习最优的隐向量表示，使预测评分尽可能接近真实评分</p>
<h4 id="改进模型BiasSVD"><a href="#改进模型BiasSVD" class="headerlink" title="改进模型BiasSVD"></a>改进模型BiasSVD</h4><p>基础模型虽然简洁，但不同用户的评分习惯差异很大</p>
<p>BiasSVD在基础模型的基础上引入了偏置项，让预测公式变成：<br>$$<br>\hat r_{ui} = \mu + b_u + b_i + p_u^T q_i<br>$$<br>新增三个项</p>
<ul>
<li>$\mu$：所有评分的全局平均值，反映了整个系统的评分水平</li>
<li>$b_u$：用户$u$的个人偏置，反映了该用户的打分习惯</li>
<li>$b_i$：物品$i$的偏置，反映了该物品相对于平均水平是受欢迎还是不受欢迎</li>
</ul>
<p>相应地，优化目标也要调整：<br>$$<br>\min_{P,Q,b_u,b_i} \frac{1}{2} \sum_{(u,i)\in \mathcal{K}} \left( r_{ui} - \mu - b_u - b_i - p_u^T q_i \right)^2 + \lambda \left( |p_u|^2 + |q_i|^2 + b_u^2 + b_i^2 \right)<br>$$<br>在参数更新时，除了用户和物品的隐向量，还需要更新偏置项<br>$$<br>b_u \leftarrow b_u + \eta \left( e_{ui} - \lambda b_u \right)\\<br>b_i \leftarrow b_i + \eta \left( e_{ui} - \lambda b_i \right)<br>$$<br>这种改进看似简单，但效果显著，通过分离出系统性偏差，模型能够更准确地捕捉用户和物品之间的真实交互模式，从而提供更精准的推荐</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># === 用户塔 ===</span></span><br><span class="line"><span class="comment"># 用户潜在因子</span></span><br><span class="line">user_factors = Embedding(</span><br><span class="line">    user_vocab_size,</span><br><span class="line">    embedding_dim,</span><br><span class="line">    embeddings_initializer=<span class="string">"normal"</span>,</span><br><span class="line">    embeddings_regularizer=tf.keras.regularizers.l2(<span class="number">0.02</span>),</span><br><span class="line">    name=<span class="string">"user_factors"</span>,</span><br><span class="line">)(user_id_input)</span><br><span class="line">user_factors = Flatten()(user_factors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户偏置</span></span><br><span class="line">user_bias = Embedding(</span><br><span class="line">    user_vocab_size,</span><br><span class="line">    <span class="number">1</span>,</span><br><span class="line">    embeddings_initializer=<span class="string">"zeros"</span>,</span><br><span class="line">    embeddings_regularizer=tf.keras.regularizers.l2(<span class="number">0.02</span>),</span><br><span class="line">    name=<span class="string">"user_bias"</span>,</span><br><span class="line">)(user_id_input)</span><br><span class="line">user_bias = Flatten()(user_bias)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户表示: [因子, 偏置]</span></span><br><span class="line">user_representation = tf.keras.layers.Concatenate()([user_factors, user_bias])</span><br><span class="line"></span><br><span class="line"><span class="comment"># === 物品塔 ===</span></span><br><span class="line"><span class="comment"># 物品潜在因子</span></span><br><span class="line">item_factors = Embedding(</span><br><span class="line">    item_vocab_size,</span><br><span class="line">    embedding_dim,</span><br><span class="line">    embeddings_initializer=<span class="string">"normal"</span>,</span><br><span class="line">    embeddings_regularizer=tf.keras.regularizers.l2(<span class="number">0.02</span>),</span><br><span class="line">    name=<span class="string">"item_factors"</span>,</span><br><span class="line">)(item_id_input)</span><br><span class="line">item_factors = Flatten()(item_factors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 物品偏置</span></span><br><span class="line">item_bias = Embedding(</span><br><span class="line">    item_vocab_size,</span><br><span class="line">    <span class="number">1</span>,</span><br><span class="line">    embeddings_initializer=<span class="string">"zeros"</span>,</span><br><span class="line">    embeddings_regularizer=tf.keras.regularizers.l2(<span class="number">0.02</span>),</span><br><span class="line">    name=<span class="string">"item_bias"</span>,</span><br><span class="line">)(item_id_input)</span><br><span class="line">item_bias = Flatten()(item_bias)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 物品表示: [因子, 偏置]</span></span><br><span class="line">item_representation = tf.keras.layers.Concatenate()([item_factors, item_bias])</span><br><span class="line"></span><br><span class="line"><span class="comment"># === 独立的用户和物品模型 ===</span></span><br><span class="line"><span class="comment"># 把两个张量拼在一起，作为一个“联合表示”</span></span><br><span class="line">user_model = Model(</span><br><span class="line">    inputs=user_inputs, outputs=user_representation, name=<span class="string">"user_tower"</span></span><br><span class="line">)</span><br><span class="line">item_model = Model(</span><br><span class="line">    inputs=item_inputs, outputs=item_representation, name=<span class="string">"item_tower"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># === 训练模型 ===</span></span><br><span class="line"><span class="comment"># 计算交互项: user_factors · item_factors</span></span><br><span class="line">interaction = Dot(axes=<span class="number">1</span>)([user_factors, item_factors])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全局偏置 - 使用带常数输入的 Dense 层的简单方法</span></span><br><span class="line">ones_input = tf.keras.layers.Lambda(<span class="keyword">lambda</span> x: tf.ones_like(x))(interaction)</span><br><span class="line">global_bias = Dense(</span><br><span class="line">    <span class="number">1</span>, use_bias=<span class="literal">True</span>, kernel_initializer=<span class="string">"zeros"</span>, name=<span class="string">"global_bias"</span></span><br><span class="line">)(ones_input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># BiasSVD 预测：global_bias + user_bias + item_bias + interaction</span></span><br><span class="line">prediction = Add()([global_bias, user_bias, item_bias, interaction])</span><br></pre></td></tr></tbody></table></figure>

<p>与FunkSVD相比，BiasSVD的关键区别在于：</p>
<ul>
<li>用户和物品都增加了独立的偏置Embedding(<code>user_bias</code>和<code>item_bias</code>)</li>
<li>添加了全局偏置项(<code>global_bias</code>)，对应公式中的 $\mu$</li>
<li>最终预测使用<code>Add</code>层将四个部分相加：$\mu + b_u + b_i + p_u^T q_i$</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><table>
<thead>
<tr>
<th>方法</th>
<th>建模对象</th>
<th>相似性来源</th>
<th>学习方式</th>
<th>是否显式相似度</th>
<th>是否有向量表示</th>
<th>是否含 Bias</th>
<th>主要解决什么问题</th>
<th>典型使用阶段</th>
</tr>
</thead>
<tbody><tr>
<td><strong>UserCF</strong></td>
<td>用户–用户</td>
<td>用户行为重叠</td>
<td>统计共现</td>
<td>是</td>
<td>否</td>
<td>否</td>
<td>找兴趣相似的用户</td>
<td>教学 / 原型</td>
</tr>
<tr>
<td><strong>ItemCF</strong></td>
<td>物品–物品</td>
<td>物品被同一用户点击</td>
<td>统计共现</td>
<td>是</td>
<td>否</td>
<td>否</td>
<td>稳定、可缓存的相似物品</td>
<td>传统召回</td>
</tr>
<tr>
<td><strong>Swing</strong></td>
<td>物品–物品</td>
<td>高质量用户共现</td>
<td>加权统计</td>
<td>是(加权)</td>
<td>否</td>
<td>否</td>
<td>稀疏数据下的相似度可靠性</td>
<td>大规模召回</td>
</tr>
<tr>
<td><strong>FunkSVD</strong></td>
<td>用户–物品</td>
<td>向量内积</td>
<td>优化目标学习</td>
<td>否</td>
<td>是</td>
<td>否</td>
<td>隐式兴趣建模、Embedding 召回</td>
<td>Embedding 召回</td>
</tr>
<tr>
<td><strong>BiasSVD</strong></td>
<td>用户–物品</td>
<td>向量内积 + 偏置</td>
<td>优化目标学习</td>
<td>否</td>
<td>是</td>
<td>是</td>
<td>拟合系统性偏好、绝对打分</td>
<td>评分 / 精排</td>
</tr>
</tbody></table>
<h2 id="向量召回"><a href="#向量召回" class="headerlink" title="向量召回"></a>向量召回</h2><p>矩阵分解奠定了推荐系统中向量化建模的基础，但矩阵分解是线性模型，表达能力有限，主要依赖<strong>用户–物品交互矩阵</strong>，在超大规模场景下面临数据稀疏以及计算与存储压力</p>
<p>向量召回延续矩阵分解的向量表示思想，用更复杂的模型(DNN)学习向量，融合多源特征，提升表达能力</p>
<p>加上NLP领域的嵌入(Embedding)技术，将离散符号映射到连续向量空间，<font color="Violetred">向量距离具备语义意义(Word2Vec)</font></p>
<p>向量召回借鉴该思想，<font color="DarkViolet">用户和物品共享向量空间</font>，距离表示相似度，<font color="DarkViolet">推荐问题转化为向量检索</font></p>
<p>向量召回技术主要沿着两条路径发展</p>
<ul>
<li><strong>I2I(Item-to-Item)召回</strong>：用“物品向量”去检索“相似物品向量”</li>
<li><strong>U2I(User-to-Item)召回</strong>：用“用户向量”去检索“物品向量”</li>
</ul>
<h3 id="I2I召回"><a href="#I2I召回" class="headerlink" title="I2I召回"></a>I2I召回</h3><p>所有I2I召回方法的本质都是在回答同一个问题：如何更好地定义和利用“序列”来学习物品之间的相似性</p>
<h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><p>Word2Vec是序列建模的理论基础</p>
<p>主要包含两种模型架构：</p>
<ul>
<li>Skip-Gram：用中心词 → 预测上下文，<strong>对低频词更友好</strong>，<font color="DarkViolet">推荐系统广泛采用 Skip-gram 视角</font></li>
<li>CBOW(Continuous Bag of Words)：用上下文 → 预测中心词，对高频词友好</li>
</ul>
<p><strong>Skip-Gram模型</strong></p>
<p>给定文本序列中位置$t$的中心词$w_t$，模型的目标是最大化其上下文窗口内所有词语的出现概率</p>
<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/w2v_skip_gram.webp" alt="w2v_skip_gram"></p>
<p>中心词$w_t$预测上下文词$w_{t+j}$的条件概率定义为(softmax格式)<br>$$<br>P(w_{t+j} | w_t) = \frac{e^{v_{w_{t+j}}^T v_{w_t}}}{\sum_{k=1}^{|V|} e^{v_{w_k}^T v_{w_t}}}<br>$$<br>其中$v_{w_i}$表示词$w_i$的向量表示，$V$是词汇表，分子中的内积$v_{w_{t+j}}^T v_{w_t}$衡量了中心词与上下文词的相似度</p>
<p><strong>负采样优化</strong></p>
<p>在 Word2Vec 中，直接计算 softmax 分母需要遍历整个词表，计算代价极高，为此引入了负采样(Negative Sampling)来进行优化，<font color="DarkViolet">从多分类 → 多个二分类</font>，其目标函数为：</p>
<p>$$<br>\color{red}\log \sigma(v_{w_{t+j}}^T v_{w_t}) + \sum_{i=1}^{k} \mathbb E_{w_i \sim P_n(w)} \log \sigma(-v_{w_i}^T v_{w_t})<br>$$<br>其中$\color{purple}\sigma(x) = \frac{1}{1 + e^{-x}}$是sigmoid函数(<font color="DarkViolet">二分类问题常用</font>)，$k$是负样本数量，$P_n(w)$是负采样分布</p>
<p>负采样的直观解释是：对于真实的词对，希望增加它们的相似度；对于随机采样的负样本词对，希望降低它们的相似度</p>
<h4 id="Item2Vec"><a href="#Item2Vec" class="headerlink" title="Item2Vec"></a>Item2Vec</h4><p>Item2Vec<u>(Barkan and Koenigstein, 2016)</u>的核心洞察在于发现了用户行为数据与文本数据的结构相似性</p>
<p>在文本中，一个句子由多个词语组成，词语之间的共现关系反映了语义相似性</p>
<p><font color="Violetred">在推荐系统中，每个用户的交互历史可以看作一个“句子”，其中包含的物品就是“词语”</font></p>
<p>这种映射关系可以表示为：</p>
<ul>
<li>词语 → 物品</li>
<li>句子 → 用户交互序列</li>
<li>词语共现 → 物品共同被用户交互</li>
</ul>
<p>Item2Vec直接采用Word2Vec的Skip-Gram架构，但在序列构建上有所简化</p>
<p>给定数据集$\mathcal{S} = {s_1, s_2, \ldots, s_n}$，其中每个$s_i$包含用户$i$交互过的所有物品，Item2Vec将每个用户的交互历史视为一个集合而非序列，忽略了交互的时间顺序</p>
<p>优化目标函数与Word2Vec保持一致：<br>$$<br>\color{red}\mathcal L = \sum_{s \in \mathcal{S}} \sum_{l_{i} \in s} \sum_{-m \leq j \leq m, j \neq 0} \log P(l_{i+j} | l_{i})<br>$$<br>其中$l_i$表示物品，$m$是上下文窗口大小，$P(l_{i+j} | l_{i})$采用与Word2Vec相同的softmax形式计算</p>
<p>Item2Vec的实现可以直接调用gensim库<u>(Řehůřek and Sojka, 2010)</u>的Word2Vec模型，核心在于将用户交互序列作为训练语料，训练完成后，每个物品都得到一个稠密的向量表示</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Item2Vec</span>:  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, train_hist_movie_id_list</span>):</span><br><span class="line">        <span class="comment"># train_hist_movie_id_list: 用户交互序列列表</span></span><br><span class="line">        <span class="comment"># 每个元素是一个用户的物品ID序列</span></span><br><span class="line">        <span class="variable language_">self</span>.model = Word2Vec(</span><br><span class="line">            train_hist_movie_id_list,                     <span class="comment"># 用户交互数据集</span></span><br><span class="line">            vector_size=<span class="variable language_">self</span>.model_config[<span class="string">"EmbDim"</span>],      <span class="comment"># 嵌入维度</span></span><br><span class="line">            window=<span class="variable language_">self</span>.model_config[<span class="string">"Window"</span>],           <span class="comment"># 上下文窗口大小</span></span><br><span class="line">            min_count=<span class="variable language_">self</span>.model_config[<span class="string">"MinCount"</span>],      <span class="comment"># 最小出现次数</span></span><br><span class="line">            workers=<span class="variable language_">self</span>.model_config[<span class="string">"Workers"</span>],         <span class="comment"># 并行线程数</span></span><br><span class="line">        )</span><br></pre></td></tr></tbody></table></figure>

<hr>
<p>Item2Vec使用了序列形式的数据，但训练目标<font color="DarkViolet">只关心是否共现</font>，不关心先后顺序</p>
<p>U1：手机 → 手机壳 → 钢化膜 → 充电线 → 移动电源  (标准购买路径)</p>
<p>U2：手机 → 蓝牙耳机 → 手机壳 → 充电线  (跳跃 + 回退)</p>
<p>U3：手机 → 游戏手柄 → 电竞耳机   (冷启动 + 小众)</p>
<p>Item2Vec 会从所有序列中抽共现对</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(手机, 手机壳)</span><br><span class="line">(手机壳, 钢化膜)</span><br><span class="line">(钢化膜, 充电线)</span><br><span class="line">(充电线, 移动电源)</span><br><span class="line"></span><br><span class="line">(手机, 蓝牙耳机)</span><br><span class="line">(蓝牙耳机, 手机壳)</span><br><span class="line">(手机壳, 充电线)</span><br><span class="line"></span><br><span class="line">(手机, 游戏手柄)</span><br><span class="line">(游戏手柄, 电竞耳机)</span><br></pre></td></tr></tbody></table></figure>

<p>顺序在模型中是弱化甚至被对称化的</p>
<p>Item2Vec 会倾向于把 手机壳 / 钢化膜 / 充电线 / 移动电源 拉得很近，把游戏手柄漂的比较远，小众兴趣被头部物品淹没</p>
<h4 id="EGES"><a href="#EGES" class="headerlink" title="EGES"></a>EGES</h4><p>Item2Vec 证明了序列建模在推荐系统中的有效性，但其设计较为简单，也存在明显不足</p>
<ol>
<li>将用户行为视为无序集合，忽略时序信息，难以刻画真实的行为模式</li>
<li>严重依赖历史交互数据，面对新物品时无法学习有效表示</li>
</ol>
<p>EGES<u>(wang, 2018)</u>针对这些问题进行了改进：</p>
<ol>
<li>基于会话构建更精细的商品关系图，更准确地建模用户行为</li>
<li>引入商品的辅助信息，缓解冷启动问题</li>
</ol>
<p><strong>构建商品关系图</strong></p>
<p>EGES的第一个创新是将物品序列的概念从简单的用户交互扩展为更精细的会话级序列</p>
<p>在固定时间窗口内，当两个商品在用户行为序列中连续出现时，便在它们之间建立一条<font color="Violetred">有向边</font>，其权重由该转移关系在所有用户历史中出现的频率决定</p>
<p>![eges_item_graph (1)](<a target="_blank" rel="noopener" href="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/eges_item_graph">https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/eges_item_graph</a> (1).webp)</p>
<p>相比将整个用户历史视为一个序列，这种基于会话的商品图能够更准确地刻画用户在短时间内的连续兴趣转移</p>
<p>在此基础上，EGES 在商品图上进行采用带权随机游走策略生成训练序列，转移概率由边权重决定：<br>$$<br>\begin{split}P(v_j|v_i) = \begin{cases}<br>\frac{M_{ij}}{\sum_{j=1}^{|N_+(v_i)|}M_{ij}} &amp; \text{if } v_j \in N_+(v_i) \<br>0 &amp; \text{if } e_{ij} \notin E<br>\end{cases}\end{split}<br>$$<br>其中$M_{ij}$表示节点$v_i$到节点$v_j$的边权重，$N_+(v_i)$表示节点$v_i$的邻居集合</p>
<p>通过随机游走过程，可以生成大量的商品序列用于后续的embedding学习</p>
<p><strong>融合辅助信息解决稀疏性问题</strong></p>
<p>引入商品的辅助信息(如类别、品牌、价格区间等)来增强商品的向量表示</p>
<p><font color="DarkViolet">GES的核心思想</font>是将商品本身的Embedding与其各种属性的Embedding进行平均聚合<br>$$<br>H_v=\frac{1}{n+1} \sum_{s=0}^n{W_v^s}<br>$$<br>其中$W_v^s$表示商品$v$的第$s$种属性的向量表示，$W_v^0$表示商品ID的向量表示</p>
<p>这种方法虽然有效缓解了冷启动问题，但它假设所有类型的辅助信息对商品表示的贡献是相等的，这显然不符合实际情况</p>
<p><font color="DarkViolet">EGES的核心创新</font>在于认识到不同类型的辅助信息应该有不同的重要性</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/eges_model.webp" alt="eges_model" style="zoom: 33%;">

<p>对于具有$n$种辅助信息的商品$v$，EGES为其维护$n+1$个向量表示：一个商品ID的向量表示，以及$n$个属性的向量表示</p>
<p>商品的最终向量表示通过加权聚合得到<br>$$<br>H_v =\sum_{j}\left(\frac{e^{a_{v}^{j}}}{\sum_{k} e^{a_{v}^{k}}}\right) W_{v}^{j} = \frac{\sum_{j=0}^n e^{a_v^j} W_v^j}{\sum_{j=0}^n e^{a_v^j}}<br>$$<br>其中$a_v^j$是可学习的权重参数，这种设计体现了不同类型的辅助信息对不同商品的重要性</p>
<p><strong>核心代码</strong></p>
<p>EGES的核心在于商品<font color="Violetred">特定注意力层(ItemSpecificAttentionLayer)</font>，它为每个商品学习一组特征权重</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, item_indices</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        inputs: 特征嵌入 [batch_size, n+1, emb_dim]</span></span><br><span class="line"><span class="string">        item_indices: 商品索引 [batch_size]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 获取每个商品的索引取出对应的权重参数 a_v^j</span></span><br><span class="line">    <span class="comment"># self.attention_weights：[num_items, n+1]</span></span><br><span class="line">    batch_attention_weights = tf.gather(<span class="variable language_">self</span>.attention_weights, item_indices)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 e^(a_v^j)</span></span><br><span class="line">    exp_attention = tf.exp(batch_attention_weights)  <span class="comment"># [batch_size, n+1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 归一化权重: e^(a_v^j) / sum(e^(a_v^j))</span></span><br><span class="line">    attention_sum = tf.reduce_sum(exp_attention, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    normalized_attention = exp_attention / attention_sum</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用权重到特征嵌入</span></span><br><span class="line">    normalized_attention = tf.expand_dims(normalized_attention, axis=-<span class="number">1</span>) <span class="comment"># [batch_size, n+1, 1]</span></span><br><span class="line">    weighted_embedding = inputs * normalized_attention  <span class="comment"># [batch_size, n+1, emb_dim]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求和得到最终的商品表示 H_v</span></span><br><span class="line">    output = tf.reduce_sum(weighted_embedding, axis=<span class="number">1</span>)  <span class="comment"># [batch_size, emb_dim]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output, normalized_attention</span><br></pre></td></tr></tbody></table></figure>

<p>这里的<code>attention_weights</code>是一个形状为$|V| \times (n+1)$的参数矩阵，其中$|V|$是商品总数，$n+1$特征数量(商品ID + $n$种辅助信息)</p>
<p>对于每个商品，模型会学习到一组特定的权重，自动发现哪些特征对该商品更重要，这种商品特定的注意力机制是EGES相比简单平均聚合的关键优势</p>
<p><strong>冷启动商品的处理</strong></p>
<p>对于无任何用户行为的新商品，既无法学习 item ID 向量，也没有对应的注意力权重</p>
<p><font color="DarkViolet">EGES 采用 mean pooling 策略</font>，将商品的各类辅助特征向量(如类目、品牌、价格区间等)直接取平均，构造商品表示</p>
<p>虽然忽略了属性重要性的差异，但能有效利用内容信息，使冷启动商品仍可参与向量相似度召回</p>
<p><strong>训练优化</strong></p>
<p>EGES采用与Word2Vec类似的负采样策略，但损失函数经过了优化：<br>$$<br>\color{red}L(v,u,y) = -[y\log(\sigma(H_v^TZ_u)) + (1-y)\log(1-\sigma(H_v^TZ_u))]<br>$$<br>其中$y$是标签(1表示正样本，0表示负样本)，这里仍然是自监督标签，不是人工标注</p>
<p>$H_v$是商品$v$的向量表示，$Z_u$是上下文节点$u$的向量表示，$H_v^TZ_u$衡量商品$v$和上下文商品$u$是否“应该有关联”</p>
<p>EGES在淘宝的实际部署效果显著</p>
<hr>
<p><font color="Violetred">为什么EGES 不属于“序列召回”？</font></p>
<p>因为有向序列只是用于“构图”，不会作为模型输入序列去建模时间顺序</p>
<p>A → B → C → D 模型看到的只是 (A, B), (B, C), (C, D)，路径结构在这里消失了，方向信息没有形成状态差异</p>
<p>同样的例子：</p>
<p>U1：手机 → 手机壳 → 钢化膜 → 充电线 → 移动电源  (标准购买路径)</p>
<p>U2：手机 → 蓝牙耳机 → 手机壳 → 充电线  (跳跃 + 回退)</p>
<p>U3：手机 → 游戏手柄 → 电竞耳机   (冷启动 + 小众)</p>
<p>EGES能够在购买手机后推荐游戏手柄，给出小众但合理的扩展</p>
<p>但和Item2Vec一样给出的都是静态 item embedding，无法知道现在处于什么阶段</p>
<h4 id="Airbnb"><a href="#Airbnb" class="headerlink" title="Airbnb"></a>Airbnb</h4><p>Airbnb作为全球最大的短租平台，面临着与传统电商不同的挑战</p>
<p>房源不是标准化商品，用户的预订行为远比点击浏览稀疏，而且地理位置成为了一个关键因素</p>
<p>Airbnb需要的不仅仅是相似性，而是能够真正促进最终预订转化的推荐，其重新定义了“序列”的概念<u>(Grbovic and Cheng, 2018)</u>，采用基于会话的序列构建策略</p>
<p><strong>面向业务的序列构建</strong></p>
<ul>
<li><strong>会话切分机制</strong>：不再把用户所有历史点击简单串联，而是按点击会话(Click Sessions)构建序列；当相邻点击间隔超过 30 分钟时，视为新的会话，从而更准确刻画用户在单一搜索场景下的连续意图</li>
<li><strong>行为权重差异化</strong>：不同用户行为的信号强度不同，预订相比普通点击更能反映真实偏好，因此在模型训练中赋予更高权重</li>
</ul>
<p><strong>全局上下文机制</strong></p>
<p>为了强化模型对最终转化行为的学习，Airbnb设计了全局上下文机制</p>
<p>在传统的Skip-Gram模型中，只有在滑动窗口内的物品才被视为上下文，但这种局部窗口无法充分利用最终预订这一强烈的正向信号</p>
<p>Airbnb让用户最终预订的房源(booked listing)与序列中的每一个浏览房源都形成正样本对进行训练，无论它们在序列中的距离有多远</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/airbnb_global_context.webp" alt="airbnb_global_context" style="zoom: 33%;">

<p>针对有预订行为的会话(booked sessions)，Airbnb修改了优化目标函数，增加了全局上下文项：<br>$$<br>\color{red}\underset{\theta}{\text{argmax}} \sum_{(l,c) \in \mathcal D_p} \log \frac{1}{1 + e^{-v_c^T v_l}} + \sum_{(l,c) \in \mathcal D_n} \log \frac{1}{1 + e^{v_c^T v_l}} + \log \frac{1}{1 + e^{-v_{l_b}^T v_l}}<br>$$<br>前两项是标准的Skip-Gram目标函数：</p>
<ul>
<li><p>第一项最大化正样本对$(l,c)$的相似度，其中$l$是目标房源，$c$是滑动窗口内的上下文房源；</p>
</li>
<li><p>第二项最小化负样本对的相似度</p>
</li>
</ul>
<p>关键的创新在于第三项，$l_b$表示用户在该会话中最终预订的房源，预订房源为序列中的每个房源都提供了额外的学习信号</p>
<p><strong>市场感知的负采样</strong></p>
<p>传统负采样通常从全量房源中随机选择负样本，但在 Airbnb 场景下，用户的预订行为高度受地理位置约束，跨市场的负样本过于“容易区分”，会导致模型过度依赖地理特征而忽略房源本身差异</p>
<p>Airbnb 引入了<font color="Violetred">同市场负采样</font>：在负样本中加入与正样本处于同一城市或地区的房源，迫使模型在相同市场内学习更细粒度的房源差异，从而提升推荐精度，对应的损失项为：<br>$$<br>\sum_{(l, l_m^-) \in \mathcal D_m} \log \frac{1}{1 + e^{v_{l_m^-}^T v_l}}<br>$$<br>其中$l_m^-$表示来自相同市场的负样本</p>
<h3 id="U2I召回"><a href="#U2I召回" class="headerlink" title="U2I召回"></a>U2I召回</h3><p>U2I 召回的核心目标是在海量物品中，高效找到与用户兴趣匹配的候选集</p>
<p>其演进过程，本质上是将复杂的用户–物品匹配问题，转化为高效的向量搜索问题</p>
<p>这一转变的关键突破来自于一个统一的架构思想：<font color="DarkViolet">双塔模型(Two-Tower Model)</font></p>
<p>无论是经典的因子分解机FM、深度结构化语义模型DSSM，还是YouTube的深度神经网络YouTubeDNN，虽然结构不同，但本质一致：<font color="Violetred">分别将用户和物品编码为向量，通过向量相似度衡量匹配程度</font></p>
<h4 id="双塔模型"><a href="#双塔模型" class="headerlink" title="双塔模型"></a>双塔模型</h4><p>双塔模型将推荐问题拆解为两个相对独立的子问题：</p>
<ul>
<li><strong>用户塔(User Tower)</strong>：专注于理解用户——处理用户的历史行为、人口统计学特征、上下文信息等，最终输出一个代表用户兴趣的向量$u$</li>
<li><strong>物品塔(Item Tower)</strong>：专注于刻画物品——整合物品的ID、类别、属性、内容特征等，输出一个表征物品特性的向量$v$</li>
</ul>
<p><font color="DarkViolet">在训练完成后，所有物品的向量都可以离线预计算并存储在高效的向量检索系统中</font>(如Faiss、Annoy等)</p>
<p>当用户发起推荐请求时，<font color="DarkViolet">系统只需实时计算用户向量，然后通过近似最近邻(ANN)搜索获取相似物品</font></p>
<p>该架构将匹配复杂度从全量匹配$O(U \times I)$转化为向量计算$$O(U + I)$$与近邻搜索</p>
<p>用户与物品的匹配度通常通过点积或余弦相似度计算<br>$$<br>score(u, v) = u \cdot v = \sum_{i=1}^{d} u_i v_i<br>$$<br>其中$d$是向量维度, 向量空间中的距离反映了用户兴趣与物品特性的匹配程度</p>
<h4 id="FM因子分解机"><a href="#FM因子分解机" class="headerlink" title="FM因子分解机"></a>FM因子分解机</h4><p>尽管因子分解机(Factorization Machine, FM) <u>(Rendle, 2010)</u> 提出于深度学习之前，但在思想上可视为双塔模型的早期雏形</p>
<p>FM 的核心思想是：将高维稀疏特征之间的二阶交互，分解为低维向量的内积，从而高效建模用户–物品关系</p>
<p>FM 的预测函数为:<br>$$<br>\color{red}\hat y(\mathbf x):=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf v_{i}, \mathbf v_{j}\right\rangle x_{i} x_{j}<br>$$</p>
<ul>
<li>$\mathbf x$：用户特征 + 物品特征拼在一起的一个大特征向量</li>
<li>$x_i$：第 $i$ 个特征的取值，常见0/1</li>
<li>$w_i$：第 $i$ 个特征的一阶权重，表示每个特征对结果的线性影响</li>
<li>$\mathbf v_i$：第 $i$ 个特征对应的 $k$ 维隐向量</li>
</ul>
<p>两个隐向量的内积刻画特征之间的交互强度，只有当两个特征同时出现(都不为 0)时，交互才生效<br>$$<br>\left\langle\mathbf v_{i}, \mathbf v_{j}\right\rangle=\sum_{f=1}^{k} v_{i, f} v_{j, f}<br>$$<br>原本二阶交互项的计算复杂度为$O(kn^2)$复杂度的二阶交互项，可以通过代数运算重写为(<font color="Violetred">一维的平方展开</font>)<br>$$<br>\sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf v_{i}, \mathbf v_{j}\right\rangle x_{i} x_{j}  =\sum_{f=1}^{k} \sum_{i=1}^{n} \sum_{j=i+1}^{n} v_{i, f} v_{j, f} x_{i} x_{j}=\sum_{f=1}^{k} \sum_{i&lt;j} v_{i, f} v_{j, f} x_{i} x_{j}=\frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right)<br>$$</p>
<blockquote>
<p>先把所有特征在这个维度上的值加起来，用平方一次性生成所有特征对的交互，减掉不该要的“自己 × 自己”，剩下的就是交互项</p>
</blockquote>
<p>该变换将计算复杂度降为$O(kn)$，使 FM 能够高效处理大规模稀疏特征</p>
<p><strong>分解为双塔结构</strong></p>
<p>在召回场景中，同一个用户需要与大量候选物品进行匹配，而用户侧特征在这一过程中是固定不变的</p>
<p>从 FM 的二阶交互形式可以看到，其本质包含三类关系：</p>
<table>
<thead>
<tr>
<th>交互类型</th>
<th>是否随用户变</th>
<th>是否随物品变</th>
<th>在召回中地位</th>
</tr>
</thead>
<tbody><tr>
<td>用户 × 用户</td>
<td>❌</td>
<td>❌</td>
<td>常数，直接忽略</td>
</tr>
<tr>
<td>物品 × 物品</td>
<td>❌</td>
<td>✅</td>
<td>物品偏置，可离线</td>
</tr>
<tr>
<td>用户 × 物品</td>
<td>✅</td>
<td>✅</td>
<td>核心匹配信号</td>
</tr>
</tbody></table>
<p>将特征集合划分为用户侧特征集和物品侧特征集后，将FM重新组织，只保留对排序有影响的部分(去除用户×用户项)<br>$$<br>score_{FM} = \sum_{t \in I} w_{t} x_{t} + \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{t \in I} v_{t, f} x_{t}\right)^{2}  - \sum_{t \in I} v_{t, f}^{2} x_{t}^{2}\right)  + \sum_{f=1}^{k}\left( {\sum_{u \in U} v_{u, f} x_{u}}{\sum_{t \in I} v_{t, f} x_{t}} \right)<br>$$</p>
<blockquote>
<p>物品一阶项 + 物品内部二阶交互项 + 用户物品交互项</p>
</blockquote>
<p><font color="DarkViolet">可以将整个匹配分数重新组织为两个向量的内积形式</font><br>$$<br>\color{red}score_{FM} = V_{item} \cdot V_{user}^T<br>$$<br>通过这种重新组织，得到了FM的双塔表示</p>
<ul>
<li>用户向量：$V_{user} = [1; \sum_{u \in U} v_{u} x_{u}]$</li>
<li>物品向量：$V_{item} = [\sum_{t \in I} w_{t} x_{t} + \frac{1}{2} \sum_{f=1}^{k}((\sum_{t \in I} v_{t, f} x_{t})^{2} - \sum_{t \in I} v_{t, f}^{2} x_{t}^{2}); \sum_{t \in I} v_{t} x_{t}]$</li>
</ul>
<p>用户向量包含一个常数项1和用户特征的聚合表示；</p>
<p>物品向量则包含物品的内部交互信息和物品特征的聚合表示，可离线计算；</p>
<p>这样的分解揭示了一个重要原理：即使是复杂的特征交互模式，也可以通过合适的向量表示和简单的内积运算来实现</p>
<p><strong>核心代码</strong></p>
<p>FM召回的双塔实现关键在于如何将数学推导转化为实际的向量表示</p>
<p>用户塔构建了包含常数项和特征聚合的向量：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_user_tower</span>():</span><br><span class="line">    user_embeddings = group_embedding_feature_dict.get(<span class="string">"user"</span>, []) </span><br><span class="line">    <span class="comment"># 计算用户嵌入向量的和：∑(v_u * x_u)</span></span><br><span class="line">    <span class="comment"># x_u对于one-hot编码的类别特征来说就是1</span></span><br><span class="line">    user_concat = Concatenate(axis=<span class="number">1</span>, name=<span class="string">"user_concat"</span>)(</span><br><span class="line">        user_embeddings</span><br><span class="line">    )  <span class="comment"># [batch_size, num_user_features, embedding_dim]</span></span><br><span class="line"></span><br><span class="line">    user_embedding_sum = SumPooling(name=<span class="string">"user_embedding_sum"</span>)(</span><br><span class="line">        user_concat</span><br><span class="line">    )  <span class="comment"># [batch_size, embedding_dim] 对应 ∑(v_u * x_u)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建用户向量：[1; ∑(v_u * x_u)]</span></span><br><span class="line">    ones_vector = OnesLayer(name=<span class="string">"ones_vector"</span>)(</span><br><span class="line">        user_embedding_sum</span><br><span class="line">    )  <span class="comment"># [batch_size, 1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拼接：[1; ∑(v_u * x_u)]</span></span><br><span class="line">    user_vector = Concatenate(axis=<span class="number">1</span>, name=<span class="string">"user_vector"</span>)(</span><br><span class="line">        [ones_vector, user_embedding_sum]</span><br><span class="line">    )  <span class="comment"># [batch_size, embedding_dim + 1]</span></span><br></pre></td></tr></tbody></table></figure>

<blockquote>
<p><code>SumPooling</code> 通常表示在“特征维度”上聚合，但保留 embedding 维度</p>
</blockquote>
<p>物品塔则更为复杂，需要计算一阶线性项和FM交互项：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_item_tower</span>():</span><br><span class="line">    item_embeddings = group_embedding_feature_dict.get(<span class="string">"item"</span>, [])</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 计算物品嵌入向量的和：∑(v_t * x_t)</span></span><br><span class="line">    item_concat = Concatenate(axis=<span class="number">1</span>, name=<span class="string">"item_concat"</span>)(</span><br><span class="line">        item_embeddings</span><br><span class="line">    )  <span class="comment"># [batch_size, num_item_features, embedding_dim]</span></span><br><span class="line">    item_embedding_sum = SumPooling(name=<span class="string">"item_embedding_sum"</span>)(</span><br><span class="line">        item_concat</span><br><span class="line">    )  <span class="comment"># [batch_size, embedding_dim]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算一阶线性项：∑(w_t * x_t)</span></span><br><span class="line">    <span class="comment"># 为每个物品特征学习一个权重</span></span><br><span class="line">    item_linear_weights = Dense(</span><br><span class="line">        <span class="number">1</span>, activation=<span class="string">"linear"</span>, use_bias=<span class="literal">False</span>, name=<span class="string">"item_linear_weights"</span></span><br><span class="line">    )(</span><br><span class="line">        item_embedding_sum</span><br><span class="line">    )  <span class="comment"># [batch_size, 1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算FM二阶交互项：0.5 * ((∑v_t*x_t)² - ∑(v_t²*x_t²))</span></span><br><span class="line">    <span class="comment"># 1. 计算 (∑v_t*x_t)²</span></span><br><span class="line">    sum_squared = SquareLayer(name=<span class="string">"item_sum_squared"</span>)(</span><br><span class="line">        item_embedding_sum</span><br><span class="line">    )  <span class="comment"># [batch_size, embedding_dim]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 计算 ∑(v_t²*x_t²) = ∑(v_t²)，因为对于one-hot特征x_t=1</span></span><br><span class="line">    item_squared = SquareLayer(name=<span class="string">"item_squared"</span>)(</span><br><span class="line">        item_concat</span><br><span class="line">    )  <span class="comment"># [batch_size, num_item_features, embedding_dim]</span></span><br><span class="line">    squared_sum = SumPooling(name=<span class="string">"item_squared_sum"</span>)(</span><br><span class="line">        item_squared</span><br><span class="line">    )  <span class="comment"># [batch_size, embedding_dim]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 计算FM交互项：0.5 * (sum_squared - squared_sum)</span></span><br><span class="line">    fm_interaction_vector = Subtract(name=<span class="string">"fm_subtract"</span>)(</span><br><span class="line">        [sum_squared, squared_sum]</span><br><span class="line">    )  <span class="comment"># [batch_size, embedding_dim]</span></span><br><span class="line">    <span class="comment"># 乘以0.5</span></span><br><span class="line">    fm_interaction_half = ScaleLayer(<span class="number">0.5</span>, name=<span class="string">"fm_half_scale"</span>)(</span><br><span class="line">        fm_interaction_vector</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. 聚合FM交互项为标量</span></span><br><span class="line">    fm_interaction_scalar = SumScalarLayer(name=<span class="string">"fm_interaction_scalar"</span>)(</span><br><span class="line">        fm_interaction_half</span><br><span class="line">    )  <span class="comment"># [batch_size, 1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. 计算first_term = ∑(w_t*x_t) + FM_interaction</span></span><br><span class="line">    first_term = Add(name=<span class="string">"item_first_term"</span>)(</span><br><span class="line">        [item_linear_weights, fm_interaction_scalar]</span><br><span class="line">    )  <span class="comment"># [batch_size, 1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6. 构建物品向量：[first_term; ∑(v_t * x_t)]</span></span><br><span class="line">    item_vector = Concatenate(axis=<span class="number">1</span>, name=<span class="string">"item_vector"</span>)(</span><br><span class="line">        [first_term, item_embedding_sum]</span><br><span class="line">    )  <span class="comment"># [batch_size, embedding_dim + 1]</span></span><br></pre></td></tr></tbody></table></figure>

<blockquote>
<p><code>SumScalarLayer</code>通常表示结束 embedding维度，回到标量打分空间</p>
</blockquote>
<p>最终通过内积计算匹配分数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fm_score = Dot(axes=<span class="number">1</span>)([item_vector, user_vector]) <span class="comment"># [batch_size, 1]</span></span><br></pre></td></tr></tbody></table></figure>

<p>这种设计使得物品向量可以离线预计算，用户向量实时计算，从而支持高效的召回检索</p>
<h4 id="DSSM"><a href="#DSSM" class="headerlink" title="DSSM"></a>DSSM</h4><p>虽然 FM 通过向量分解高效建模特征交互，但其本质仍是线性模型，对复杂的非线性用户–物品关系表达能力有限</p>
<p><font color="Darkviolet">深度结构化语义模型(Deep Structured Semantic Model, DSSM)</font> <u>(Huang <em>et al.</em>, 2013)</u> 引入深度神经网络分别对用户和物品进行建模，以非线性变换替代线性映射，显著增强了双塔模型的特征表达与表示学习能力</p>
<p>其核心思想是通过深度神经网络将用户和物品映射到共同的语义空间中，通过向量间的相似度计算来衡量匹配程度</p>
<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/dssm_architecture.jpg" alt="dssm_architecture"></p>
<p>DSSM 采用双塔结构，用户塔与物品塔分别由独立的 DNN 构成</p>
<p>与 FM 的线性建模不同，DSSM 在塔内引入非线性变换以增强特征表达，而用户与物品之间仅在最终通过向量内积进行交互</p>
<p><strong>多分类训练范式</strong></p>
<p>DSSM将召回任务视为一个极端多分类问题，将物料库中的所有物品看作不同的类别</p>
<p>模型的目标是最大化用户对正样本物品的预测概率：<br>$$<br>P(y|x,\theta) = \frac{e^{s(x,y)}}{\sum_{j\in M}e^{s(x,y_j)}}<br>$$</p>
<ul>
<li>$s(x,y)$：用户$x$和物品$y$的相似度分数</li>
<li>$P(y|x,\theta)$：匹配概率</li>
<li>$M$：整个物料库</li>
</ul>
<p>由于物料库规模庞大，直接计算这个softmax在计算上不可行，因此实际训练时采用负采样技术，为每个正样本采样一定数量的负样本来近似计算</p>
<p><strong>双塔模型的细节</strong></p>
<p><u>(Yi <em>et al.</em>, 2019)</u>等对双塔结构的细节进行了分析</p>
<blockquote>
<p>Yi X, Yang J, Hong L, et al. Sampling-bias-corrected neural modeling for large corpus item recommendations[C]//Proceedings of the 13th ACM conference on recommender systems. 2019: 269-277.</p>
</blockquote>
<p><font color="Violetred">向量归一化</font>：对用户塔和物品塔输出的Embedding进行<strong>L2归一化</strong><br>$$<br>u \leftarrow \frac{u}{||u||_2}, \quad v \leftarrow \frac{v}{||v||_2}<br>$$<br>归一化的主要作用在于<font color="DarkViolet">解决向量点积不具备度量性质的问题</font></p>
<p>原始点积既不满足三角不等式，也无法作为稳定的距离度量，可能导致相似性排序与几何直觉不一致</p>
<blockquote>
<p>对于三个点$A=(10,0)$、$B=(0,10)$、$C=(11,0)$，使用点积计算会得到$\text{dist}(A,B) &lt; \text{dist}(A,C)$，但这与直观的几何距离不符</p>
</blockquote>
<p>在向量归一化后，点积与欧式距离之间建立了等价关系：<br>$$<br>||u - v|| = \sqrt{2-2\langle u,v \rangle}<br>$$<br>对单位向量而言，最大化点积等价于最小化欧式距离</p>
<p>这种转换的关键意义在于<font color="Violetred">训练与检索的一致性</font>：</p>
<p>模型训练阶段使用归一化后的点积作为相似度目标，而线上 ANN 检索系统通常基于欧式距离进行近邻搜索。二者在数学上等价，从而保证了离线训练学到的向量关系能够在线上检索阶段被正确保留，避免训练–服务不一致问题</p>
<p><strong>温度系数调节</strong></p>
<p>在归一化后的向量计算内积后，除以温度系数$\tau$：<br>$$<br>s(u,v) = \frac{\langle u,v \rangle}{\tau}<br>$$<br>这里的温度系数$\tau$看起来是个简单的除法操作，但实际上它对模型的训练效果有着深远的影响</p>
<p>从数学角度来看，温度系数本质上是在缩放logits，进而改变Softmax函数的输出分布形状</p>
<p>$\tau &lt; 1$相似度的差异会被放大，这意味着模型会对高分样本给出更高的概率，预测变得更加“确定”</p>
<p>相反$\tau &gt; 1$分布会变得更加平滑，模型的预测也更加保守</p>
<p><strong>核心代码</strong></p>
<p>DSSM的实现核心在于构建独立的用户塔和物品塔，每个塔都是一个深度神经网络：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拼接用户侧和物品侧特征</span></span><br><span class="line">user_feature = concat_group_embedding(</span><br><span class="line">    group_embedding_feature_dict, <span class="string">"user"</span>, axis=<span class="number">1</span>, flatten=<span class="literal">True</span></span><br><span class="line">)  <span class="comment"># B x (N*D)</span></span><br><span class="line">item_feature = concat_group_embedding(</span><br><span class="line">    group_embedding_feature_dict, <span class="string">"item"</span>, axis=<span class="number">1</span>, flatten=<span class="literal">True</span></span><br><span class="line">)  <span class="comment"># B x (N*D)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建用户塔和物品塔(深度神经网络)</span></span><br><span class="line">user_tower = DNNs(</span><br><span class="line">    units=dnn_units, activation=<span class="string">"tanh"</span>, dropout_rate=dropout_rate, use_bn=<span class="literal">True</span></span><br><span class="line">)(user_feature)</span><br><span class="line">item_tower = DNNs(</span><br><span class="line">    units=dnn_units, activation=<span class="string">"tanh"</span>, dropout_rate=dropout_rate, use_bn=<span class="literal">True</span></span><br><span class="line">)(item_feature)</span><br></pre></td></tr></tbody></table></figure>

<p>关键的向量归一化和相似度计算：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L2归一化：确保训练与检索的一致性</span></span><br><span class="line">user_embedding = tf.keras.layers.Lambda(<span class="keyword">lambda</span> x: tf.nn.l2_normalize(x, axis=<span class="number">1</span>))(</span><br><span class="line">    user_tower</span><br><span class="line">)</span><br><span class="line">item_embedding = tf.keras.layers.Lambda(<span class="keyword">lambda</span> x: tf.nn.l2_normalize(x, axis=<span class="number">1</span>))(</span><br><span class="line">    item_tower</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算余弦相似度(归一化向量的点积)</span></span><br><span class="line">cosine_similarity = tf.keras.layers.Dot(axes=<span class="number">1</span>)([user_embedding, item_embedding])</span><br></pre></td></tr></tbody></table></figure>

<p>这种设计使得用户和物品的表示完全独立，支持离线预计算物品向量并存储在ANN索引中，实现毫秒级的召回响应</p>
<h4 id="YouTubeDNN"><a href="#YouTubeDNN" class="headerlink" title="YouTubeDNN"></a>YouTubeDNN</h4><p>YouTube深度神经网络推荐系统 (<u>Covington <em>et al.</em>, 2016</u>) 代表了双塔模型演进的一个重要里程碑</p>
<p>YouTubeDNN在架构上延续了双塔设计，但引入了一个关键的思想转变：将召回任务重新定义为“预测用户下一个会观看的视频”</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/youtubednn_candidate.webp" alt="youtubednn_candidate" style="zoom: 50%;">

<p>YouTubeDNN 采用了一种<font color="DarkViolet">非对称双塔结构</font></p>
<ul>
<li><p>用户塔融合了观看历史、搜索行为和人口统计等多源特征，其中历史观看视频的 ID 经嵌入层映射后通过平均池化进行聚合，并引入时间相关因素(Example Age)以建模内容新鲜度的影响</p>
<p>本质是把“用户最近看过的一堆视频”压缩成一个兴趣向量</p>
</li>
<li><p>物品塔结构较为简单，本质上是一个大规模可学习的嵌入矩阵，每个视频对应一个向量，从而避免了复杂的物品特征工程</p>
</li>
</ul>
<blockquote>
<p>YouTube 视频数量极大，召回阶段追求速度 + 覆盖，典型的“重用户、轻物品”设计</p>
</blockquote>
<p>该模型将“预测用户下一次观看的视频”建模为一个极端多分类问题，形式上类似于 NLP 中的 next-token 预测：<br>$$<br>P(w_t=i|U,C) = \frac{e^{v_i \cdot u}}{\sum_{j \in V} e^{v_j \cdot u}}<br>$$<br>这里$w_t$表示用户在时间$t$观看的视频，$U$是用户特征，$C$是上下文信息，$V$是整个视频库</p>
<p>由于视频库规模庞大，直接计算全量 Softmax 代价过高，训练阶段采用 <strong>Sampled Softmax</strong> (1 个正样本 + K 个负样本)进行近似与加速</p>
<p><strong>关键的工程技巧</strong></p>
<ul>
<li><p><strong>非对称的时序分割</strong>：不同于随机划分验证集的做法，YouTubeDNN 采用严格的时序分割策略</p>
<p>对每个预测目标，<font color="Violetred">仅使用其发生之前的用户行为作为输入特征</font>，从而避免未来信息泄露(类似Attention Mask)</p>
<p>基于时间回滚的样本构造方式更符合真实的推荐场景，也更贴合视频消费中存在的顺序性与不对称性</p>
<blockquote>
<p>虽然这里考虑了历史，但YouTubeDNN不属于序列召回，是因为在考虑历史特征时只是简单的pooling</p>
</blockquote>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/youtubednn_temporal_split.webp" alt="youtubednn_temporal_split" style="zoom: 33%;">
</li>
<li><p><strong>负采样策略</strong>：为了高效处理数百万类别的Softmax，模型采用重要性采样技术，每次只对数千个负样本进行计算，将训练速度提升并保持有效的区分能力</p>
</li>
<li><p><strong>用户样本均衡</strong>：为每个用户生成固定数量的训练样本，防止高活跃用户在训练中占据主导，从而提升对中低活跃用户与长尾兴趣的建模效果</p>
</li>
</ul>
<p>YouTubeDNN 的核心价值在于其工程化的系统设计：训练阶段使用复杂的多分类目标和丰富的用户特征，服务阶段通过离线预计算物品向量、在线生成用户向量，并结合高效的 ANN 检索完成召回，在训练复杂度与线上效率之间取得了良好平衡</p>
<p>这一范式至今仍是大规模召回系统的重要参考</p>
<p><strong>核心代码</strong></p>
<p>YouTubeDNN的用户塔设计体现了“非对称”的思想，它整合了多种用户特征和历史行为序列：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 整合用户特征和历史行为序列</span></span><br><span class="line">user_feature_embedding = concat_group_embedding(</span><br><span class="line">    group_embedding_feature_dict, <span class="string">"user_dnn"</span></span><br><span class="line">)  <span class="comment"># [B, N * D]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果有历史特征</span></span><br><span class="line"><span class="keyword">if</span> <span class="string">"raw_hist_seq"</span> <span class="keyword">in</span> group_embedding_feature_dict:</span><br><span class="line">    hist_seq_embedding = concat_group_embedding(</span><br><span class="line">        group_embedding_feature_dict, <span class="string">"raw_hist_seq"</span></span><br><span class="line">    )  <span class="comment"># [B, D] pooled history embedding</span></span><br><span class="line">    user_dnn_inputs = tf.keras.layers.Concatenate(axis=<span class="number">1</span>)(</span><br><span class="line">        [user_feature_embedding, hist_seq_embedding]</span><br><span class="line">    )  <span class="comment"># [B, N * D + D]</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    user_dnn_inputs = user_feature_embedding  <span class="comment"># [B, N * D]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建用户塔：输出归一化的用户向量</span></span><br><span class="line">user_dnn_output = DNNs(</span><br><span class="line">    units=dnn_units + [emb_dim], activation=<span class="string">"relu"</span>, use_bn=<span class="literal">False</span></span><br><span class="line">)(user_dnn_inputs)</span><br><span class="line">user_dnn_output = L2NormalizeLayer(axis=-<span class="number">1</span>)(user_dnn_output) <span class="comment"># [B, D]</span></span><br></pre></td></tr></tbody></table></figure>

<blockquote>
<p>这里不用BN是避免引入batch依赖，对 embedding 表示稳定性不友好</p>
</blockquote>
<p>物品塔则采用简化设计，直接使用物品Embedding表：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 物品Embedding表(从特征列配置中获取)</span></span><br><span class="line">item_embedding_table = embedding_table_dict[label_name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为评估构建物品模型</span></span><br><span class="line"><span class="comment"># input_layer_dict[label_name]: [B, 1]</span></span><br><span class="line"><span class="comment"># embedding lookup 后: [B, 1, D]</span></span><br><span class="line">output_item_embedding = SqueezeLayer(axis=<span class="number">1</span>)(</span><br><span class="line">    item_embedding_table(input_layer_dict[label_name])</span><br><span class="line">) <span class="comment"># [B, D]</span></span><br><span class="line">output_item_embedding = L2NormalizeLayer(axis=-<span class="number">1</span>)(output_item_embedding) <span class="comment"># [B, D]</span></span><br></pre></td></tr></tbody></table></figure>

<p>训练时采用Sampled Softmax优化，将百万级的多分类问题转化为高效的采样学习</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建采样softmax层</span></span><br><span class="line">sampled_softmax_layer = SampledSoftmaxLayer(</span><br><span class="line">    item_vocab_size,  <span class="comment"># 物品总数 |V|</span></span><br><span class="line">    neg_sample,       <span class="comment"># 每个样本采样的负例数</span></span><br><span class="line">    emb_dim           <span class="comment"># embedding 维度</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">output = sampled_softmax_layer([</span><br><span class="line">    item_embedding_table.embeddings, <span class="comment"># [V, emb_dim]</span></span><br><span class="line">    user_dnn_output,                 <span class="comment"># [B, emb_dim]</span></span><br><span class="line">    input_layer_dict[label_name]     <span class="comment"># [B, 1] 或 [B] 真实 ID</span></span><br><span class="line">])</span><br></pre></td></tr></tbody></table></figure>

<p>这种设计的核心优势在于：用户塔可以根据业务需求灵活扩展特征和模型复杂度，而物品塔保持简洁高效，易于离线预计算和实时检索</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>向量召回的核心贡献在于将推荐系统从“匹配计算”转变为“向量搜索”，通过将用户与物品映射到同一向量空间，把推荐问题统一为相似度搜索问题，从全量匹配优化为高效近邻检索，实现了从$O(U \times I)$到$O(U + I)$的复杂度优化</p>
<p><strong>I2I召回的演进脉络</strong>：I2I 召回源于 NLP 领域的 Word2Vec，并沿着“序列语义深化”的方向持续演进</p>
<ul>
<li>Item2Vec 将词序列建模思想迁移到用户行为序列，验证了序列共现在推荐中的有效性</li>
<li>EGES 通过引入商品关系图与辅助属性，缓解了数据稀疏与冷启动问题</li>
<li>Airbnb 则进一步将业务目标融入序列构建与负采样策略，实现了从相似性优化向转化目标优化的转变</li>
</ul>
<table>
<thead>
<tr>
<th>模型</th>
<th>核心思想</th>
<th>关键特点</th>
<th>主要价值</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Word2Vec</strong></td>
<td>通过上下文共现学习向量</td>
<td>Skip-gram / CBOW<br>局部上下文预测</td>
<td>“向量表示+相似度搜索”方法论起点</td>
</tr>
<tr>
<td><strong>Item2Vec</strong></td>
<td>用户行为序列 ≈ 词序列</td>
<td>仅建模物品共现关系<br>不依赖用户特征</td>
<td>验证序列建模在推荐中的可行性</td>
</tr>
<tr>
<td><strong>EGES</strong></td>
<td>图结构 + 多源信息融合</td>
<td>引入商品关系图与属性 embedding<br>缓解稀疏与冷启动</td>
<td>提升长尾与冷启动物品表示质量</td>
</tr>
<tr>
<td><strong>Airbnb I2I</strong></td>
<td>业务目标驱动的序列建模</td>
<td>会话切分<br>上下文感知<br>市场感知负采样</td>
<td>从“相似性召回”走向“转化目标召回”</td>
</tr>
</tbody></table>
<p><strong>U2I召回的技术路径</strong>：U2I 召回以双塔模型为核心架构，体现了可分解的系统设计思想</p>
<ul>
<li>FM 通过向量分解形式刻画用户–物品交互，为双塔结构提供了理论基础</li>
<li>DSSM 引入深度网络提升了表示能力，并确立了大规模多分类训练与工程化部署范式</li>
<li>YouTubeDNN 则通过 next-item 预测任务重构训练目标，并结合时序分割与采样优化，使双塔模型在超大规模推荐场景中具备可行性</li>
</ul>
<table>
<thead>
<tr>
<th>模型</th>
<th>核心思想</th>
<th>关键特点</th>
<th>主要价值</th>
</tr>
</thead>
<tbody><tr>
<td><strong>双塔模型</strong></td>
<td>映射到同一向量空间</td>
<td>用户塔 + 物品塔可分解<br>向量检索</td>
<td>高效候选生成，实时配合ANN</td>
</tr>
<tr>
<td><strong>FM</strong></td>
<td>用户–物品交互的向量分解</td>
<td>显式建模二阶特征交互<br>线性结构</td>
<td>奠定双塔模型的数学基础</td>
</tr>
<tr>
<td><strong>DSSM</strong></td>
<td>深度双塔相似度学习</td>
<td>DNN 替代线性映射<br>多分类 / softmax 训练</td>
<td>建立工业级双塔训练与部署范式</td>
</tr>
<tr>
<td><strong>YouTubeDNN</strong></td>
<td>next-item 预测</td>
<td>非对称双塔<br>时序分割<br>用户样本均衡</td>
<td>证明双塔模型可在超大规模场景落地</td>
</tr>
</tbody></table>
<h2 id="序列召回"><a href="#序列召回" class="headerlink" title="序列召回"></a>序列召回</h2><p><font color="Violetred">协同过滤和向量召回将用户的历史行为汇总成一个静态表示</font>(比如一个向量)，然后基于这个表示进行推荐</p>
<p>但是用户的行为其实是有时间顺序的，而且这个顺序往往包含了重要的信息，如果只是简单地把这些行为加起来或者平均，就丢失了这种时间顺序的信息</p>
<p><font color="DarkViolet">序列召回的核心思想在于显式利用用户行为的时间顺序进行建模</font>，相较于静态表示方法，序列召回能够更准确、全面地理解用户兴趣</p>
<p>两类具有代表性的方法：</p>
<ul>
<li><strong>多兴趣用户表示方法</strong>：通过多个向量或动态结构来更好地刻画用户的复杂兴趣模式，如 MIND 和 SDM</li>
<li><strong>生成式序列预测方法</strong>：将推荐问题视为序列建模任务，借鉴 NLP 领域的经验，引入 Transformer 等结构来建模序列依赖关系，如 SASRec 及其后续模型 HSTU 和 TIGER</li>
</ul>
<h3 id="深化用户兴趣表示"><a href="#深化用户兴趣表示" class="headerlink" title="深化用户兴趣表示"></a>深化用户兴趣表示</h3><p>传统的向量召回方法(如双塔模型)通常将用户的历史行为压缩为一个单一的静态向量</p>
<p>这种“平均化”的表示方式虽然计算高效，但难以刻画用户兴趣的复杂性</p>
<ol>
<li>无法表达用户同时存在的多样化兴趣</li>
<li>忽略了兴趣的时间特性，难以区分长期稳定的偏好与短期即时需求，例如对摄影的长期关注与临时搜索感冒药所反映的意图差异</li>
</ol>
<h4 id="MIND"><a href="#MIND" class="headerlink" title="MIND"></a>MIND</h4><p>MIND(Multi-Interest Network with Dynamic Routing)<u>(Li <em>et al.</em>, 2019)</u>借鉴了胶囊网络中的动态路由机制，将用户历史行为自动聚类为若干兴趣组，并为每一组生成对应的兴趣向量</p>
<p>这些向量分别代表用户在不同兴趣维度上的偏好，使推荐系统能够根据当前场景更有针对性地匹配物品，从而提升对多样化和动态兴趣的建模能力</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/mind_model.webp" alt="mind_model" style="zoom: 50%;">

<p>从整体架构来看，除了常规的Embedding层，MIND模型还包含了两个重要的组件：多兴趣提取层和Label-Aware注意力层</p>
<p><strong><font color="Violetred">多兴趣提取</font></strong></p>
<p>胶囊网络<u>(Sabour <em>et al.</em>, 2017)</u>最初提出于计算机视觉领域，其核心思想是用向量而非标量表示特征，<font color="Violetred">其中向量的方向编码属性信息，长度表示该特征存在的概率</font>。动态路由算法用于确定不同层级胶囊之间的连接强度，通过迭代更新实现对输入特征的软聚类</p>
<p>这种软聚类机制无需预先定义类别数量或明确的边界，而是由数据本身驱动分组过程，正好契合了用户兴趣发现的需求</p>
<p>MIND 模型将胶囊网络的思想引入推荐系统，提出了行为到兴趣(Behavior to Interest，B2I)的动态路由机制：将用户历史行为建模为行为胶囊，将用户的多重兴趣建模为兴趣胶囊，并通过动态路由将相关行为自动聚合到对应的兴趣表示中</p>
<p>针对推荐场景的特点，MIND 对原始动态路由算法进行了若干关键改进：</p>
<ol>
<li><p><strong>共享变换矩阵</strong>：与原始胶囊网络为每对胶囊使用独立变换矩阵不同，MIND采用共享的双线性映射矩阵$S \in \mathbb R^{d \times d}$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.bilinear_mapping_matrix = <span class="variable language_">self</span>.add_weight(</span><br><span class="line">    shape=[<span class="variable language_">self</span>.input_units, <span class="variable language_">self</span>.out_units], </span><br><span class="line">    initializer=tf.keras.initializers.RandomNormal(stddev=<span class="variable language_">self</span>.init_std), </span><br><span class="line">    name=<span class="string">"S"</span>, dtype=tf.float32)</span><br></pre></td></tr></tbody></table></figure>

<p>MIND训练的就是$S$矩阵</p>
<p><font color="DarkViolet">$S$ 是把“物品行为空间”映射到“兴趣聚合空间”的全局线性规则</font></p>
<p>这种设计有两个重要考虑</p>
<ul>
<li>用户行为序列长度变化很大，从几十到几百不等，共享矩阵确保了算法的通用性</li>
<li>共享变换保证所有兴趣向量位于同一表示空间，便于后续的相似度计算和检索操作</li>
</ul>
<p>行为与兴趣之间的路由连接强度定义为：<br>$$<br>b_{ij} = \boldsymbol u_j^T \boldsymbol S \boldsymbol e_i<br>$$<br>其中<font color="Violetred"> $\boldsymbol e_i$ 表示用户历史行为 $i$ 的物品向量</font>，$\boldsymbol u_j$ 表示第 $j$ 个兴趣胶囊的向量，$b_{ij}$ 衡量行为 $i$ 与兴趣 $j$ 的关联程度</p>
</li>
<li><p><strong>随机初始化策略</strong>：为避免所有兴趣胶囊收敛到相同状态，算法采用高斯分布随机初始化路由系数 $b_{ij}$</p>
<p>这一策略类似于K-Means聚类中的随机中心初始化，确保不同兴趣胶囊能够捕捉用户兴趣的不同方面</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.routing_logits = <span class="variable language_">self</span>.add_weight(</span><br><span class="line">    shape=[<span class="number">1</span>, <span class="variable language_">self</span>.k_max, <span class="variable language_">self</span>.max_len], <span class="comment"># batch维为1，因为对所有用户都一样</span></span><br><span class="line">    initializer=tf.keras.initializers.RandomNormal(stddev=<span class="variable language_">self</span>.init_std), <span class="comment"># 不一定0初始化</span></span><br><span class="line">    trainable=<span class="literal">False</span>, name=<span class="string">"B"</span>, dtype=tf.float32)</span><br><span class="line"><span class="comment"># routing_logits(b_ij)不参与梯度下降</span></span><br><span class="line"><span class="comment"># S 参与训练，在下一次前向传播计算中，b_ij会被重新计算成不一样的结果</span></span><br></pre></td></tr></tbody></table></figure>

<p>如果$b_{ij}$进入训练，结果会变成记忆行为序列中哪个位置的行为匹配哪个兴趣，和具体的行为内容无关了</p>
</li>
<li><p><strong>自适应兴趣数量</strong>：考虑到不同用户的兴趣复杂度差异很大，MIND引入了动态兴趣数量机制<br>$$<br>K_u’ = \max(1, \min(K, \log_2 (|\mathcal I_u|)))<br>$$<br>其中 $|\mathcal I_u|$ 表示用户 $u$ 的历史行为数量，$K$ 是预设的最大兴趣数</p>
<p>这种设计为行为较少的用户节省计算资源，同时为活跃用户提供更丰富的兴趣表示</p>
</li>
</ol>
<hr>
<p>改进后的动态路由过程通过迭代方式进行更新，在每轮迭代中更新路由系数 $b_{ij}$ 和兴趣胶囊向量 $\boldsymbol u_j$，直到收敛</p>
<p>关键的兴趣胶囊向量$\boldsymbol u_j$通过以下步骤计算：</p>
<ol>
<li><p><strong>计算路由权重</strong>：对于每一个历史行为(低层胶囊$i$)，其分配到各个兴趣(高层胶囊 $j$)的权重 $w_{ij}$ 通过对路由系数 $b_{ij}$ 进行Softmax操作得到<br>$$<br>w_{ij} = \frac{\exp{b_{ij}}}{\sum_{k=1}^{K_u’} \exp{b_{ik}}}<br>$$<br>这里的 $w_{ij}$ 表示行为 $i$ 属于兴趣 $j$ 的“软分配”概率</p>
</li>
<li><p><strong>聚合行为以形成兴趣向量</strong>：每一个兴趣胶囊的初步向量$\boldsymbol z_j$是通过对所有行为向量$\boldsymbol e_i$进行加权求和得到的，每个行为向量在求和前会先经过共享变换矩阵$\boldsymbol S$的转换<br>$$<br>\boldsymbol z_j = \sum_{i\in \mathcal I_u} w_{ij} \boldsymbol S \boldsymbol e_i<br>$$<br>该过程本质上实现了对用户行为的软聚类，将相关行为聚合为代表特定兴趣的向量</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">behavior_embddings = [B, max_len, d]  # 用户按时间排序的一串历史行为 embedding</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. 通过共享的双线性映射矩阵 S 把所有行为投影到兴趣空间</span></span><br><span class="line">behavior_embdding_mapping = tf.tensordot(</span><br><span class="line">    behavior_embddings, <span class="variable language_">self</span>.bilinear_mapping_matrix, axes=<span class="number">1</span></span><br><span class="line">)  <span class="comment"># [B, max_len, out_units] 对应Se_i</span></span><br></pre></td></tr></tbody></table></figure>

<p><font color="DarkViolet">不同的行为序列映射到兴趣空间的结果不一样，也就是从行为序列中建模用户状态</font></p>
</li>
<li><p><strong>非线性压缩</strong>：在完成行为聚合后，模型通过非线性的squash函数对中间向量$\boldsymbol z_j$进行压缩，得到本轮迭代的最终兴趣胶囊向量$\boldsymbol u_j$</p>
<p><font color="Violetred">squash函数</font>在不改变向量方向的前提下，将其模长约束到$[0, 1)$区间内，使向量长度可解释为兴趣存在的强度，而方向编码具体的兴趣语义<br>$$<br>\boldsymbol u_j = \text{squash}(\boldsymbol z_j) = \frac{\left\lVert \boldsymbol z_j \right\rVert ^ 2}{1 + \left\lVert \boldsymbol z_j \right\rVert ^ 2} \frac{\boldsymbol z_j}{\left\lVert \boldsymbol z_j \right\rVert}<br>$$</p>
</li>
<li><p><strong>更新路由系数</strong>(Updating Routing Logits)：最后根据新生成的兴趣胶囊$\boldsymbol u_j$和行为向量$\boldsymbol e_i$的一致性(通过点积衡量)，来更新下一轮迭代的路由系数<br>$$<br>b_{ij} \leftarrow b_{ij} + \boldsymbol u_j^T \boldsymbol S \boldsymbol e_i<br>$$</p>
</li>
</ol>
<p>以上四个步骤会重复进行固定的次数(通常为3次)，最终输出收敛后的兴趣胶囊向量集合${\boldsymbol u_j, j=1,…,K_{u}^\prime}$作为该用户的多兴趣表示</p>
<table>
<thead>
<tr>
<th>变量</th>
<th>物理意义</th>
</tr>
</thead>
<tbody><tr>
<td>$(e_i)$</td>
<td>原始行为的物品向量</td>
</tr>
<tr>
<td>$(S e_i)$</td>
<td>行为在兴趣空间的投影</td>
</tr>
<tr>
<td>$(u_j)$</td>
<td>当前估计的兴趣向量</td>
</tr>
<tr>
<td>($u_j^T S e_i$)</td>
<td>行为与兴趣的一致性</td>
</tr>
<tr>
<td>($b_{ij}$)</td>
<td>到目前为止，这种一致性累计了多少</td>
</tr>
<tr>
<td>($w_{ij}$)</td>
<td>当前轮的分配概率</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>变量</th>
<th>是否可训练</th>
<th>生命周期</th>
<th>物理意义</th>
</tr>
</thead>
<tbody><tr>
<td>$S$</td>
<td>✅</td>
<td>跨 step / epoch</td>
<td>行为→兴趣的判定规则</td>
</tr>
<tr>
<td>$b_{ij}$</td>
<td>❌</td>
<td>一次 forward</td>
<td>行为→兴趣的累计倾向</td>
</tr>
<tr>
<td>$u_j$</td>
<td>❌</td>
<td>一次 forward</td>
<td>当前用户的兴趣表示</td>
</tr>
</tbody></table>
<p><strong>核心代码</strong></p>
<p>MIND的核心在于胶囊网络的动态路由实现</p>
<p>在每次迭代中，模型首先通过softmax计算路由权重，然后通过双线性变换聚合行为向量，最后使用squash函数进行非线性压缩</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">capsule_mask[b, j, i] =</span><br><span class="line">    第 b 个用户</span><br><span class="line">    第 j 个兴趣是否存在</span><br><span class="line">    第 i 个行为</span><br><span class="line">    </span><br><span class="line">mask[b, j, i] = </span><br><span class="line">	第 b 个用户</span><br><span class="line">	第 j 个兴趣</span><br><span class="line">	第 i 个行为是不是 padding</span><br></pre></td></tr></tbody></table></figure>

<p>假设</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">k_max = 4 # 最大兴趣数</span><br><span class="line">capsule_num = 2 # 用户兴趣数</span><br><span class="line">max_len = 5 # 最大行为长度</span><br><span class="line">history_len = 3 # 用户行为长度</span><br></pre></td></tr></tbody></table></figure>

<p>有效区域</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">兴趣 0  [✔ ✔ ✔ ✘ ✘]</span><br><span class="line">兴趣 1  [✔ ✔ ✔ ✘ ✘]</span><br><span class="line">兴趣 2  [✘ ✘ ✘ ✘ ✘]</span><br><span class="line">兴趣 3  [✘ ✘ ✘ ✘ ✘]</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动态路由的核心循环</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.iteration_times):  <span class="comment"># 通常迭代3次</span></span><br><span class="line">    <span class="comment"># self.routing_logits = [1, k_max, max_len]</span></span><br><span class="line">    mask_routing_logits = tf.where( </span><br><span class="line">        capsule_mask, <span class="comment"># 胶囊数量 mask</span></span><br><span class="line">        tf.tile(<span class="variable language_">self</span>.routing_logits, [batch_size, <span class="number">1</span>, <span class="number">1</span>]),</span><br><span class="line">        capsule_padding) <span class="comment"># [B, k_max, max_len]</span></span><br><span class="line">    </span><br><span class="line">    pad = tf.ones_like(mask, dtype=tf.float32) * (-<span class="number">2</span> ** <span class="number">32</span> + <span class="number">1</span>)  </span><br><span class="line">    routing_logits_with_padding = tf.where(</span><br><span class="line">        mask, <span class="comment"># 行为序列 mask</span></span><br><span class="line">        mask_routing_logits,</span><br><span class="line">        pad)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 1. 计算路由权重 w_ij</span></span><br><span class="line">    <span class="comment"># 行为i分配到不同兴趣j的比例，一开始是随便分的</span></span><br><span class="line">    weight = tf.nn.softmax(routing_logits_with_padding)  <span class="comment"># [B, k_max, max_len]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 通过共享的双线性映射矩阵 S 把所有行为投影到兴趣空间</span></span><br><span class="line">    behavior_embdding_mapping = tf.tensordot(</span><br><span class="line">        behavior_embddings, <span class="variable language_">self</span>.bilinear_mapping_matrix, axes=<span class="number">1</span></span><br><span class="line">    )  <span class="comment"># [B, max_len, out_units] 对应Se_i</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 加权聚合形成兴趣胶囊</span></span><br><span class="line">    Z = tf.matmul(weight, behavior_embdding_mapping)  <span class="comment"># [B, k_max, out_units]</span></span><br><span class="line">    interest_capsules = squash(Z)  <span class="comment"># 非线性压缩到 [0, 1) 对应u_j</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. 更新路由系数：基于兴趣胶囊与行为的一致性</span></span><br><span class="line">    delta_routing_logits = tf.reduce_sum(</span><br><span class="line">        tf.matmul(</span><br><span class="line">            interest_capsules,</span><br><span class="line">            tf.transpose(behavior_embdding_mapping, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">        ), <span class="comment"># [B, k_max, max_len]</span></span><br><span class="line">        axis=<span class="number">0</span>, keepdims=<span class="literal">True</span></span><br><span class="line">    ) <span class="comment"># [1, k_max, max_len]</span></span><br><span class="line">    </span><br><span class="line">    <span class="variable language_">self</span>.routing_logits.assign_add(delta_routing_logits) <span class="comment"># b_ij更新</span></span><br></pre></td></tr></tbody></table></figure>

<p>这里的squash函数实现了向量长度的非线性压缩，确保输出向量的模长在$[0, 1)$区间内：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squash</span>(<span class="params">inputs</span>):</span><br><span class="line">    <span class="string">"""非线性压缩函数，将向量长度映射到 [0, 1) 区间"""</span></span><br><span class="line">    vec_squared_norm = tf.reduce_sum(tf.square(inputs), axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    scalar_factor = vec_squared_norm / (<span class="number">1</span> + vec_squared_norm) / tf.sqrt(vec_squared_norm + <span class="number">1e-9</span>)</span><br><span class="line">    <span class="keyword">return</span> scalar_factor * inputs</span><br></pre></td></tr></tbody></table></figure>

<p><strong><font color="Violetred">标签感知的注意力机制</font></strong></p>
<p>多兴趣提取层会为用户生成多个兴趣向量${u_1,u_2,\cdots,u_K  }$，刻画用户在不同行为模式下的潜在兴趣</p>
<p>在训练阶段需要确定哪个兴趣向量与当前目标商品最相关</p>
<p>由于训练时已知用户真实点击的下一个商品，MIND 利用该标签信息，引入<font color="Violetred">标签感知注意力(Label-aware Attention)</font>，指导模型在多个兴趣向量中选择最匹配的一项</p>
<p><font color="DarkViolet">标签感知注意力以目标商品向量作为查询(Query)，以用户的多个兴趣向量作为键和值(Key/Value)</font>，计算过程为<br>$$<br>v_u = V_u \cdot \text{Softmax}(\text{pow}(V_u^T e_i, p))<br>$$<br>其中</p>
<ul>
<li><p>$V_u = (u_1, \ldots, u_K)$表示用户的兴趣胶囊矩阵</p>
</li>
<li><p>$e_i$是目标商品 $i$ 的embedding向量</p>
</li>
<li><p>$p$是控制注意力集中度的超参数，决定注意力的“软硬”程度</p>
<p>$p\rightarrow 0$：各兴趣向量权重接近均匀；</p>
<p>$p$ 增大：注意力逐渐集中到与目标商品最相似的兴趣</p>
<p>$p\rightarrow \infty$：退化为硬选择，只保留相似度最高的兴趣向量</p>
<p>实验表明，使用较大的 $p$ 值能够加快模型收敛速度</p>
</li>
</ul>
<p>标签感知注意力直接在由共享映射矩阵$S$和动态路由共同确定的兴趣空间中，利用目标商品embedding和多兴趣表示${ u_j}$进行匹配和选择，通过标签感知得到当前训练样本对应的用户向量$v_u$</p>
<p>MIND模型的训练目标是：最大化$v_u$与正样本商品的相似度，同时最小化与负样本的相似度</p>
<p>由于商品规模巨大，MIND 采用与 YouTubeDNN 相同的 <strong>Sampled Softmax</strong>，通过采样少量负样本近似完整 softmax 计算，从而降低训练成本</p>
<p>核心是使用目标商品向量作为查询，计算与各个兴趣向量的相似度：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">    keys = inputs[<span class="number">0</span>]  <span class="comment"># 多个兴趣胶囊向量 [batch_size, k_max, dim] u_j (interest_capsules)</span></span><br><span class="line">    query = inputs[<span class="number">1</span>]  <span class="comment"># 目标商品向量 [batch_size, dim]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算每个兴趣向量与目标商品的相似度</span></span><br><span class="line">    <span class="comment"># 广播点积，把dim维消去</span></span><br><span class="line">    weight = tf.reduce_sum(keys * query, axis=-<span class="number">1</span>, keepdims=<span class="literal">True</span>)  <span class="comment"># [batch_size, k_max, 1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过幂次运算控制注意力集中度</span></span><br><span class="line">    weight = tf.<span class="built_in">pow</span>(weight, <span class="variable language_">self</span>.pow_p)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果 pow_p 很大(&gt;= 100)，直接选择最相似的兴趣</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.pow_p &gt;= <span class="number">100</span>:</span><br><span class="line">        idx = tf.argmax(weight, axis=<span class="number">1</span>, output_type=tf.int32)</span><br><span class="line">        output = tf.gather_nd(keys, idx)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 否则使用 softmax 进行加权聚合</span></span><br><span class="line">        weight = tf.nn.softmax(weight, axis=<span class="number">1</span>)</span><br><span class="line">        output = tf.reduce_sum(keys * weight, axis=<span class="number">1</span>)  <span class="comment"># [batch_size, dim]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></tbody></table></figure>

<h4 id="SDM"><a href="#SDM" class="headerlink" title="SDM"></a>SDM</h4><p>MIND 通过多兴趣建模有效缓解了用户兴趣过于单一的问题，能够刻画用户在不同主题下的潜在兴趣结构</p>
<p><font color="DarkViolet">虽然MIND能捕捉多个兴趣，但并未在结构上显式地区分它们的时效性</font>，对历史行为的建模未引入明确的时间权重</p>
<p>序列深度匹配模型(Sequential Deep Matching，SDM)<u>(Lv <em>et al.</em>, 2019)</u> 正是为了解决这一问题而提出的</p>
<p>SDM模型的核心思想是分别建模用户的短期即时兴趣和长期稳定偏好，然后智能地融合它们</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/sdm_model_architecture (1).webp" alt="sdm_model_architecture (1)" style="zoom:80%;">

<p><strong><font color="Violetred">捕捉短期兴趣</font></strong></p>
<p>为了精准捕捉短期兴趣，SDM 设计了一套三层结构来处理用户的当前会话序列**(左下角)**</p>
<p>首先，将用户在当前会话中的商品点击序列输入 LSTM 网络，以建模行为之间的时序依赖关系</p>
<blockquote>
<p>为什么 SDM 利用 LSTM + Self-Attention，而不是直接用 Transformer？</p>
<p>Transformer更擅长长序列，全局建模以及大规模数据，而对于搜推场景，会话序列其实通常很短，噪声点击很常见</p>
<p>LSTM对随机点击更鲁棒，并且LSTM + Attention参数更少，相比直接使用 Transformer 更适合短会话、高噪声的推荐场景</p>
</blockquote>
<p>LSTM 通过遗忘门Forget Gate、输入门Input Gate和输出门Output Gate对信息进行选择性保留与更新，从而在序列建模过程中抑制随机点击等噪声行为，更好地提取与当前意图相关的有效信息<br>$$<br>\begin{aligned}<br>\boldsymbol i \boldsymbol n_{t}^{u} &amp;=\sigma\left(\boldsymbol W_{i n}^{1} \boldsymbol e_{i_{t}^{u}}+\boldsymbol W_{i n}^{2} \boldsymbol h_{t-1}^{u}+b_{i n}\right)<br>\\<br>\boldsymbol f_{t}^{u} &amp;=\sigma\left(\boldsymbol W_{f}^{1} \boldsymbol e_{i_{t}^{u}}+\boldsymbol W_{f}^{2} \boldsymbol h_{t-1}^{u}+b_{f}\right) \\<br>\boldsymbol o_{t}^{u} &amp;=\sigma\left(\boldsymbol W_{o}^{1} \boldsymbol e_{i_{t}^{u}}+\boldsymbol W_{o}^{2} \boldsymbol h_{t-1}^{u}+b_{o}\right) \\<br>\boldsymbol c_{t}^{u} &amp;=\boldsymbol f_{t}^{u} \boldsymbol c_{t-1}^{u}+\boldsymbol i \boldsymbol n_{t}^{u} \tanh \left(\boldsymbol W_{c}^{1} \boldsymbol e_{i_{t}^{u}}+\boldsymbol W_{c}^{2} \boldsymbol h_{t-1}^{u}+b_{c}\right) \\<br>\boldsymbol h_{t}^{u} &amp;=\boldsymbol o_{t}^{u} \tanh \left(\boldsymbol c_{t}^{u}\right)<br>\end{aligned}<br>$$</p>
<ul>
<li>$\boldsymbol e_{i_{t}^{u}}$：第$t$个时间步的商品embedding</li>
<li>$\sigma$：sigmoid激活函数</li>
<li>$\boldsymbol W$：权重矩阵，$b$表示偏置向量</li>
</ul>
<p>每个时间步都输出隐藏状态$\boldsymbol h_{t}^{u} \in \mathbb R^{d \times 1}$，最终得到会话序列的表示<br>$$<br>\boldsymbol X^{u} = [\boldsymbol h_{1}^{u}, \ldots, \boldsymbol h_{t}^{u}]<br>$$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 序列信息学习层：使用LSTM处理序列依赖</span></span><br><span class="line">lstm_layer = tf.keras.layers.LSTM(</span><br><span class="line">    emb_dim,</span><br><span class="line">    return_sequences=<span class="literal">True</span>,  <span class="comment"># 返回所有时间步的输出</span></span><br><span class="line">    recurrent_initializer=<span class="string">'glorot_uniform'</span></span><br><span class="line">)</span><br><span class="line">sequence_output = lstm_layer(short_history_item_emb)  <span class="comment"># [batch_size, seq_len, dim]</span></span><br></pre></td></tr></tbody></table></figure>

<p>在获得序列表示后，SDM 引入多头自注意力机制以刻画短期兴趣的多样性</p>
<p>通过对序列表示$\boldsymbol X^{u}$进行多组线性映射，模型在不同注意力头中并行关注序列的不同兴趣模式，每个注意力头独立计算序列内部的相关性(<font color="Violetred">这里是自注意力</font>)，并生成对应的兴趣表示</p>
<blockquote>
<p>不需要位置编码是因为LSTM的隐藏状态里已经包含了时序信息</p>
</blockquote>
<p>$$<br>head_{i}^{u}=\operatorname{Attention}\left(\boldsymbol W_{i}^{Q} \boldsymbol X^{u}, \boldsymbol W_{i}^{K} \boldsymbol X^{u}, \boldsymbol W_{i}^{V} \boldsymbol X^{u}\right) = \operatorname{Attention}(Q_i^u,K_i^u,V_i^u)\\<br>\operatorname{Attention}(Q,K,V) = \operatorname{softmax}(QK^T)V<br>$$</p>
<blockquote>
<p>这里和transformer的注意力机制的区别只在于没有$\sqrt d_k$的分母</p>
<p>因为NLP中的embedding维度通常512/1024，维度大，容易出现数值爆炸，需要除$\sqrt d_k$防止梯度爆炸；推荐系统的embedding维度通常在32/64/128，点积数值可控</p>
</blockquote>
<p>其中$Q_{i}^{u}$、$K_{i}^{u}$、$V_{i}^{u}$分别表示第$i$个头的查询、键、值矩阵，$\boldsymbol W_{i}^{Q}$、$\boldsymbol W_{i}^{K}$、$\boldsymbol W_{i}^{V}$是对应的权重矩阵</p>
<p>所有注意力头的输出经过拼接和线性变换后，得到融合多种兴趣信息的序列表示<br>$$<br>\hat X^{u}=\text{MultiHead}\left(X^{u}\right)=W^{O} \operatorname{concat}\left(head_{1}^{u}, \ldots, head_{h}^{u}\right)<br>$$<br>$W^{O}$是输出权重矩阵</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 2. 多兴趣提取层：多头自注意力捕捉序列内的复杂关系</span></span><br><span class="line">norm_sequence_output = tf.keras.layers.LayerNormalization()(sequence_output)</span><br><span class="line">sequence_output = tf.keras.layers.MultiHeadAttention(</span><br><span class="line">    num_heads=num_heads,</span><br><span class="line">    key_dim=emb_dim // num_heads,</span><br><span class="line">    dropout=<span class="number">0.1</span></span><br><span class="line">)(norm_sequence_output, sequence_output)  <span class="comment"># [batch_size, seq_len, dim]</span></span><br><span class="line"></span><br><span class="line">short_term_output = tf.keras.layers.LayerNormalization()(sequence_output)</span><br></pre></td></tr></tbody></table></figure>

<p>在此基础上 SDM 进一步引入个性化注意力层，利用用户画像向量$\boldsymbol e_u$作为查询(<font color="Violetred">这里不是自注意力</font>)，对多头注意力输出进行加权，从而突出与用户长期偏好更一致的会话行为<br>$$<br>\begin{aligned}<br>\alpha_{k} &amp;=\frac{\exp\left(\hat{\boldsymbol h_{k}^{u T}} \boldsymbol e_{u}\right)}{\sum_{k=1}^{t} \exp\left(\hat{\boldsymbol h_{k}^{u T}} \boldsymbol e_{u}\right)} \\<br>\boldsymbol s_{t}^{u} &amp;=\sum_{k=1}^{t} \alpha_{k} \hat{\boldsymbol h_{k}^{u}}<br>\end{aligned}<br>$$<br>$\hat{\boldsymbol h_{k}^{u}}$是多头注意力输出$\hat{X}^{u}$中第$k$个位置的隐藏状态，$\alpha_{k}$是对应的注意力权重</p>
<p>通过该机制，模型能够在短期会话兴趣的基础上注入用户的个性化信息，最终得到融合长期偏好与短期意图的短期兴趣表示$\boldsymbol{s}_{t}^{u} \in \mathbb{R}^{d \times 1}$</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3. 用户个性化注意力层：使用用户画像作为查询向量</span></span><br><span class="line">user_attention = UserAttention(name=<span class="string">'user_attention_short'</span>)</span><br><span class="line">short_term_interest = user_attention(</span><br><span class="line">    user_embedding,  <span class="comment"># [batch_size, 1, dim] 用户画像作为查询</span></span><br><span class="line">    short_term_output  <span class="comment"># [batch_size, seq_len, dim] 序列作为键和值</span></span><br><span class="line">)  <span class="comment"># [batch_size, 1, dim]</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>核心代码</strong></p>
<p>SDM的短期兴趣建模采用了三层架构，逐步从原始序列中提取用户的即时兴趣</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 序列信息学习层：使用LSTM处理序列依赖</span></span><br><span class="line">lstm_layer = tf.keras.layers.LSTM(</span><br><span class="line">    emb_dim,</span><br><span class="line">    return_sequences=<span class="literal">True</span>,  <span class="comment"># 返回所有时间步的输出</span></span><br><span class="line">    recurrent_initializer=<span class="string">'glorot_uniform'</span></span><br><span class="line">)</span><br><span class="line">sequence_output = lstm_layer(short_history_item_emb)  <span class="comment"># [batch_size, seq_len, dim]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 多兴趣提取层：多头自注意力捕捉序列内的复杂关系</span></span><br><span class="line">norm_sequence_output = tf.keras.layers.LayerNormalization()(sequence_output)</span><br><span class="line">sequence_output = tf.keras.layers.MultiHeadAttention(</span><br><span class="line">    num_heads=num_heads,</span><br><span class="line">    key_dim=emb_dim // num_heads,</span><br><span class="line">    dropout=<span class="number">0.1</span></span><br><span class="line">)(norm_sequence_output, sequence_output)  <span class="comment"># [batch_size, seq_len, dim]</span></span><br><span class="line"></span><br><span class="line">short_term_output = tf.keras.layers.LayerNormalization()(sequence_output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 用户个性化注意力层：使用用户画像作为查询向量</span></span><br><span class="line">user_attention = UserAttention(name=<span class="string">'user_attention_short'</span>)</span><br><span class="line">short_term_interest = user_attention(</span><br><span class="line">    user_embedding,  <span class="comment"># [batch_size, 1, dim] 用户画像作为查询</span></span><br><span class="line">    short_term_output  <span class="comment"># [batch_size, seq_len, dim] 序列作为键和值</span></span><br><span class="line">)  <span class="comment"># [batch_size, 1, dim]</span></span><br></pre></td></tr></tbody></table></figure>

<p>个性化注意力层的实现通过用户画像与序列特征的点积来计算注意力权重：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UserAttention</span>(tf.keras.layers.Layer):</span><br><span class="line">    <span class="string">"""用户注意力层，使用用户基础表示作为查询向量"""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, query_vector, key_vectors</span>):</span><br><span class="line">        <span class="comment"># 计算注意力分数：query · key^T</span></span><br><span class="line">        attention_scores = tf.matmul(</span><br><span class="line">            query_vector,  <span class="comment"># [batch_size, 1, dim]</span></span><br><span class="line">            tf.transpose(key_vectors, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])  <span class="comment"># [batch_size, dim, seq_len]</span></span><br><span class="line">        )  <span class="comment"># [batch_size, 1, seq_len]</span></span><br><span class="line"></span><br><span class="line">        attention_scores = tf.squeeze(attention_scores, axis=<span class="number">1</span>) <span class="comment"># [batch_size, seq_len]</span></span><br><span class="line">        attention_weights = tf.nn.softmax(attention_scores, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加权求和得到上下文向量</span></span><br><span class="line">        context_vector = tf.matmul(</span><br><span class="line">            tf.expand_dims(attention_weights, axis=<span class="number">1</span>),</span><br><span class="line">            key_vectors <span class="comment"># # [batch_size, seq_len, dim]</span></span><br><span class="line">        )  <span class="comment"># [batch_size, 1, dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> context_vector</span><br></pre></td></tr></tbody></table></figure>

<p><strong><font color="Violetred">捕捉长期兴趣</font></strong></p>
<p>长期行为蕴含着稳定而丰富的偏好信息，但这类偏好并不依赖严格的时间顺序，更适合从特征维度进行建模</p>
<p>SDM 不再将长期行为视为单一序列，而是按照不同特征类型对历史行为进行拆分，形成多个特征子集**(左上角)**：<br>$$<br>\mathcal L^{u}={\mathcal L_{f}^{u} \mid f \in \mathcal F}<br>$$<br>包括：</p>
<ul>
<li>交互过的商品ID集合 $\mathcal{L}^{u}_{id}$</li>
<li>叶子类别集合 $\mathcal{L}^{u}_{leaf}$</li>
<li>一级类别集合 $\mathcal{L}^{u}_{cate}$</li>
<li>访问过的商店集合 $\mathcal{L}^{u}_{shop}$</li>
<li>交互过的品牌集合 $\mathcal{L}^{u}_{brand}$</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从不同特征维度对长期行为进行聚合</span></span><br><span class="line">long_history_features = group_embedding_feature_dict[<span class="string">'raw_hist_seq_long'</span>]</span><br></pre></td></tr></tbody></table></figure>

<p><code>long_history_features</code> 是一个 <strong>dict</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  "item_id":   [B, max_len_long, dim],</span><br><span class="line">  "brand":     [B, max_len_long, dim],</span><br><span class="line">  "cate":      [B, max_len_long, dim],</span><br><span class="line">  "shop":      [B, max_len_long, dim],</span><br><span class="line">  ...</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p>这种特征维度的分离使模型能够从不同角度理解用户的长期偏好模式</p>
<p>对每个特征子集，模型使用注意力机制计算用户在该维度上的偏好</p>
<p>将特征实体$f^u_k \in \mathcal L^u_f$通过嵌入矩阵转换为向量$\boldsymbol g^u_k$，然后使用用户画像$\boldsymbol e_u$计算注意力权重<br>$$<br>\begin{aligned}<br>\alpha_{k} &amp;=\frac{\exp (\boldsymbol g_{k}^{u T} \boldsymbol e_{u})}{\sum_{k=1}^{|\mathcal L_{f}^{u}|} \exp (\boldsymbol g_{k}^{u T} \boldsymbol e_{u})} \\<br>\boldsymbol z_{f}^{u} &amp;=\sum_{k=1}^{|\mathcal L_{f}^{u}|} \alpha_{k} \boldsymbol g_{k}^{u}<br>\end{aligned}<br>$$<br>其中$\left|\mathcal L_{f}^{u}\right|$表示特征子集的大小</p>
<p>最终将各特征维度的表示拼接，通过全连接网络得到长期兴趣表示<br>$$<br>\begin{aligned}<br>\boldsymbol z^{u} &amp;=\operatorname{concat}({\boldsymbol z_{f}^{u} \mid f \in \mathcal F}) \\<br>\boldsymbol p^{u} &amp;=\tanh (\boldsymbol W^{p} \boldsymbol z^{u}+\boldsymbol b)<br>\end{aligned}<br>$$<br><strong>核心代码</strong></p>
<p>长期兴趣的建模采用特征维度聚合的方式，对每个特征维度分别应用注意力机制</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从不同特征维度对长期行为进行聚合</span></span><br><span class="line">long_history_features = group_embedding_feature_dict[<span class="string">'raw_hist_seq_long'</span>] </span><br><span class="line"><span class="comment"># [batch_size, max_len_long, dim]</span></span><br><span class="line"></span><br><span class="line">long_term_interests = []</span><br><span class="line"><span class="keyword">for</span> name, long_history_feature <span class="keyword">in</span> long_history_features.items():</span><br><span class="line">    <span class="comment"># 为每个特征维度生成 mask，找到padding embedding</span></span><br><span class="line">    long_history_mask = tf.keras.layers.Lambda(</span><br><span class="line">        <span class="keyword">lambda</span> x: tf.expand_dims(</span><br><span class="line">            tf.cast(tf.not_equal(x, <span class="number">0</span>), dtype=tf.float32), axis=-<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">    )(input_layer_dict[name])  <span class="comment"># [batch_size, max_len_long, 1]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 应用 mask 到特征嵌入</span></span><br><span class="line">    long_history_item_emb = tf.keras.layers.Lambda(<span class="keyword">lambda</span> x: x[<span class="number">0</span>] * x[<span class="number">1</span>])(</span><br><span class="line">        [long_history_feature, long_history_mask]</span><br><span class="line">    )  <span class="comment"># [batch_size, max_len_long, dim]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对每个特征维度应用用户注意力</span></span><br><span class="line">    user_attention = UserAttention(name=<span class="string">f'user_attention_long_<span class="subst">{name}</span>'</span>)</span><br><span class="line">    long_term_interests.append(</span><br><span class="line">        user_attention(</span><br><span class="line">            user_embedding,   <span class="comment"># [batch_size, 1, dim] 用户画像作为查询</span></span><br><span class="line">            long_history_item_emb</span><br><span class="line">        )</span><br><span class="line">    )  <span class="comment"># [batch_size, 1, dim]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拼接所有特征维度的表示</span></span><br><span class="line">long_term_interests_concat = tf.keras.layers.Concatenate(axis=-<span class="number">1</span>)(</span><br><span class="line">    long_term_interests</span><br><span class="line">)  <span class="comment"># [batch_size, 1, dim * len(long_history_features)]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过全连接层融合</span></span><br><span class="line">long_term_interest = tf.keras.layers.Dense(emb_dim, activation=<span class="string">'tanh'</span>)(</span><br><span class="line">    long_term_interests_concat</span><br><span class="line">)  <span class="comment"># [batch_size, 1, dim]</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong><font color="Violetred">长短期兴趣融合</font></strong></p>
<p>用户的长期兴趣包含大量稳定偏好信息，但在具体推荐场景中，只有其中一部分与当前决策相关，简单的拼接或加权求和难以准确提取相关信息</p>
<p>SDM设计了<font color="DarkViolet">门控融合机制</font>，类似LSTM中的门控思想(<strong>中间部分</strong>)，对长期兴趣和短期兴趣进行自适应融合</p>
<p>门控网络接收三个输入：用户画像$\boldsymbol e_{u}$、短期兴趣$\boldsymbol s_{t}^{u}$和长期兴趣$\boldsymbol p^{u}$，生成门控向量$\boldsymbol G_{t}^{u} \in \mathbb R^{d \times 1}$，取值范围在[0,1]<br>$$<br>\boldsymbol G_{t}^{u} = \operatorname{sigmoid}(\boldsymbol W^{1} \boldsymbol e_{u}+\boldsymbol W^{2} \boldsymbol s_{t}^{u}+\boldsymbol W^{3} \boldsymbol p^{u}+\boldsymbol b)<br>$$<br>门控向量的每一维表示在对应兴趣维度上，模型更应依赖短期兴趣还是长期偏好</p>
<p>最终的用户表示由门控向量对短期兴趣$\boldsymbol s_{t}^{u}$和长期兴趣$\boldsymbol p^{u}$进行逐维加权获得($\odot$表示逐元素乘法)：<br>$$<br>\boldsymbol o_{t}^{u} = (1-\boldsymbol G_{t}^{u} ) \odot \boldsymbol p^{u}+\boldsymbol G_{t}^{u} \odot \boldsymbol s_{t}^{u}<br>$$<br>通过这种逐维的门控融合方式，模型能够在不同兴趣维度上灵活选择长期或短期信息，既保留用户稳定偏好，又突出当前行为意图，从而避免简单融合策略带来的信息干扰，更准确地刻画用户的真实需求</p>
<p><strong>核心代码</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GatedFusion</span>(tf.keras.layers.Layer):</span><br><span class="line">    <span class="string">"""门控融合层，用于融合长期和短期兴趣"""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        dim = input_shape[<span class="number">0</span>][-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 为用户画像、短期兴趣、长期兴趣分别学习权重矩阵</span></span><br><span class="line">        <span class="variable language_">self</span>.W1 = <span class="variable language_">self</span>.add_weight(</span><br><span class="line">            shape=(dim, dim),</span><br><span class="line">            initializer=<span class="string">"glorot_uniform"</span>,</span><br><span class="line">            trainable=<span class="literal">True</span>,</span><br><span class="line">            name=<span class="string">"W1"</span></span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.W2 = <span class="variable language_">self</span>.add_weight(</span><br><span class="line">            shape=(dim, dim),</span><br><span class="line">            initializer=<span class="string">"glorot_uniform"</span>,</span><br><span class="line">            trainable=<span class="literal">True</span>,</span><br><span class="line">            name=<span class="string">"W2"</span></span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.W3 = <span class="variable language_">self</span>.add_weight(</span><br><span class="line">            shape=(dim, dim),</span><br><span class="line">            initializer=<span class="string">"glorot_uniform"</span>,</span><br><span class="line">            trainable=<span class="literal">True</span>,</span><br><span class="line">            name=<span class="string">"W3"</span></span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.b = <span class="variable language_">self</span>.add_weight(</span><br><span class="line">            shape=(dim,),</span><br><span class="line">            initializer=<span class="string">"zeros"</span>,</span><br><span class="line">            trainable=<span class="literal">True</span>,</span><br><span class="line">            name=<span class="string">"bias"</span></span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">super</span>(GatedFusion, <span class="variable language_">self</span>).build(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        user_embedding, short_term, long_term = inputs</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算门控向量：G = sigmoid(W1·e_u + W2·s_t + W3·p_u + b)</span></span><br><span class="line">        gate = tf.sigmoid(</span><br><span class="line">            tf.matmul(user_embedding, <span class="variable language_">self</span>.W1) +</span><br><span class="line">            tf.matmul(short_term, <span class="variable language_">self</span>.W2) +</span><br><span class="line">            tf.matmul(long_term, <span class="variable language_">self</span>.W3) +</span><br><span class="line">            <span class="variable language_">self</span>.b</span><br><span class="line">        )  <span class="comment"># [batch_size, 1, dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 门控融合：o_t = (1 - G) ⊙ p_u + G ⊙ s_t</span></span><br><span class="line">        output = (<span class="number">1</span> - gate) * long_term + gate * short_term</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></tbody></table></figure>

<p>整个SDM模型的最终实现将三个模块串联起来：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 短期兴趣建模</span></span><br><span class="line">short_term_interest = build_short_term_interest(</span><br><span class="line">    short_history_item_emb, user_embedding</span><br><span class="line">)  <span class="comment"># [batch_size, 1, dim]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 长期兴趣建模</span></span><br><span class="line">long_term_interest = build_long_term_interest(</span><br><span class="line">    long_history_features, user_embedding</span><br><span class="line">)  <span class="comment"># [batch_size, 1, dim]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 门控融合</span></span><br><span class="line">gated_fusion = GatedFusion(name=<span class="string">'gated_fusion'</span>)</span><br><span class="line">final_interest = gated_fusion(</span><br><span class="line">    [user_embedding, short_term_interest, long_term_interest]</span><br><span class="line">)  <span class="comment"># [batch_size, 1, dim]</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="生成式召回方法"><a href="#生成式召回方法" class="headerlink" title="生成式召回方法"></a>生成式召回方法</h3><p>序列召回方法的核心目标是分析用户的历史行为，并将其总结成一个或多个能够代表用户兴趣的向量，好比是为用户拍摄一张静态的“兴趣快照”，用以在海量物品中进行检索</p>
<p><strong>生成式召回</strong>不再总结用户兴趣，而是将推荐问题建模为<font color="DarkViolet">序列预测任务</font>，直接学习物品之间的动态依赖关系，并预测用户在当前状态下最可能交互的下一个物品</p>
<p><font color="Violetred">SASRec</font>是生成式召回的代表性起点，它首次将 Transformer 架构引入推荐序列建模，通过<font color="Violetred">自注意力机制</font>对用户行为序列进行建模，并以“预测下一个物品 ID”为目标，验证了该范式在召回任务中的有效性</p>
<p>SASRec后续研究主要沿着两个方向进一步深化这一范式：</p>
<ol>
<li><strong>增强输入的信息</strong>：<font color="Violetred">HSTU</font>将用户属性、行为类型、时间等多源异构信息统一建模为连续的“事件流”，为模型提供更加丰富的上下文信息</li>
<li><strong>增强物品的信息</strong>：传统推荐系统中，物品通常用简单的ID来表示，这种方式虽然直观，但缺乏语义信息。<font color="Violetred">TIGER</font> 摒弃了单一物品 ID 表示方式，引入由多个码本组成的“语义 ID”，使物品表示具备更强的语义表达能力，同时适用于序列输入和预测目标</li>
</ol>
<h4 id="SASRec"><a href="#SASRec" class="headerlink" title="SASRec"></a>SASRec</h4><p>在SASRec出现之前，序列推荐方法主要依赖马尔可夫链和 RNN 等模型，但存在明显缺陷</p>
<ul>
<li>马尔可夫链 <u>(Rendle <em>et al.</em>, 2010)</u> 通常只建模最近的少量行为，难以利用完整的历史信息；</li>
<li>RNN虽然理论上能够捕捉长期依赖关系，但其串行计算方式限制了训练效率和并行能力</li>
</ul>
<p>SASRec <u>(Kang and McAuley, 2018)</u> 的核心出发点在于：在保留完整行为序列信息的同时，高效地建模其中最关键的依赖关系，引入了在自然语言处理领域已被充分验证的 Transformer 架构<u>(Vaswani <em>et al.</em>, 2017)</u></p>
<p>在 SASRec 中，用户行为序列被视为一段“句子”，序列中的物品对应“词语”，模型通过自注意力机制自动学习序列中任意两个物品之间的关联强度，并基于这些全局依赖关系预测用户下一步可能交互的物品</p>
<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2026-01-19_22-38-59.webp" alt="Snipaste_2026-01-19_22-38-59"></p>
<p>类似于Transformer，SASRec的基本模块如图左所示，主要包含自注意力层和前馈网络层两个组件</p>
<p><strong>自注意力层</strong></p>
<p>对于序列中的每个商品，通过嵌入矩阵 $\bf M\in\mathbb R^{|\mathcal I|\times d}$ 将其映射为d维向量，其中 $|\mathcal I|$ 是商品总数</p>
<p>输入序列的嵌入矩阵记为 $\bf E\in\mathbb R^{n\times d}$，其中 $\bf E_i=\bf M_{s_i}$</p>
<p>由于自注意力机制本身无法感知位置信息，入了可学习的位置Embedding $\bf P\in\mathbb R^{n\times d}$，因此最终的序列输入表示为：<br>$$<br>\begin{split}\widehat{\bf E}=\left[<br>  \begin{array}{c}<br>    \bf M_{s_1}+\bf P_{1} \\<br>    \bf M_{s_2}+\bf P_{2} \\<br>    \dots \\<br>    \bf M_{s_n}+\bf P_{n}<br>  \end{array}<br>\right]\end{split}<br>$$<br>SASRec使用自注意力机制来捕捉序列中物品之间的依赖关系，采用了缩放点积注意力：<br>$$<br>\text{Attention}(\bf Q,\bf K,\bf V)=\text{Softmax}\left(\frac{\bf Q\bf K^T}{\sqrt{d}}\right)\bf V<br>$$<br>$\sqrt{d}$是缩放因子用于稳定训练</p>
<p>这里自注意力机制需要采用因果掩码，不能偷看未来的行为</p>
<p><strong>前馈网络层</strong></p>
<p>得到自注意力层的输出后，前馈网络为模型引入了非线性变换能力：<br>$$<br>\bf F_i = \text{FFN}(\bf S_i)=\text{ReLU}(\bf S_i\bf W^{(1)}+\bf b^{(1)})\bf W^{(2)}+\bf b^{(2)}<br>$$<br><strong>预测与训练</strong></p>
<p>经过多层Transformer模块的加工后，模型会基于最后一个物品(第$t$个)的输出表示$\bf F_t$来预测用户最可能交互的下一个物品</p>
<p>这个预测过程，本质上就是在整个物品库中，寻找与该输出表示向量最相似的物品向量</p>
<p>物品$i$的相关性分数为<br>$$<br>r_{i,t}=\bf F_t\bf M_i^T<br>$$<br>这里复用了物品Embedding矩阵,既减少了参数量又提高了性能</p>
<p><font color="Violetred">SASRec属于Decoder Only的Transformer模型</font>，对于时间步$t$，期望输出 $o_t$ 定义为：<br>$$<br>o_t=\begin{cases}<br>\text{pad} &amp; \text{如果 } s_t \text{ 是填充项}\\<br>s_{t+1} &amp; 1\leq t&lt;n\\<br>\mathcal S^{u}_{|\mathcal S^u|} &amp; t=n<br>\end{cases}<br>$$<br>其中<code>&lt;pad&gt;</code>表示填充项，$\mathcal{S}^{u}$表示用户$u$的交互序列</p>
<p>模型训练时，输入为$s$，期望输出为$o$，采用二元交叉熵作为损失函数：<br>$$<br>-\sum_{\mathcal S^{u}\in\mathcal S}\sum_{t\in[1,2,\dots,n]}\left[\log(\sigma(r_{o_t,t}))+\sum_{j\not\in \mathcal S^{u}}\log(1-\sigma(r_{j,t}))\right]<br>$$<br>其中 $\sigma$ 是sigmoid函数，$r_{o_t,t}$ 是正样本分数，$r_{j,t}$ 是负样本分数</p>
<p><strong>核心代码</strong></p>
<p>SASRec的核心在于将位置编码与序列嵌入相结合，然后通过多层Transformer模块处理</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 位置编码与序列嵌入相加</span></span><br><span class="line">position_embedding = PositionEncodingLayer(</span><br><span class="line">    dims=emb_dim,</span><br><span class="line">    max_len=max_seq_len,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">)(sequence_embedding)</span><br><span class="line">sequence_embedding = add_tensor_func([sequence_embedding, position_embedding])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多头注意力层，使用因果掩码确保只看到历史信息</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(mha_num):</span><br><span class="line">    sequence_embedding_norm = tf.keras.layers.LayerNormalization()(</span><br><span class="line">        sequence_embedding</span><br><span class="line">    )</span><br><span class="line">    sequence_embedding_output = tf.keras.layers.MultiHeadAttention(</span><br><span class="line">        num_heads=nums_heads,</span><br><span class="line">        key_dim=emb_dim,</span><br><span class="line">        dropout=dropout,</span><br><span class="line">        name=<span class="string">f"<span class="subst">{i}</span>_block"</span>,</span><br><span class="line">    )(sequence_embedding_norm, sequence_embedding, use_causal_mask=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># use_causal_mask=True 确保预测第t个位置时只能看到前t-1个位置</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 残差连接</span></span><br><span class="line">    sequence_embedding = add_tensor_func(</span><br><span class="line">        [sequence_embedding, sequence_embedding_output]</span><br><span class="line">    )</span><br><span class="line">    sequence_embedding = tf.keras.layers.LayerNormalization()(sequence_embedding)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前馈网络层</span></span><br><span class="line">    sequence_embedding = DNNs(</span><br><span class="line">        units=[emb_dim, emb_dim],</span><br><span class="line">        dropout_rate=dropout,</span><br><span class="line">        activation=<span class="string">'relu'</span>,</span><br><span class="line">        name=<span class="string">f"<span class="subst">{i}</span>_dnn"</span>,</span><br><span class="line">    )(sequence_embedding)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="HSTU"><a href="#HSTU" class="headerlink" title="HSTU"></a>HSTU</h4><p>SASRec 验证了将推荐问题建模为序列预测的有效性，但其建模对象相对单一，仅使用由物品 ID 构成的行为序列</p>
<p>在真实场景中，用户行为远比这复杂，除物品本身外，还包含用户属性、行为类型(点击、加购、购买)以及时间等上下文信息</p>
<p>HSTU(<u>Zhai <em>et al.</em>, 2024</u>) 正是对这一问题的系统探索。该模型代表了生成式推荐范式的重要演进方向：不再将序列视为简单的物品 ID 列表，而是将所有异构特征统一编码为一个更复杂的事件流(Event Stream)，并以此作为模型输入</p>
<p>HSTU 的目标是学习这一复杂事件序列的结构，并预测下一个可能发生的事件</p>
<p>尽管这种统一化建模方式在概念上更为优雅，但在实现层面面临特征处理、模型架构和信号传递等多重挑战</p>
<p><font color="Violetred">HSTU旨在以单一模块取代传统推荐系统中特征提取、交互和预测等多个异构组件</font></p>
<p><strong>特征处理</strong></p>
<p>HSTU的特征处理分为两个策略</p>
<p>对于类别特征，它将所有信息按时间戳“拉平”成一个统一的序列</p>
<p>一个用户的行为流：<code>[用户年龄=30, 登录, 浏览物品A(类别:手机), 浏览物品B(类别:手机壳), 将B加入购物车, 退出]</code></p>
<p>SASRec可能只看到 <code>[A, B]</code></p>
<p>HSTU则试图理解整个事件流：<code>[(特征:年龄,值:30), (行为:登录), (行为:浏览,物品:A), (行为:浏览,物品:B), (行为:加购,物品:B), (行为:退出)]</code></p>
<p>对于变化频繁的数值特征，HSTU则采用隐式建模策略</p>
<blockquote>
<p>加权计数器：近 7 天点击次数；近 30 分钟曝光数</p>
<p>比率：点击 / 购买</p>
<p>这些信息如果直接塞到输入里会比较麻烦，让模型通过观察用户在序列中的实际行为模式来自动推断这些数值信息</p>
</blockquote>
<p>HSTU的输入输出可以表示为</p>
<ul>
<li>输入$x_i$：$(\Phi_0,a_0), (\Phi_1,a_1), \ldots, (\Phi_{n_c-1},a_{n_c-1})$</li>
<li>输出$y_i$：$\Phi_1’,\Phi_2’,\ldots,\Phi_{n_c-1}’,\varnothing$</li>
</ul>
<p>其中 $\Phi_i$ 表示用户在时刻 $i$ 交互的内容，$a_i$ 表示用户在时刻 $i$ 的行为</p>
<p>$\Phi_i’$ 表示用户在时刻 $i$ 交互的内容，$\Phi_i’ = \Phi_i$ (如果 $a_i$ 为正向行为)，否则为 $\varnothing$</p>
<blockquote>
<p>输出序列最后一位设为 $\varnothing$，是为了在自回归建模中保持输入与输出长度对齐，并显式表示“该位置不存在下一事件”，从而简化模型结构与训练过程，同时避免引入无意义的监督信号</p>
</blockquote>
<p>在推荐系统的召回阶段，模型学习一个条件分布$p(\Phi_{i+1}|u_i)$，其中 $u_i$ 是用户在当前时刻的表示向量，$\Phi_{i+1}$ 是候选物品</p>
<p>召回的目标是从物品库 $\mathbb X_c$ 中选择能够最大化用户满意度的物品，即 $\text{argmax}_{\Phi \in \mathbb X_c} p(\Phi|u_i)$</p>
<p>HSTU召回与标准的序列生成任务有两个不同的地方：</p>
<ul>
<li>首先用户可能对推荐的物品产生负面反应，因此监督信号不总是下一个物品</li>
<li>其次当序列中包含非交互相关的特征(如人口统计学信息)时，对应的输出标签是未定义的</li>
</ul>
<p><font color="DarkViolet">所以HSTU不是每个时间步都提供监督，只有“有效交互事件”才产生训练信号，并且用 ∅ 表示这个位置不训练、不预测</font></p>
<p>通过这种方式，HSTU将复杂的推荐问题转化为序列到序列的学习问题，使得模型能够基于用户的历史行为序列预测其未来可能感兴趣的内容</p>
<p><strong>架构统一：HSTU单元</strong></p>
<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2026-01-19_22-38-59.webp" alt="Snipaste_2026-01-19_22-38-59"></p>
<p>为用单一模块替代 DLRM(深度学习推荐模型) 中分散的特征提取、特征交互和预测组件，HSTU 提出了<strong>层次序列转换单元(Hierarchical Sequential Transduction Unit, HSTU)</strong></p>
<p>HSTU 作为统一的建模单元，每一层由三个紧密耦合的子模块组成：</p>
<ul>
<li><p><strong>点向投影(Pointwise Projection)</strong>：通过一次线性变换后再切分的方式，同时生成注意力计算和门控所需的多个信号<br>$$<br>U(X), V(X), Q(X), K(X) = \text{Split}(\phi_1(f_1(X)))<br>$$<br>避免了多分支结构，使特征映射在同一空间内完成</p>
</li>
<li><p><strong>点向聚合(Pointwise Aggregation)</strong>：采用点向聚合而非传统Softmax来保持推荐系统中用户偏好强度信息<br>$$<br>A(X)V(X) = \phi_2\left(Q(X)K(X)^T + \text{rab}^{p,t}\right)V(X)<br>$$<br>引入了包含位置与时间信息的相对注意力偏置$\text{rab}^{p,t}$，以保留用户偏好强度与时序信号，避免 Softmax 归一化带来的信息压缩</p>
</li>
<li><p><strong>点向转换(Pointwise Transformation)</strong>：使用门控机制进行特征选择与重组<br>$$<br>Y(X) = f_2\left(\text{Norm}\left(A(X)V(X)\right) \odot U(X)\right)<br>$$<br>对不同特征维度进行条件控制，起到类似 MoE(Mixture of Experts)中路由器的作用</p>
</li>
</ul>
<p>HSTU 的三个子模块在功能上统一覆盖了传统 DLRM 的核心流程：</p>
<ol>
<li>特征提取：通过注意力机制实现对历史特征的目标感知聚合；</li>
<li>特征交互：注意力聚合结果($A(X)V(X)$)捕捉高阶关系，门控部分($U(X)$)保留低阶信息，实现动态特征组合</li>
<li>表示转换：门控机制对每个特征维度进行条件选择，形成类似 MoE 的条件计算效果</li>
</ol>
<blockquote>
<p>SE 模块通过全局压缩生成静态的通道权重来增强特征表示，而 HSTU 中的 U(X) 是基于当前序列上下文逐点生成的动态门控信号，用于在特征维度上进行条件化选择与路由</p>
</blockquote>
<p><strong>替换Softmax归一化机制</strong></p>
<p>HSTU 在 Pointwise Aggregation 阶段放弃了传统 Transformer 中的 <strong>Softmax 注意力归一化</strong>，转而采用非归一化的点向聚合方式</p>
<p>因为在推荐场景中，用户偏好的强度本身就是关键信号，而 Softmax 会将所有历史行为的注意力权重强制归一化为 1，从而只能表达相对重要性，容易削弱行为数量本身所蕴含的绝对强度信息</p>
<p>例如，当推荐科幻电影时，用户 A 的历史中包含大量科幻片，而用户 B 仅有少量相关行为。Softmax 注意力会迫使用户 A 的多次科幻行为分摊有限的权重总量，导致其整体偏好强度与用户 B 的差异被压缩</p>
<p>相比之下，HSTU 的点向聚合通过独立计算每个历史事件的相关性(如使用 SiLU 激活)，不进行跨项归一化，使多个相关行为能够累积产生更强的激活信号，从而更真实地反映用户偏好的强度<br>$$<br>\text{SiLU}(x) = x\cdot \sigma(x) = \frac{x}{1+e^{-x}}<br>$$<br>这种设计使模型不仅能够进行排序，还能更好地<font color="DarkViolet">服务于点击率、观看时长等对数值强度敏感的推荐目标</font></p>
<p><strong>训练与召回</strong></p>
<p>HSTU 的整体流程分为训练与召回两个阶段</p>
<p>在训练阶段，模型按照前述的输入–输出定义，采用标准的 Transformer 训练方式，通过预测用户行为序列中的<font color="Violetred">下一个正向交互物品</font>，学习用户偏好演化和物品之间的依赖关系</p>
<p>在召回阶段，经过多层HSTU编码器处理后，用户的历史行为事件序列被压缩为一个能够概括长期兴趣并反映当前状态的动态表示向量$u_i$。基于该表示，召回过程转化为向量检索问题：将$u_i$与物品语料库中所有物品的嵌入向量进行高效的相似度计算，并结合近似最近邻(ANN)搜索，从大规模物品集合中高效检索出与用户兴趣最匹配的 Top-K 物品，形成召回候选集</p>
<p>如果说 SASRec 将推荐序列建模为由物品 ID 构成的“句子”，那么 HSTU 则将用户属性、行为类型、物品信息和上下文统一视为一种更复杂的“语言”，并通过 HSTU 单元这一新的“语法规则”进行建模。其核心创新在于显著丰富了输入表达能力，从而实现对用户行为更深入、更细粒度的理解</p>
<p><strong>核心代码</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">HstuLayer</span>(tf.keras.layers.Layer):</span><br><span class="line">    <span class="string">"""HSTU层实现点向聚合的注意力机制"""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_units, num_heads, attention_type=<span class="string">'dot_product'</span>,</span></span><br><span class="line"><span class="params">                 dropout_rate=<span class="number">0</span>, causality=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(HstuLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.num_units = num_units</span><br><span class="line">        <span class="variable language_">self</span>.num_heads = num_heads</span><br><span class="line">        <span class="variable language_">self</span>.attention_type = attention_type</span><br><span class="line">        <span class="variable language_">self</span>.dropout_rate = dropout_rate</span><br><span class="line">        <span class="variable language_">self</span>.causality = causality</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        <span class="comment"># 点向投影：同时产生Q、K、V(这里简化为独立投影)</span></span><br><span class="line">        <span class="variable language_">self</span>.query_dense = tf.keras.layers.Dense(</span><br><span class="line">            <span class="variable language_">self</span>.num_units, activation=<span class="literal">None</span>, use_bias=<span class="literal">False</span>, name=<span class="string">'query_projection'</span></span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.key_dense = tf.keras.layers.Dense(</span><br><span class="line">            <span class="variable language_">self</span>.num_units, activation=<span class="literal">None</span>, use_bias=<span class="literal">False</span>, name=<span class="string">'key_projection'</span></span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.value_dense = tf.keras.layers.Dense(</span><br><span class="line">            <span class="variable language_">self</span>.num_units, activation=<span class="literal">None</span>, use_bias=<span class="literal">False</span>, name=<span class="string">'value_projection'</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">silu</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">"""SiLU (Swish) 激活函数，用于点向聚合"""</span></span><br><span class="line">        <span class="keyword">return</span> x * tf.sigmoid(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, input_interval=<span class="literal">None</span>, training=<span class="literal">True</span></span>):</span><br><span class="line">        queries = keys = inputs</span><br><span class="line">        batch_size = tf.shape(queries)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 点向投影：线性变换生成Q、K、V</span></span><br><span class="line">        Q = <span class="variable language_">self</span>.query_dense(queries)  <span class="comment"># (N, T_q, C)</span></span><br><span class="line">        K = <span class="variable language_">self</span>.key_dense(keys)       <span class="comment"># (N, T_k, C)</span></span><br><span class="line">        V = <span class="variable language_">self</span>.value_dense(keys)     <span class="comment"># (N, T_k, C)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分割多头</span></span><br><span class="line">        <span class="comment"># 合并batch维和head维，加快matmul</span></span><br><span class="line">        Q_ = tf.reshape(Q, [batch_size * <span class="variable language_">self</span>.num_heads, -<span class="number">1</span>,</span><br><span class="line">                           <span class="variable language_">self</span>.num_units // <span class="variable language_">self</span>.num_heads]) <span class="comment"># (h*N, T_q, d_k)</span></span><br><span class="line">        K_ = tf.reshape(K, [batch_size * <span class="variable language_">self</span>.num_heads, -<span class="number">1</span>,</span><br><span class="line">                           <span class="variable language_">self</span>.num_units // <span class="variable language_">self</span>.num_heads]) <span class="comment"># (h*N, T_k, d_k)</span></span><br><span class="line">        V_ = tf.reshape(V, [batch_size * <span class="variable language_">self</span>.num_heads, -<span class="number">1</span>,</span><br><span class="line">                           <span class="variable language_">self</span>.num_units // <span class="variable language_">self</span>.num_heads]) <span class="comment"># (h*N, T_k, d_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用SiLU激活(HSTU特色)</span></span><br><span class="line">        Q_, K_, V_ = <span class="variable language_">self</span>.silu(Q_), <span class="variable language_">self</span>.silu(K_), <span class="variable language_">self</span>.silu(V_)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 点向聚合：计算注意力但不使用Softmax归一化</span></span><br><span class="line">        outputs = tf.matmul(Q_, tf.transpose(K_, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]))  <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 因果掩码(用于序列预测)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.causality:</span><br><span class="line">            diag_vals = tf.ones_like(outputs[<span class="number">0</span>, :, :]) <span class="comment"># (T_q, T_k)</span></span><br><span class="line">            tril = tf.linalg.band_part(diag_vals, -<span class="number">1</span>, <span class="number">0</span>) <span class="comment"># 下三角矩阵</span></span><br><span class="line">            causality_mask = tf.tile(tf.expand_dims(tril, <span class="number">0</span>), <span class="comment"># (1，T_q, T_k)</span></span><br><span class="line">                                    [tf.shape(outputs)[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>]) <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line">            paddings = tf.ones_like(causality_mask) * (-(<span class="number">2</span> ** <span class="number">32</span>) + <span class="number">1</span>)</span><br><span class="line">            outputs = tf.where(tf.equal(causality_mask, <span class="number">0</span>), paddings, outputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里不使用Softmax，而是直接应用激活(保留强度信息)</span></span><br><span class="line">        weights = <span class="variable language_">self</span>.silu(outputs)  <span class="comment"># 点向聚合的核心：用SiLU代替Softmax</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加权求和(点向转换)</span></span><br><span class="line">        attention_output = tf.matmul(weights, V_)  <span class="comment"># (h*N, T_q, d_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 合并多头</span></span><br><span class="line">        outputs = tf.reshape(attention_output,</span><br><span class="line">                           [batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.num_units]) <span class="comment"># (N, T_q, C)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 残差连接</span></span><br><span class="line">        outputs += queries</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></tbody></table></figure>

<p>HSTU的关键设计在于用SiLU激活代替Softmax归一化</p>
<p>传统Softmax会将注意力权重归一化使其和为1，这抹平了不同用户兴趣强度的差异</p>
<p>而HSTU通过 <code>weights = self.silu(outputs)</code> 保持了原始的相关性强度，使得历史行为丰富的用户能够产生更强的激活信号，更准确地反映用户的真实偏好强度</p>
<h4 id="TIGER"><a href="#TIGER" class="headerlink" title="TIGER"></a>TIGER</h4><p>HSTU 通过丰富输入信息增强了模型对用户行为上下文的理解能力，但生成式召回的演进还存在另一条同样重要的路径：<font color="DarkViolet">对“物品表示方式”本身的重新思考</font></p>
<p>在传统推荐系统中，物品通常被表示为彼此独立、无语义的原子 ID，无论作为输入还是预测目标，这种表示方式都存在存在两个固有局限</p>
<ul>
<li><strong>语义鸿沟</strong>：模型难以理解物品间的内在联系，例如两款耐克篮球鞋在语义上高度接近，但模型只能通过大量共现数据间接学习这种关系，效率较低</li>
<li><strong>泛化难题</strong>：对于训练阶段未出现的新物品，模型由于词表中不存在对应 ID，无法进行推荐，从而导致冷启动困难</li>
</ul>
<p>TIGER(Transformer Index for GEnerative Recommenders) <u>(Rajput et al., 2023)</u> 正是针对这一问题提出的解决方案，其核心思想是：<font color="Violetred">不再让模型生成无语义的原子 ID，而是生成能够描述物品语义的“语义 ID”</font></p>
<p>通过语义 ID，相似物品可以共享表示和知识，即使是从未出现过的新物品，也能基于其语义特征被合理推荐</p>
<p>TIGER 的整体实现分为两步：</p>
<ol>
<li>首先基于物品内容特征为每个物品构建结构化的语义 ID</li>
<li>随后以这些语义 ID 组成的序列为基础，训练生成式推荐模型</li>
</ol>
<p>通过这种方式，模型既提升了语义建模能力，也以更紧凑的形式刻画了大规模物品空间</p>
<p><strong>语义ID的生成 (RQ-VAE)</strong></p>
<p>语义 ID 的目标是为每个物品构建一个具有语义结构的离散表示：内容特征相似的物品，其语义 ID 在多个维度上应具有部分重叠，从而使模型能够显式建模物品之间的深层语义关系</p>
<p>TIGER采用残差量化变分自编码器 (RQ-VAE) <u>(Zeghidour <em>et al.</em>, 2021)</u> 来实现这一目标</p>
<p>RQ-VAE 是一种<font color="Violetred">多层向量量化模型</font>，通过对潜在表示的残差进行逐层量化，将连续语义空间映射为一个由多个码本索引组成的语义 ID 元组</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/tiger_rqvae (1).webp" alt="tiger_rqvae (1)" style="zoom: 50%;">

<p>RQ-VAE量化方法的具体过程：</p>
<ol>
<li><p>内容编码：预训练的内容编码器(如 Sentence-T5)将物品的文本信息(标题、描述等)编码为语义向量 $\mathbf x$</p>
</li>
<li><p>潜在表示与初始残差：RQ-VAE编码器 $\mathcal E$ 将输入向量 $\mathbf x$ 进一步编码为潜在表示 $\mathbf z := \mathcal E(\mathbf x)$，并将初始残差定义为 $\mathbf r_0 := \mathbf z$</p>
</li>
<li><p>分层残差量化：模型包含 $m$ 个层级，每个层级 $d \in {0, 1, …, m-1}$ 都有一个独立的码本 $\mathcal C_d$</p>
<p>在第 $d$ 层，通过在码本中寻找与当前残差 $\mathbf r_d$ 最接近的码字来完成量化<br>$$<br>c_d = \arg\min_{k} |\mathbf r_d - \mathbf e_k|^2 \quad \text{其中 } \mathbf e_k \in \mathcal C_d<br>$$</p>
</li>
<li><p>残差更新：量化后计算新的残差并传入下一层<br>$$<br>\mathbf r_{d+1} := \mathbf r_d - \mathbf e_{c_d}<br>$$</p>
</li>
<li><p>重复此过程$m$次，最终得到一个由$m$个码本组成的语义ID元组$(c_0, c_1, …, c_{m-1})$</p>
</li>
</ol>
<p>RQ-VAE 通过联合损失进行端到端训练，损失函数同时包含：</p>
<ul>
<li><strong>重构损失</strong>：确保解码器能够从量化表示中恢复原始语义</li>
<li><strong>量化损失</strong>：用于更新各层码本向量</li>
</ul>
<p>从而联合优化编码器、解码器和所有码本向量</p>
<p>为保证物品 ID 的唯一性，若多个物品生成了完全相同的语义 ID，TIGER 会在语义 ID 末尾追加一个额外索引位进行区分，例如 <code>(12, 24, 52, 0)</code> 和 <code>(12, 24, 52, 1)</code></p>
<p><strong>基于语义ID的生成式检索</strong></p>
<p>拥有了每个物品的语义ID后，推荐任务就转变为一个标准的序列到序列生成问题</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/tiger_transformer (1).webp" alt="tiger_transformer (1)" style="zoom:67%;">

<ol>
<li><p>将用户的历史交互序列$(\text{item}<em>1, \text{item}<em>2, …, \text{item}<em>n)$转换为其对应的语义ID序列：<br>$$<br>((c</em>{1,0}, …, c</em>{1,m-1}), (c</em>{2,0}, …, c_{2,m-1}), …, (c_{n,0}, …, c_{n,m-1}))<br>$$<br>其中 $(c_{i,0}, …, c_{i,m-1})$ 是 $\text{item}_i$ 的语义ID</p>
</li>
<li><p>将每个物品的语义ID元组展平，形成一个长序列作为Transformer模型的输入：<br>$$<br>(c_{1,0}, …, c_{1,m-1}, c_{2,0}, …, c_{2,m-1}, …, c_{n,0}, …, c_{n,m-1})<br>$$</p>
<blockquote>
<p>一个物品 = 一个 token 变为 一个物品 = 一个 token 序列</p>
</blockquote>
</li>
<li><p>练一个标准的Encoder-Decoder Transformer模型</p>
<p>Encoder负责理解用户历史序列的上下文，Decoder则自回归地、逐个码本地生成下一个最可能被用户交互的物品语义ID $(c_{n+1,0}, …, c_{n+1,m-1})$</p>
</li>
<li><p>在推理阶段，一旦生成了完整的预测语义ID，就可以通过查找表将其映射回具体的物品，完成推荐</p>
</li>
</ol>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p><strong>多兴趣召回</strong></p>
<p>解决的是一个用户不止一个兴趣</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>MIND</th>
<th>SDM</th>
</tr>
</thead>
<tbody><tr>
<td>提出背景</td>
<td>用户兴趣多样性</td>
<td>用户兴趣 + 行为依赖</td>
</tr>
<tr>
<td>核心思想</td>
<td>学习多个兴趣向量</td>
<td>在多兴趣基础上引入序列建模</td>
</tr>
<tr>
<td>用户表示</td>
<td>多个兴趣 embedding</td>
<td>多个兴趣 embedding</td>
</tr>
<tr>
<td>兴趣生成方式</td>
<td>动态路由(Capsule)</td>
<td>Attention + LSTM/GRU</td>
</tr>
<tr>
<td>是否建模顺序</td>
<td>否(集合视角)</td>
<td>是(序列视角)</td>
</tr>
<tr>
<td>行为依赖建模</td>
<td>弱</td>
<td>强</td>
</tr>
<tr>
<td>召回方式</td>
<td>各兴趣向量分别召回</td>
<td>各兴趣向量分别召回</td>
</tr>
<tr>
<td>代表优势</td>
<td>兴趣解耦清晰</td>
<td>兼顾兴趣多样性与演化</td>
</tr>
<tr>
<td>局限</td>
<td>忽略时序信息</td>
<td>结构复杂、训练成本高</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>对比点</th>
<th>GRU</th>
<th>LSTM</th>
</tr>
</thead>
<tbody><tr>
<td>门的数量</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>是否有独立记忆单元</td>
<td>否</td>
<td>是</td>
</tr>
<tr>
<td>参数量</td>
<td>少</td>
<td>多</td>
</tr>
<tr>
<td>训练速度</td>
<td>快</td>
<td>慢</td>
</tr>
<tr>
<td>表达能力</td>
<td>强</td>
<td>很强</td>
</tr>
</tbody></table>
<p>MIND 强调“兴趣分离”，SDM 在此基础上进一步刻画“兴趣如何随序列演化”</p>
<p><strong>生成式序列召回模型</strong></p>
<p>解决的是如何用生成式方式做序列召回</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>SASRec</th>
<th>HSTU</th>
<th>TIGER</th>
</tr>
</thead>
<tbody><tr>
<td>建模范式</td>
<td>自回归序列预测</td>
<td>自回归序列预测</td>
<td>Seq2Seq 生成</td>
</tr>
<tr>
<td>输入表示</td>
<td>物品 ID 序列</td>
<td>统一事件流</td>
<td>语义 ID 序列</td>
</tr>
<tr>
<td>是否丰富输入</td>
<td>否</td>
<td>是</td>
<td>是(内容侧)</td>
</tr>
<tr>
<td>核心创新</td>
<td>Transformer 引入推荐</td>
<td>点向聚合 + 门控</td>
<td>语义 ID</td>
</tr>
<tr>
<td>Attention 形式</td>
<td>Softmax</td>
<td>非 Softmax(SiLU)</td>
<td>标准 Softmax</td>
</tr>
<tr>
<td>用户建模能力</td>
<td>中</td>
<td>强</td>
<td>中</td>
</tr>
<tr>
<td>物品语义建模</td>
<td>无</td>
<td>无</td>
<td>强</td>
</tr>
<tr>
<td>冷启动能力</td>
<td>差</td>
<td>差</td>
<td>好</td>
</tr>
<tr>
<td>与 NLP 同构性</td>
<td>中</td>
<td>中</td>
<td>高</td>
</tr>
</tbody></table>
<p>SASRec 打开了生成式序列召回的大门，HSTU深化了用户理解，而 TIGER 重构了物品语言本身</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>深化用户兴趣表示</th>
<th>重构物品表示(生成式召回)</th>
</tr>
</thead>
<tbody><tr>
<td>代表模型</td>
<td>SASRec、HSTU</td>
<td>TIGER</td>
</tr>
<tr>
<td>关注重点</td>
<td>用户侧</td>
<td>物品侧</td>
</tr>
<tr>
<td>核心问题</td>
<td>如何更好理解用户</td>
<td>如何更好表示物品</td>
</tr>
<tr>
<td>输入改进</td>
<td>行为、上下文、事件流</td>
<td>内容 → 语义 ID</td>
</tr>
<tr>
<td>输出形式</td>
<td>原子物品 ID</td>
<td>语义 ID 序列</td>
</tr>
<tr>
<td>是否生成式</td>
<td>是(自回归)</td>
<td>是(Seq2Seq)</td>
</tr>
<tr>
<td>是否解决冷启动</td>
<td>否</td>
<td>是</td>
</tr>
<tr>
<td>主要优势</td>
<td>用户建模能力强</td>
<td>泛化能力强、语义共享</td>
</tr>
<tr>
<td>主要代价</td>
<td>仍受 ID 限制</td>
<td>系统复杂度高</td>
</tr>
</tbody></table>
<h2 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h2><p>召回方法的三次范式跃迁</p>
<ul>
<li><p>基于共现的召回(协同过滤)：UserCF / ItemCF</p>
<p>相似的用户喜欢相似的物品，共现统计 + 相似度计算</p>
<p>统计时代 的召回</p>
</li>
<li><p>基于向量的召回(Embedding / 双塔)：Item2Vec / EGES / FM / DSSM / YouTubeDNN</p>
<p>不再算相似度，而是学表示，向量内积 ≈ 相似度</p>
<p>工程范式：</p>
<ol>
<li>离线算 item embedding</li>
<li>在线算 user embedding</li>
<li>ANN 做近邻搜索</li>
</ol>
<p>表示学习 + 检索时代 的召回</p>
</li>
<li><p>基于状态与生成的召回(序列 / Transformer)：MIND / SDM / SASRec / HSTU / TIGER</p>
<p>用户不是一个点，而是一个随时间变化的状态</p>
<p>两条路线：</p>
<ol>
<li>多兴趣表示：一个用户 = 多个向量</li>
<li>生成式预测：预测下一个 item token</li>
</ol>
<p>区分长期 vs 短期兴趣，捕捉行为顺序，对当前意图更敏感</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ItemCF / Swing</span><br><span class="line">   ↘</span><br><span class="line">    Item2Vec / EGES ——→ ANN</span><br><span class="line">   ↗</span><br><span class="line">双塔 / YouTubeDNN</span><br><span class="line">   ↓</span><br><span class="line">MIND / SDM / SASRec</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<table>
<thead>
<tr>
<th>大类</th>
<th>模型</th>
<th>建模对象</th>
<th>用户表示</th>
<th>时序建模</th>
<th>相似性来源/<br>预测目标</th>
<th>主要优点</th>
<th>主要问题</th>
<th>典型使用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>协同过滤</strong></td>
<td>UserCF</td>
<td>U-U</td>
<td>❌</td>
<td>❌</td>
<td>用户行为重叠</td>
<td>可解释性强实现简单</td>
<td>用户规模大稀疏严重</td>
<td>教学/原型</td>
</tr>
<tr>
<td></td>
<td>ItemCF</td>
<td>I-I</td>
<td>❌</td>
<td>❌</td>
<td>物品共现统计</td>
<td>稳定、可缓存工业友好</td>
<td>热门物品偏置</td>
<td>传统I2I召回</td>
</tr>
<tr>
<td></td>
<td>Swing</td>
<td>I-I</td>
<td>❌</td>
<td>❌</td>
<td>高质量用户共现</td>
<td>抗噪声抑制热门</td>
<td>计算复杂</td>
<td>大规模I2I</td>
</tr>
<tr>
<td><strong>矩阵分解</strong></td>
<td>FunkSVD</td>
<td>U-I</td>
<td>✅</td>
<td>❌</td>
<td>向量内积</td>
<td>缓解稀疏Embedding 起点</td>
<td>表达能力有限</td>
<td>早期Embedding</td>
</tr>
<tr>
<td></td>
<td>BiasSVD</td>
<td>U-I</td>
<td>✅</td>
<td>❌</td>
<td>向量内积+偏置</td>
<td>拟合打分习惯</td>
<td>不适合冷启动</td>
<td>评分/精排</td>
</tr>
<tr>
<td><strong>向量召回(I2I)</strong></td>
<td>Item2Vec</td>
<td>I-I</td>
<td>❌</td>
<td>❌</td>
<td>序列共现</td>
<td>简单有效无用户特征</td>
<td>头部淹没长尾</td>
<td>I2I补充召回</td>
</tr>
<tr>
<td></td>
<td>EGES</td>
<td>I-I</td>
<td>❌</td>
<td>❌构图</td>
<td>图随机游走+属性</td>
<td>冷启动友好</td>
<td>静态 embedding</td>
<td>电商I2I</td>
</tr>
<tr>
<td></td>
<td>Airbnb I2I</td>
<td>I-I</td>
<td>❌</td>
<td>⚠️会话级</td>
<td>业务目标驱动</td>
<td>强转化导向</td>
<td>强业务依赖</td>
<td>搜索/预订</td>
</tr>
<tr>
<td><strong>向量召回(U2I)</strong></td>
<td>FM</td>
<td>U-I</td>
<td>✅</td>
<td>❌</td>
<td>特征交互分解</td>
<td>数学清晰</td>
<td>线性表达</td>
<td>轻量召回</td>
</tr>
<tr>
<td></td>
<td>DSSM</td>
<td>U-I</td>
<td>✅</td>
<td>❌</td>
<td>深度相似度</td>
<td>非线性能力强</td>
<td>负采样敏感</td>
<td>通用双塔</td>
</tr>
<tr>
<td></td>
<td>YouTubeDNN</td>
<td>U-I</td>
<td>✅</td>
<td>⚠️ pooling</td>
<td>next-item预测</td>
<td>超大规模可落地</td>
<td>静态兴趣</td>
<td>主流工业召回</td>
</tr>
<tr>
<td><strong>多兴趣召回</strong></td>
<td>MIND</td>
<td>U-I</td>
<td>✅</td>
<td>❌</td>
<td>兴趣胶囊匹配</td>
<td>捕捉多兴趣</td>
<td>兴趣坍缩风险</td>
<td>电商/内容</td>
</tr>
<tr>
<td></td>
<td>SDM</td>
<td>U-I</td>
<td>✅</td>
<td>✅</td>
<td>长短期融合</td>
<td>意图感知强</td>
<td>结构复杂</td>
<td>高阶召回</td>
</tr>
<tr>
<td><strong>生成式召回</strong></td>
<td>SASRec</td>
<td>ID序列</td>
<td>❌</td>
<td>✅</td>
<td>Transformer 预测</td>
<td>全局依赖</td>
<td>计算成本高</td>
<td>精细召回</td>
</tr>
<tr>
<td></td>
<td>HSTU</td>
<td>事件流</td>
<td>❌</td>
<td>✅</td>
<td>多模态事件</td>
<td>信息密度高</td>
<td>工程复杂</td>
<td>大厂实验</td>
</tr>
<tr>
<td></td>
<td>TIGER</td>
<td>语义 ID序列</td>
<td>❌</td>
<td>✅</td>
<td>生成语义 token</td>
<td>语义泛化强</td>
<td>体系成本高</td>
<td>前沿探索</td>
</tr>
</tbody></table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://yhblogs.cn">今天睡够了吗</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://yhblogs.cn/posts/24333.html">http://yhblogs.cn/posts/24333.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yhblogs.cn" target="_blank">がんばろう</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E2%8C%A8%EF%B8%8Fpython/">⌨️python</a></div><div class="post_share"><div class="social-share" data-image="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-vpp725_1280x720_(1).webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/31208.html" title="FunRec 推荐系统_精排模型"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7j931e_1280x720_(1) (1).png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">FunRec 推荐系统_精排模型</div></div></a></div><div class="next-post pull-right"><a href="/posts/30698.html" title="BERT_Pytorch"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7jjyd9_2560x1440.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">BERT_Pytorch</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/30698.html" title="BERT_Pytorch"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7jjyd9_2560x1440.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-09</div><div class="title">BERT_Pytorch</div></div></a></div><div><a href="/posts/31208.html" title="FunRec 推荐系统_精排模型"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7j931e_1280x720_(1) (1).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-18</div><div class="title">FunRec 推荐系统_精排模型</div></div></a></div><div><a href="/posts/58676.html" title="Leetcode100记录"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-9ozdyx_1280x720.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-26</div><div class="title">Leetcode100记录</div></div></a></div><div><a href="/posts/22642.html" title="windows安装ROCm"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/ROCm_logo.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-10</div><div class="title">windows安装ROCm</div></div></a></div><div><a href="/posts/3865533702.html" title="pyqt5简单实践"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071521231.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-28</div><div class="title">pyqt5简单实践</div></div></a></div><div><a href="/posts/35959.html" title="python信号处理"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-pokg2e.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-18</div><div class="title">python信号处理</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/b_2a1aef95f351a5f7ef72eb81e6838fd6.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">今天睡够了吗</div><div class="author-info__description">相遇是最小单位的奇迹</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071549233.webp" target="_blank" title="QQ"><i class="iconfont icon-QQ"></i></a><a class="social-icon" href="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071549234.webp" target="_blank" title="微信"><i class="iconfont icon-weixin"></i></a><a class="social-icon" href="https://space.bilibili.com/277953459?spm_id_from=333.1007.0.0" target="_blank" title="bilibili"><i class="iconfont icon-bilibili"></i></a><a class="social-icon" href="https://github.com/YaoHui-Wu06022" target="_blank" title="Github"><i class="iconfont icon-GitHub"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">保持理智，相信明天</div><div class="twopeople"><div class="twopeople"><div class="container" style="height:200px;"><canvas class="illo" width="800" height="800" style="max-width: 200px; max-height: 200px; touch-action: none; width: 640px; height: 640px;"></canvas></div> <script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople1.js"></script> <script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/zdog.dist.js"></script> <script id="rendered-js" src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople.js"></script> <style>.twopeople{margin:0;align-items:center;justify-content:center;text-align:center}canvas{display:block;margin:0 auto;cursor:move}</style></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4"><span class="toc-number">1.</span> <span class="toc-text">协同过滤</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ItemCF"><span class="toc-number">1.1.</span> <span class="toc-text">ItemCF</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Swing"><span class="toc-number">1.1.1.</span> <span class="toc-text">Swing</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#UserCF"><span class="toc-number">1.2.</span> <span class="toc-text">UserCF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3"><span class="toc-number">1.3.</span> <span class="toc-text">矩阵分解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8BFunkSVD"><span class="toc-number">1.3.1.</span> <span class="toc-text">基础模型FunkSVD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B%E6%A8%A1%E5%9E%8BBiasSVD"><span class="toc-number">1.3.2.</span> <span class="toc-text">改进模型BiasSVD</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.4.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8F%AC%E5%9B%9E"><span class="toc-number">2.</span> <span class="toc-text">向量召回</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#I2I%E5%8F%AC%E5%9B%9E"><span class="toc-number">2.1.</span> <span class="toc-text">I2I召回</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Word2Vec"><span class="toc-number">2.1.1.</span> <span class="toc-text">Word2Vec</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Item2Vec"><span class="toc-number">2.1.2.</span> <span class="toc-text">Item2Vec</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#EGES"><span class="toc-number">2.1.3.</span> <span class="toc-text">EGES</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Airbnb"><span class="toc-number">2.1.4.</span> <span class="toc-text">Airbnb</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#U2I%E5%8F%AC%E5%9B%9E"><span class="toc-number">2.2.</span> <span class="toc-text">U2I召回</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.2.1.</span> <span class="toc-text">双塔模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#FM%E5%9B%A0%E5%AD%90%E5%88%86%E8%A7%A3%E6%9C%BA"><span class="toc-number">2.2.2.</span> <span class="toc-text">FM因子分解机</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DSSM"><span class="toc-number">2.2.3.</span> <span class="toc-text">DSSM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#YouTubeDNN"><span class="toc-number">2.2.4.</span> <span class="toc-text">YouTubeDNN</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">2.3.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%8F%AC%E5%9B%9E"><span class="toc-number">3.</span> <span class="toc-text">序列召回</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%8C%96%E7%94%A8%E6%88%B7%E5%85%B4%E8%B6%A3%E8%A1%A8%E7%A4%BA"><span class="toc-number">3.1.</span> <span class="toc-text">深化用户兴趣表示</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MIND"><span class="toc-number">3.1.1.</span> <span class="toc-text">MIND</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SDM"><span class="toc-number">3.1.2.</span> <span class="toc-text">SDM</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%BC%8F%E5%8F%AC%E5%9B%9E%E6%96%B9%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">生成式召回方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SASRec"><span class="toc-number">3.2.1.</span> <span class="toc-text">SASRec</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HSTU"><span class="toc-number">3.2.2.</span> <span class="toc-text">HSTU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TIGER"><span class="toc-number">3.2.3.</span> <span class="toc-text">TIGER</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-2"><span class="toc-number">3.3.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-3"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">©2022 - 2026 By 今天睡够了吗</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">You must always have faith in who you are！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="false"></script><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>