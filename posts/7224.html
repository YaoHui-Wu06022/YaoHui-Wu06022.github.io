<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>循环神经网络 | がんばろう</title><meta name="author" content="今天睡够了吗"><meta name="copyright" content="今天睡够了吗"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="到目前为止遇到过两种类型的数据：表格数据和图像数据 卷积神经网络可以有效地处理空间信息，**循环神经网络(recurrent neural network，RNN)**则可以更好地处理序列信息，循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出 许多使用循环网络的例子都是基于文本数据的，因此将在本章中重点介绍语言模型 序列模型**序列模型(Sequence Model)">
<meta property="og:type" content="article">
<meta property="og:title" content="循环神经网络">
<meta property="og:url" content="http://yhblogs.cn/posts/7224.html">
<meta property="og:site_name" content="がんばろう">
<meta property="og:description" content="到目前为止遇到过两种类型的数据：表格数据和图像数据 卷积神经网络可以有效地处理空间信息，**循环神经网络(recurrent neural network，RNN)**则可以更好地处理序列信息，循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出 许多使用循环网络的例子都是基于文本数据的，因此将在本章中重点介绍语言模型 序列模型**序列模型(Sequence Model)">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-d8633m_1280x720.webp">
<meta property="article:published_time" content="2025-10-28T14:53:57.000Z">
<meta property="article:modified_time" content="2026-01-31T12:00:30.725Z">
<meta property="article:author" content="今天睡够了吗">
<meta property="article:tag" content="⌨️python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-d8633m_1280x720.webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yhblogs.cn/posts/7224.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '循环神经网络',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-01-31 12:00:30'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_3319458_ks437t3n4r.css"><link rel="stylesheet" href="/css/modify.css"><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/b_2a1aef95f351a5f7ef72eb81e6838fd6.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouye"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw iconfont icon-rili"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw iconfont icon-biaoqian"></i><span> 标签</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="がんばろう"><img class="site-icon" src="/img/favicon.png"><span class="site-name">がんばろう</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouye"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw iconfont icon-rili"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw iconfont icon-biaoqian"></i><span> 标签</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">循环神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-10-28T14:53:57.000Z" title="发表于 2025-10-28 14:53:57">2025-10-28</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">23.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>89分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="循环神经网络"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p>到目前为止遇到过两种类型的数据：表格数据和图像数据</p>
<p>卷积神经网络可以有效地处理空间信息，**循环神经网络(recurrent neural network，RNN)**则可以更好地处理序列信息，循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出</p>
<p>许多使用循环网络的例子都是基于文本数据的，因此将在本章中重点介绍语言模型</p>
<h2 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h2><p>**序列模型(Sequence Model)**是一类能够处理序列数据的模型，可以理解“顺序”这个维度的信息</p>
<p>比如：</p>
<ul>
<li>一段文字(词语或字母按顺序组成句子)</li>
<li>一段语音(声音信号是随时间变化的)</li>
<li>股票价格(每天的数据有先后关系)</li>
<li>传感器时间序列(如心率随时间的变化)</li>
</ul>
<h3 id="统计工具"><a href="#统计工具" class="headerlink" title="统计工具"></a>统计工具</h3><p>处理序列数据需要统计工具和新的深度神经网络架构，以下图所示的股票价格(富时100指数)为例</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/ftse100.png" alt="ftse100" style="zoom: 50%;">

<p>其中，用$x_t$表示价格，即在<strong>时间步(time step)</strong>$t \in \mathbb{Z}^+$时，观察到的价格</p>
<p>假设一个交易员想在$t$日的股市中表现良好，于是通过以下途径预测$x_t$<br>$$<br>x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1).<br>$$</p>
<h4 id="自回归模型"><a href="#自回归模型" class="headerlink" title="自回归模型"></a>自回归模型</h4><p>为了实现这个预测，交易员可以使用回归模型(如线性回归)，但会出现一个问题，输入$x_{t-1}, \ldots, x_1$本身因$t$而异，输入数据的数量会随着时间变化，输入维度不断增加，因此需要一种近似方法来简化计算</p>
<p>主要两种策略：</p>
<ul>
<li><p>策略一，序列考虑最近长度为$\tau$的时间范围，这使得模型的参数量保持不变，便于训练一个类似之前的深度网络，这种模型被称为<strong>自回归模型(autoregressive models)</strong>，因为它是对自身过去的数据进行回归</p>
</li>
<li><p>策略二，保留对过去观测的总结$h_t$，在每一步同时更新预测$\hat x_t = P(x_t \mid h_t)$和总结$h_t = g(h_{t-1}, x_{t-1})$，由于$h_t$不可直接观测，这种模型被称为<strong>隐变量自回归模型(latent autoregressive models)</strong></p>
<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/sequence-model.jpg" alt="sequence-model"></p>
</li>
</ul>
<p>但问题是训练数据如何生成，通常用历史观测来预测下一个时刻的值</p>
<p>一个常见的假设是：虽然$x_t$会变化，但序列的动力学特性保持不变。这个假设是合理的，因为新的动力学只能由新数据决定，而无法用已有数据预测它</p>
<p>统计学上，这种动力学不变的性质被称为<strong>平稳性(stationarity)</strong>，因此整个序列的估计值都将通过以下的方式获得：<br>$$<br>P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}, \ldots, x_1).<br>$$<br>如果处理的是离散的对象(如单词)而不是连续的数字，则上述的考虑仍然有效，唯一的差别是，对于离散的对象需要使用分类器而不是回归模型来估计$P(x_t \mid x_{t-1}, \ldots, x_1)$</p>
<h4 id="马尔可夫模型"><a href="#马尔可夫模型" class="headerlink" title="马尔可夫模型"></a>马尔可夫模型</h4><p>在自回归模型的近似法中使用$x_{t-1}, \ldots, x_{t-\tau}$而不是$x_{t-1}, \ldots, x_1$来估计$x_t$</p>
<p><strong>马尔可夫性(Markov condition)</strong>：未来只与现在有关，与过去无关，当前状态包含了预测未来所需的全部信息$P(x_{t+1} \mid x_t, x_{t-1}) = P(x_{t+1} \mid x_t)$</p>
<p>在估计当前状态时，只需要知道前一个状态而不用关心更久以前的状态</p>
<p>如果$\tau = 1$，得到<strong>一阶马尔可夫模型(first-order Markov model)</strong></p>
<p>序列的联合概率可以写成<br>$$<br>P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}) \text{ 当 } P(x_1 \mid x_0) = P(x_1).<br>$$<br>当$x_t$是离散值时，可以利用动态规划沿着**马尔可夫链(Markov chain)**精确地计算出结果，“链”指的就是状态之间一步步转移的关系</p>
<p>例如可以高效地计算$P(x_{t+1} \mid x_{t-1})$<br>$$<br>\begin{split}\begin{aligned}<br>P(x_{t+1} \mid x_{t-1})<br>&amp;= \frac{\sum_{x_t} P(x_{t+1}, x_t, x_{t-1})}{P(x_{t-1})}\\<br>&amp;= \frac{\sum_{x_t} P(x_{t+1} \mid x_t, x_{t-1}) P(x_t, x_{t-1})}{P(x_{t-1})}\\<br>&amp;= \sum_{x_t} P(x_{t+1} \mid x_t) P(x_t \mid x_{t-1})<br>\end{aligned}\end{split}<br>$$<br>可以通过中间状态$x_t$把两个相邻状态之间的概率联系起来</p>
<p>在隐马尔可夫模型中，动态规划能高效地计算这些概率</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>使用正弦函数和一些可加性噪声来生成序列数据</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">T = <span class="number">1000</span>  <span class="comment"># 总共产生1000个点</span></span><br><span class="line">time = torch.arange(<span class="number">1</span>, T + <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">x = torch.sin(<span class="number">0.01</span> * time) + torch.normal(<span class="number">0</span>, <span class="number">0.2</span>, (T,))</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br><span class="line">plt.plot(time, x)</span><br><span class="line">plt.grid()</span><br></pre></td></tr></tbody></table></figure>

<p>将这个序列转换为模型的特征－标签对</p>
<p>将数据映射为数据对$y_t = x_t$和$\mathbf{x}<em>t = [x</em>{t-\tau}, \ldots, x_{t-1}]$，比提供的数据样本少了$\tau$个</p>
<p>对于前$\tau$个的解决方案：如果拥有足够长的序列就丢弃这几项；或是用零填充序列</p>
<p>仅使用前600个“特征－标签”对进行训练</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tau = <span class="number">4</span></span><br><span class="line">features = torch.zeros((T - tau, tau)) <span class="comment"># 构建特征张量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">    features[:, i] = x[i: T - tau + i] <span class="comment"># 列填充feature</span></span><br><span class="line">labels = x[tau:].reshape((-<span class="number">1</span>, <span class="number">1</span>)) <span class="comment"># 标签从tau开始</span></span><br><span class="line"></span><br><span class="line">batch_size, n_train = <span class="number">16</span>, <span class="number">600</span></span><br><span class="line"><span class="comment"># 只有前n_train个样本用于训练</span></span><br><span class="line">train_iter = load_array((features[:n_train], labels[:n_train]),</span><br><span class="line">                            batch_size, is_train=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>使用一个相当简单的架构训练模型：一个拥有两个全连接层的多层感知机，ReLU激活函数和平方损失</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化网络权重的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)  <span class="comment"># 对应ReLU激活</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个简单的多层感知机</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">4</span>,<span class="number">10</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="comment"># MSELoss计算平方误差时不带系数1/2</span></span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">'none'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p>训练代码</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, loss, epochs, lr</span>):</span><br><span class="line">    trainer = torch.optim.Adam(net.parameters(), lr=lr) <span class="comment"># 这里不再用SGD了</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"epoch <span class="subst">{epoch+<span class="number">1</span>}</span>, loss: <span class="subst">{evaluate_loss(net, train_iter, loss):f}</span>"</span>)</span><br><span class="line">net = get_net()</span><br><span class="line">train(net, train_iter, loss, <span class="number">5</span>, <span class="number">0.01</span>)</span><br></pre></td></tr></tbody></table></figure>

<blockquote>
<p>Adam比SGD更智能，自动调整学习率步子大小，更适用于自然语言处理</p>
<p>SGD一般用在图像分类，在后期泛化更好</p>
</blockquote>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss: 0.072635</span><br><span class="line">epoch 2, loss: 0.060879</span><br><span class="line">epoch 3, loss: 0.056276</span><br><span class="line">epoch 4, loss: 0.059151</span><br><span class="line">epoch 5, loss: 0.053477</span><br></pre></td></tr></tbody></table></figure>

<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>由于训练损失很小，因此期望模型能有很好的工作效果</p>
<p>首先是检查模型预测下一个时间步的能力，也就是<strong>单步预测(one-step-ahead prediction)</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">onestep_preds = net(features)</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br><span class="line">plt.plot(time, x.detach().numpy(), label=<span class="string">"data"</span>)</span><br><span class="line">plt.plot(time[tau:], onestep_preds.detach().numpy(),label=<span class="string">"1-step preds"</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend()</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510292303.webp" alt="202510292303" style="zoom:80%;">

<p>单步预测效果不错，即使这些预测的时间步超过了600+4(也就是n_train+tau)，其结果看起来仍然是可信的</p>
<p>但这里有一个问题，后续的迈进需要一步步，必须使用自己的预测(而不是原始数据)来进行多步预测</p>
<p>对于直到$x_t$的观测序列，其在时间步$t+k$处的预测输出$\hat{x}_{t+k}$称为**$k$步预测(k-step-ahead-prediction)**</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">multistep_preds = torch.zeros(T) <span class="comment"># 初始化</span></span><br><span class="line">multistep_preds[: n_train + tau] = x[:n_train + tau] <span class="comment"># 选取测试集数据</span></span><br><span class="line"><span class="comment"># [n_train : n_train + τ] 测试集开始前的真实值</span></span><br><span class="line"><span class="comment"># [n_train + τ :] 用模型逐步预测后续的所有值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_train + tau, T):</span><br><span class="line">    multistep_preds[i] = net(multistep_preds[i-tau:i].reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br><span class="line">plt.plot(time, x.detach().numpy(), label=<span class="string">"data"</span>)</span><br><span class="line">plt.plot(time[tau:], onestep_preds.detach().numpy(),label=<span class="string">"1-step preds"</span>)</span><br><span class="line">plt.plot(time[n_train + tau:], multistep_preds[n_train+tau:].detach().numpy(), <span class="string">'-.'</span>, label=<span class="string">"multistep preds"</span>) <span class="comment"># 显示利用预测值去预测的结果</span></span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend()</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510301258.webp" alt="202510301258" style="zoom:80%;">

<p>绿线的预测显然并不理想，预测的结果很快就会衰减到一个常数，这是由于错误的积累，误差可能会相当快地偏离真实的观测结果</p>
<p>对于不同的k值，对比结果</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">max_step = <span class="number">64</span> <span class="comment"># 定义最大步伐</span></span><br><span class="line"><span class="comment"># 定义特征矩阵，前tau列为历史数据，后max_step存储模型预测值，以最大创建</span></span><br><span class="line">features = torch.zeros((T-tau-max_step+<span class="number">1</span>, tau+max_step))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">    features[:, i] = x[i: i+T-tau-max_step+<span class="number">1</span>] <span class="comment"># 前tau列填入真实历史值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau, tau + max_step):</span><br><span class="line">    <span class="comment"># 后tau+max_step填入预测值</span></span><br><span class="line">    features[:, i] = net(features[:, i - tau:i]).reshape(-<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line">steps = (<span class="number">1</span>, <span class="number">4</span>, <span class="number">16</span>, <span class="number">64</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> steps:</span><br><span class="line">    plt.plot(time[tau + i - <span class="number">1</span> : T - max_step + i], <span class="comment"># time长度与y匹配</span></span><br><span class="line">             features[:, tau + i - <span class="number">1</span>].detach().numpy(),</span><br><span class="line">             label=<span class="string">f"<span class="subst">{i}</span>-step preds"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid()</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510301326.webp" alt="202510301326" style="zoom: 80%;">

<p>清楚地说明了试图预测更远的未来时，预测的质量是如何变化的</p>
<p>一般超过4步预测跨度的预测几乎就是无用的</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul>
<li>内插法(在现有观测值之间进行估计)和外推法(对超出已知观测范围进行预测)在实践的难度上差别很大，对于所拥有的序列数据，在训练时始终要尊重其时间顺序，最好不要基于未来的数据进行训练</li>
<li>序列模型的估计需要专门的统计工具，两种较流行的选择是自回归模型和隐变量自回归模型</li>
<li>着对预测时间$k$值的增加，会造成误差的快速累积和预测质量的极速下降</li>
</ul>
<h2 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h2><p>文本的常见预处理步骤，包括：</p>
<ol>
<li>将文本作为字符串加载到内存中</li>
<li>将字符串拆分为词元(如单词和字符)</li>
<li>建立一个词表，将拆分的词元映射到数字索引</li>
<li>将文本转换为数字索引序列，方便模型操作</li>
</ol>
<p>新增加的导入库</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections <span class="comment">#　提供了一些比普通字典、列表更灵活的数据结构</span></span><br><span class="line"><span class="keyword">import</span> re          <span class="comment">#  用于字符串匹配、查找、替换、分割等操作</span></span><br></pre></td></tr></tbody></table></figure>

<p><code>collections</code> —— 高级数据结构工具箱</p>
<p>常见成员：</p>
<ul>
<li><code>Counter</code>：计数器，用来统计元素出现次数</li>
<li><code>deque</code>(双端队列)：高效地在头尾插入或删除元素</li>
</ul>
<p><code>re</code> —— 正则表达式库</p>
<p>常用函数：</p>
<ul>
<li><p><code>re.search(pattern, text)</code>：在文本中搜索匹配项</p>
</li>
<li><p><code>re.findall(pattern, text)</code>：找到所有匹配的子串</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">re.findall(<span class="string">r"[A-Za-z]+"</span>, <span class="string">"Hi 123 there!"</span>)  <span class="comment"># ['Hi', 'there']</span></span><br><span class="line"><span class="comment"># +号代表 匹配连续的一个或多个字母</span></span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p><code>re.sub(pattern, repl, text)</code>：按规则替换字符串</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.sub(<span class="string">r"\d+"</span>, <span class="string">"X"</span>, <span class="string">"Room 404"</span>)  <span class="comment"># "Room X"</span></span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<h3 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h3><p>从H.G.Well的<a target="_blank" rel="noopener" href="https://www.gutenberg.org/ebooks/35">The Time Machine</a>中加载文本，这是一个相当小的语料库，只有30000多个单词</p>
<p>下面的函数将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串，在这里忽略了标点符号和字母大写</p>
<p>数据集导入，和Kaggle类似</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DATA_HUB[<span class="string">'time_machine'</span>] = (  <span class="comment">#@save</span></span><br><span class="line">    DATA_URL + <span class="string">'timemachine.txt'</span>,</span><br><span class="line">    <span class="string">'090b5e7e70c295757f55df93cb0a180b9691891a'</span>) </span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read_time_machine</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""将时间机器数据集加载到文本行的列表中"""</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(download(<span class="string">'time_machine'</span>), <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># 一次性读取文件中所有行，结果是一个列表，每个元素是一行字符串</span></span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="comment"># 正则表达式，匹配所有非英文字母，替换为空格，' '</span></span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">'[^A-Za-z]+'</span>, <span class="string">' '</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="comment"># .strip() 去掉字符串首尾多余的空格，中间的保留</span></span><br><span class="line">    <span class="comment"># .lower() 转换成小写</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lines = read_time_machine()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'# 文本总行数: <span class="subst">{<span class="built_in">len</span>(lines)}</span>'</span>)</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">10</span>])</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 文本总行数: 3221</span><br><span class="line">the time machine by h g wells</span><br><span class="line">twinkled and his usually pale face was flushed and animated the</span><br></pre></td></tr></tbody></table></figure>

<h3 id="词元化"><a href="#词元化" class="headerlink" title="词元化"></a>词元化</h3><p><code>tokenize</code>函数将文本行列表(<code>lines</code>)作为输入，列表中的每个元素是一个文本序列(如一条文本行)</p>
<p>每个文本序列又被拆分成一个词元列表，**词元(token)**是文本的基本单位</p>
<p>最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">lines, token=<span class="string">'words'</span></span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""将文本行拆分为单词或字符词元"""</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">'words'</span>:</span><br><span class="line">        <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> lines] <span class="comment"># 根据空格切分字符串</span></span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">'char'</span>: <span class="comment"># 直接把每行拆成单个字符</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">list</span>(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'unknown token'</span> + token)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokens = tokenize(lines) <span class="comment"># 将刚刚拆开的行放入</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>): <span class="comment"># 前11行</span></span><br><span class="line">    <span class="built_in">print</span>(tokens[i]) </span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">['the', 'time', 'machine', 'by', 'h', 'g', 'wells']</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">['i']</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']</span><br><span class="line">['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']</span><br><span class="line">['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']</span><br></pre></td></tr></tbody></table></figure>

<h3 id="词表"><a href="#词表" class="headerlink" title="词表"></a>词表</h3><p>在文本处理中，词元通常是字符串，但深度学习模型只能处理数值输入，需要构建一个<strong>词表(vocabulary)</strong>，用于将每个词元映射为从0开始的整数索引</p>
<p>构建词表的步骤如下：</p>
<ol>
<li>将训练集中的所有文本合并，对其中出现的唯一词元进行统计，这个整体称为<strong>语料(corpus)</strong></li>
<li>根据每个词元的出现频率为其分配索引，出现频率过低的词元通常会被舍弃，以降低模型复杂度</li>
<li>对于语料中未出现或被删除的词元，会统一映射到一个特殊的<strong>未知词元</strong>(<code>"&lt;unk&gt;"</code>)</li>
</ol>
<p>词表中还可以包含一些特殊标记，用于在训练和生成过程中发挥作用</p>
<ul>
<li><code>"&lt;pad&gt;"</code>：填充词元，用于对齐序列长度</li>
<li><code>"&lt;bos&gt;"</code>：序列开始标记</li>
<li><code>"&lt;eos&gt;"</code>：序列结束标记</li>
</ul>
<p>构建一个<code>Vocab</code>类</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">count_corpus</span>(<span class="params">tokens</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""统计词元的频率"""</span></span><br><span class="line">    <span class="comment"># 这里的tokens是1D列表或2D列表</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokens) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(tokens[<span class="number">0</span>], <span class="built_in">list</span>):</span><br><span class="line">        <span class="comment"># 将词元列表展平成一个列表</span></span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        flat = []</span></span><br><span class="line"><span class="string">        for line in tokens:</span></span><br><span class="line"><span class="string">            for token in line:</span></span><br><span class="line"><span class="string">                flat.append(token)</span></span><br><span class="line"><span class="string">        tokens = flat</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    <span class="comment"># Python 内置的计数器容器类型</span></span><br><span class="line">    <span class="comment"># 返回一个字典状的对象：键是词元，值是出现次数</span></span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Vocab</span>: <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""文本词表"""</span></span><br><span class="line">    <span class="comment"># reserved_tokens 包含特殊标记</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tokens=<span class="literal">None</span>, min_freq=<span class="number">0</span>, reserved_tokens=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tokens=[]</span><br><span class="line">        <span class="keyword">if</span> reserved_tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            reserved_tokens=[]</span><br><span class="line">        <span class="comment"># 按出现频率排序</span></span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        <span class="comment"># 变量名前加一个下划线_ 表示这是类的内部属性，外部最好别直接访问</span></span><br><span class="line">        <span class="variable language_">self</span>._token_freqs = <span class="built_in">sorted</span>(</span><br><span class="line">            counter.items(),</span><br><span class="line">            key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>],</span><br><span class="line">            reverse=<span class="literal">True</span>) <span class="comment"># 降序排序，默认升序</span></span><br><span class="line">        <span class="comment"># 未知词元的索引为0</span></span><br><span class="line">        <span class="variable language_">self</span>.idx_to_token = [<span class="string">'&lt;unk&gt;'</span>] + reserved_tokens</span><br><span class="line">        <span class="comment"># 构建反向映射字典：“词元 → 索引”</span></span><br><span class="line">        <span class="variable language_">self</span>.token_to_idx = {token: idx</span><br><span class="line">                             <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.idx_to_token)}</span><br><span class="line">        <span class="keyword">for</span> token, freq <span class="keyword">in</span> <span class="variable language_">self</span>._token_freqs:</span><br><span class="line">            <span class="keyword">if</span> freq &lt; min_freq: <span class="comment"># 如果频率太小直接抛弃</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="comment"># 检查当前词是否已在词表中</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> <span class="variable language_">self</span>.token_to_idx:</span><br><span class="line">                <span class="variable language_">self</span>.idx_to_token.append(token) <span class="comment"># 加入 idx_to_token 列表末尾</span></span><br><span class="line">                <span class="comment"># 在 token_to_idx 里记录它的索引，长度-1为索引</span></span><br><span class="line">                <span class="variable language_">self</span>.token_to_idx[token] = <span class="built_in">len</span>(<span class="variable language_">self</span>.idx_to_token) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, tokens</span>): <span class="comment"># 索引运算符重载</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(tokens, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="comment"># 输入是单个词元</span></span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.token_to_idx.get(tokens, <span class="variable language_">self</span>.unk)</span><br><span class="line">            <span class="comment"># .get(tokens, self.unk) 词元在词表里，返回它对应的索引，不在返回unk</span></span><br><span class="line">        <span class="comment"># 输入是多个词元(列表or元组)</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="variable language_">self</span>.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_tokens</span>(<span class="params">self, indices</span>): <span class="comment"># 将输出转回词元</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(indices, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [<span class="variable language_">self</span>.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property </span><span class="comment"># 装饰器</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">unk</span>(<span class="params">self</span>):  <span class="comment"># 未知词元的索引为0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 其实它是一个函数，但用 @property 包装后可以当变量用</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">token_freqs</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._token_freqs</span><br></pre></td></tr></tbody></table></figure>

<p>使用时光机器数据集作为语料库来构建词表，然后打印前几个高频词元及其索引</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vocab = Vocab(tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(vocab.token_to_idx.items())[:<span class="number">10</span>])</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[('&lt;unk&gt;', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]</span><br></pre></td></tr></tbody></table></figure>

<p>现在可以将每一条文本行转换成一个数字索引列表</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'文本:'</span>, tokens[i])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'索引:'</span>, vocab[tokens[i]])</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">文本: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']</span><br><span class="line">索引: [1, 19, 50, 40, 2183, 2184, 400]</span><br><span class="line">文本: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']</span><br><span class="line">索引: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]</span><br></pre></td></tr></tbody></table></figure>

<h3 id="整合"><a href="#整合" class="headerlink" title="整合"></a>整合</h3><p>将所有功能打包到<code>load_corpus_time_machine</code>函数中，该函数返回<code>corpus</code>(词元索引列表)和<code>vocab</code>(时光机器语料库的词表)</p>
<p>在这里做了一些改变：</p>
<ol>
<li>为了简化训练，使用字符实现文本词元化(字符词表量级小，26个字母+空格)</li>
<li>时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的<code>corpus</code>仅处理为单个列表，而不是使用多词元列表构成的一个列表</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_corpus_time_machine</span>(<span class="params">max_tokens=-<span class="number">1</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""返回时光机器数据集的词元索引列表和词表"""</span></span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    tokens = tokenize(lines, <span class="string">'char'</span>) <span class="comment"># 每一行的文本被拆成一个个字母</span></span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    <span class="comment"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span></span><br><span class="line">    <span class="comment"># 所以将所有文本行展平到一个列表中</span></span><br><span class="line">    corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">if</span> max_tokens &gt; <span class="number">0</span>:</span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    <span class="keyword">return</span> corpus, vocab <span class="comment"># 字符索引的序列，词表对象</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">corpus, vocab = load_corpus_time_machine()</span><br><span class="line"><span class="built_in">len</span>(corpus), <span class="built_in">len</span>(vocab)  <span class="comment"># (170580, 28)</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(vocab.idx_to_token)</span><br><span class="line"><span class="comment"># ['&lt;unk&gt;', ' ', 'e', 't', 'a', 'i', 'n', 'o', 's', 'h', 'r', 'd', 'l', 'm', 'u', 'c', 'f', 'w', 'g', 'y', 'p', 'b', 'v', 'k', 'x', 'z', 'j', 'q']</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><p>词元化是一个关键的预处理步骤，它因语言而异，尝试找到另外三种常用的词元化文本的方法</p>
<p>方法一：词级分词：直接以空格和标点作为分隔符，把句子切分成单词，最直观、最传统的方式</p>
<p>依赖空格分割，对中文、日文等无空格语言完全失效，标点、缩写(如<code>H.G.</code>)可能导致歧义，会造成词表巨大，模型容易出现未知词，一般用于入门级语言模型</p>
<p>方法二：子词分词，这是现代 NLP 模型最常用的方式，代表算法有：</p>
<ul>
<li>BPE(Byte Pair Encoding)</li>
<li>WordPiece</li>
<li>SentencePiece</li>
</ul>
<p>把单词拆成更小的、可重复组合的单元(子词 subword)，高效，词表小、覆盖率高且与语言无关</p>
<p>但子词边界不总与语义边界对齐，实现复杂</p>
<p>方法三：中文分词，针对没有空格的语言，必须借助统计或机器学习方法决定词边界，常用工具：</p>
<ul>
<li>jieba(结巴分词)</li>
<li>THULAC(清华大学)</li>
<li>HanLP(多语言自然语言处理库)</li>
</ul>
<p>依赖词典，难以处理新词；不同分词标准会造成语义差异</p>
<h2 id="语言模型和数据集"><a href="#语言模型和数据集" class="headerlink" title="语言模型和数据集"></a>语言模型和数据集</h2><p>在给定文本序列时，语言模型的目标是估计序列的联合概率，一个理想的语言模型能够基于模型本身生成自然文本</p>
<h3 id="学习语言模型"><a href="#学习语言模型" class="headerlink" title="学习语言模型"></a>学习语言模型</h3><p>假设在单词级别对文本数据进行词元化，从基本概率规则开始<br>$$<br>P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t  \mid  x_1, \ldots, x_{t-1}).<br>$$<br>包含了四个单词的一个文本序列的概率是：<br>$$<br>P(\text{deep}, \text{learning}, \text{is}, \text{fun}) =  P(\text{deep}) P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep}, \text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning}, \text{is}).<br>$$<br>为了训练语言模型，需要计算单词的概率，以及在给定前面几个单词后，下一个单词出现的条件概率。这些概率就是语言模型的参数</p>
<p>假设训练数据集是一个大型的文本语料库，单词的概率可以用它在语料中的相对频率来近似计算</p>
<p>例如，单词 “deep” 的概率可以通过它在文本中出现的次数除以所有单词总数来估计<br>$$<br>\hat{P}( \text{deep}) = \frac{n(\text{deep})}{总词数}<br>$$<br>对于频繁出现的词这种方法不错，可以尝试估计<br>$$<br>\hat{P}(\text{learning} \mid \text{deep}) = \frac{n(\text{deep, learning})}{n(\text{deep})}<br>$$<br>其中$n(x)$和$n(x, x’)$分别是单个单词和连续单词对的出现次数</p>
<p>但“deep learning”这样连续词对出现得远比单个词少，因此这种估计在遇到罕见的词组时会不太可靠，因为样本太少，很难得到准确的概率</p>
<p>一种常见的策略是执行某种形式的<strong>拉普拉斯平滑(Laplace smoothing)</strong>，具体方法是在所有计数中添加一个小常量</p>
<p>用$n$表示训练集中的单词总数，用$m$表示词表大小<br>$$<br>\begin{split}\begin{aligned}<br>\hat{P}(x) &amp; = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \\<br>\hat{P}(x’ \mid x) &amp; = \frac{n(x, x’) + \epsilon_2 \hat{P}(x’)}{n(x) + \epsilon_2}, \\<br>\hat{P}(x’’ \mid x,x’) &amp; = \frac{n(x, x’,x’’) + \epsilon_3 \hat{P}(x’’)}{n(x, x’) + \epsilon_3}.<br>\end{aligned}\end{split}<br>$$<br>其中$\epsilon_1,\epsilon_2,\epsilon_3$为超参数，当$\epsilon_1 = 0$时不应用平滑，接近无穷大时$\hat{P}(x)$接近均匀概率分布$1/m$</p>
<p>然而这样的模型很容易变得无效</p>
<ul>
<li>需要存储所有的计数</li>
<li>完全忽略单词的意思</li>
</ul>
<p>因此一个模型如果只是简单地统计先前“看到”的单词序列频率，那么模型面对长单词序列问题肯定是表现不佳的</p>
<h3 id="马尔可夫模型与n元语法"><a href="#马尔可夫模型与n元语法" class="headerlink" title="马尔可夫模型与n元语法"></a>马尔可夫模型与n元语法</h3><p>在语言建模中，若假设$P(x_{t+1} \mid x_t, \ldots, x_1) = P(x_{t+1} \mid x_t)$，则序列满足一阶马尔可夫性质，阶数越高，对应的依赖关系就越长</p>
<p>可以得到不同阶数下的近似形式:<br>$$<br>\begin{split}\begin{aligned}<br>P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2) P(x_3) P(x_4)(零阶)\\<br>P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_2) P(x_4  \mid  x_3)(一阶)\\<br>P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_1, x_2) P(x_4  \mid  x_2, x_3)(二阶)<br>\end{aligned}\end{split}<br>$$<br>阶数越高，模型捕捉到的上下文信息越多，但计算与存储开销也随之增加</p>
<p>通常，涉及一个、两个和三个变量的概率公式分别被称为一元语法(unigram)、二元语法(bigram)和三元语法(trigram)模型</p>
<h3 id="自然语言统计"><a href="#自然语言统计" class="headerlink" title="自然语言统计"></a>自然语言统计</h3><p>根据时光机器数据集构建词表，并打印前10个最常用的(频率最高的)单词</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokens = tokenize(read_time_machine())</span><br><span class="line">corpus = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line] <span class="comment"># 展平tokens</span></span><br><span class="line">vocab = Vocab(corpus)</span><br><span class="line">vocab.token_freqs[:<span class="number">10</span>]</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[('the', 2261),</span><br><span class="line"> ('i', 1267),</span><br><span class="line"> ('and', 1245),</span><br><span class="line"> ('of', 1155),</span><br><span class="line"> ('a', 816),</span><br><span class="line"> ('to', 695),</span><br><span class="line"> ('was', 552),</span><br><span class="line"> ('in', 541),</span><br><span class="line"> ('that', 443),</span><br><span class="line"> ('my', 440)]</span><br></pre></td></tr></tbody></table></figure>

<p>会发现，最多的词并没有意义，这些词通常被称为<strong>停用词(stop words)</strong>，因此可以被过滤掉</p>
<p>还有个明显的问题是词频衰减的速度相当地快</p>
<p>可以画出词频图</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> vocab.token_freqs] <span class="comment"># 把频率单独拉出来</span></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot(freqs)</span><br><span class="line">plt.xlabel(<span class="string">'token: x'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'freq: n(x)'</span>)</span><br><span class="line">plt.xscale(<span class="string">'log'</span>)</span><br><span class="line">plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">plt.grid()</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510301716.png" alt="202510301716" style="zoom: 67%;">

<p>词频以一种明确的方式迅速衰减</p>
<p>将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线</p>
<p>这意味着单词的频率满足<strong>齐普夫定律(Zipf’s law)</strong>，即第$i$个最常用单词的频率$n_i$为<br>$$<br>n_i \propto \frac{1}{i^\alpha}<br>$$<br>等价于<br>$$<br>\log n_i = -\alpha \log i + c,<br>$$<br>所以通过计数统计和平滑来建模单词是不可行的，因为这样会大大高估尾部单词的频率，也就是所谓的不常用单词</p>
<p>对于二元语法</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bigram_tokens = [pair <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">1</span>], corpus[<span class="number">1</span>:])] <span class="comment"># 相邻绑定</span></span><br><span class="line">bigram_vocab = Vocab(bigram_tokens)</span><br><span class="line">bigram_vocab.token_freqs[:<span class="number">10</span>]</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(('of', 'the'), 309),</span><br><span class="line"> (('in', 'the'), 169),</span><br><span class="line"> (('i', 'had'), 130),</span><br><span class="line"> (('i', 'was'), 112),</span><br><span class="line"> (('and', 'the'), 109),</span><br><span class="line"> (('the', 'time'), 102),</span><br><span class="line"> (('it', 'was'), 99),</span><br><span class="line"> (('to', 'the'), 85),</span><br><span class="line"> (('as', 'i'), 78),</span><br><span class="line"> (('of', 'a'), 73)]</span><br></pre></td></tr></tbody></table></figure>

<p>在十个最频繁的词对中，有九个是由两个停用词组成的，只有“the time”涵盖信息</p>
<p>再进一步看看三元语法的频率是否表现出相同的行为方式</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trigram_tokens = [triple <span class="keyword">for</span> triple <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">2</span>], corpus[<span class="number">1</span>:-<span class="number">1</span>], corpus[<span class="number">2</span>:])]</span><br><span class="line">trigram_vocab = Vocab(trigram_tokens)</span><br><span class="line">trigram_vocab.token_freqs[:<span class="number">10</span>]</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(('the', 'time', 'traveller'), 59),</span><br><span class="line"> (('the', 'time', 'machine'), 30),</span><br><span class="line"> (('the', 'medical', 'man'), 24),</span><br><span class="line"> (('it', 'seemed', 'to'), 16),</span><br><span class="line"> (('it', 'was', 'a'), 15),</span><br><span class="line"> (('here', 'and', 'there'), 15),</span><br><span class="line"> (('seemed', 'to', 'me'), 14),</span><br><span class="line"> (('i', 'did', 'not'), 14),</span><br><span class="line"> (('i', 'saw', 'the'), 13),</span><br><span class="line"> (('i', 'began', 'to'), 13)]</span><br></pre></td></tr></tbody></table></figure>

<p>直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510301951.webp" alt="202510301951" style="zoom:67%;">

<p>这张图非常令人振奋！原因有很多：</p>
<ol>
<li>除了一元语法词，单词序列似乎也遵循齐普夫定律，尽管公式中$\alpha$更小(指数大小受序列长度影响)</li>
<li>尽管可能的$n$元组数量理论上非常庞大，但在实际语料中却远小于理论上限，说明自然语言中存在强烈的结构规律性与约束，使得能够用模型有效地进行语言建模</li>
<li>大量的$n$元组几乎从未出现，这使得拉普拉斯平滑无法有效处理这种稀疏性，作为替代，将使用基于深度学习的模型</li>
</ol>
<h3 id="读取长序列数据"><a href="#读取长序列数据" class="headerlink" title="读取长序列数据"></a>读取长序列数据</h3><p>序列数据本质上是连续的，在建模时必须解决其长度不定的问题，当序列过长而无法被模型一次性处理时，通常会将其切分成多个较短的片段，以便模型逐段读取</p>
<p>在使用神经网络训练语言模型时，模型一次只能处理长度固定的小批量序列，需要设计一种方法，随机生成小批量的特征与标签对供模型训练</p>
<p>由于文本序列的长度可以任意，任意长序列可以被划分为若干个长度相同的子序列，每个小批量就由这些子序列组成，并输入模型进行学习</p>
<p>切分序列时的起始偏移量可以自由选择，不同的偏移量会产生不同的子序列划分方式，从而提高数据的多样性与模型的泛化能力</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/timemachine-5gram.webp" alt="timemachine-5gram" style="zoom:80%;">

<p>可以从随机偏移量开始划分序列，以同时获得覆盖性和随机性</p>
<p>有两种策略：<strong>随机采样(random sampling)<strong>和</strong>顺序分区(sequential partitioning)</strong></p>
<h4 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h4><p>在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列</p>
<p>在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</p>
<p>对于语言建模，目标是基于到目前为止看到的词元来预测下一个词元，因此标签是移位了一个词元的原始序列</p>
<p>下面的代码每次可以从数据中随机生成一个小批量，参数<code>batch_size</code>指定了每个小批量中子序列样本的数目，参数<code>num_steps</code>是模型在一次前向传播中看到的时间长度</p>
<blockquote>
<p>并不是预测num_steps长度，都是用前一个预测后一个</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_random</span>(<span class="params">corpus, batch_size, num_steps</span>): <span class="comment"># @save</span></span><br><span class="line">    <span class="string">"""使用随机抽样生成一个小批量子序列"""</span></span><br><span class="line">    <span class="comment"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1</span></span><br><span class="line">    corpus = corpus[random.randint(<span class="number">0</span>, num_steps - <span class="number">1</span>):] <span class="comment"># 随机偏移量起始，但不能超过num_steps</span></span><br><span class="line">    <span class="comment"># 减去1，是因为需要考虑标签</span></span><br><span class="line">    num_subseqs = (<span class="built_in">len</span>(corpus) - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="comment"># 长度为num_steps的子序列的起始索引</span></span><br><span class="line">    initial_indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_subseqs * num_steps, num_steps))</span><br><span class="line">    <span class="comment"># 在随机抽样的迭代过程中，</span></span><br><span class="line">    <span class="comment"># 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span></span><br><span class="line">    random.shuffle(initial_indices) <span class="comment"># 打乱起点</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">data</span>(<span class="params">pos</span>):</span><br><span class="line">        <span class="comment"># 返回从pos位置开始的长度为num_steps的序列</span></span><br><span class="line">        <span class="keyword">return</span> corpus[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    num_batches = num_subseqs // batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, batch_size * num_batches, batch_size):</span><br><span class="line">        initial_indices_per_batch = initial_indices[i : i+batch_size]</span><br><span class="line">        X = [data(j) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        Y = [data(j+<span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch] <span class="comment"># 后移一位</span></span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X), torch.tensor(Y)</span><br></pre></td></tr></tbody></table></figure>

<p>生成一个0到34的序列，批量大小为2，时间步为5，可以生成(35-1)/5 = 6个“特征－标签”子序列对，所以只能有3个小批量</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_seq = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">35</span>))</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_random(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'X: '</span>, X, <span class="string">'\nY:'</span>, Y)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X:  tensor([[25, 26, 27, 28, 29],</span><br><span class="line">        [15, 16, 17, 18, 19]]) </span><br><span class="line">Y: tensor([[26, 27, 28, 29, 30],</span><br><span class="line">        [16, 17, 18, 19, 20]])</span><br><span class="line">X:  tensor([[10, 11, 12, 13, 14],</span><br><span class="line">        [20, 21, 22, 23, 24]]) </span><br><span class="line">Y: tensor([[11, 12, 13, 14, 15],</span><br><span class="line">        [21, 22, 23, 24, 25]])</span><br><span class="line">X:  tensor([[5, 6, 7, 8, 9],</span><br><span class="line">        [0, 1, 2, 3, 4]]) </span><br><span class="line">Y: tensor([[ 6,  7,  8,  9, 10],</span><br><span class="line">        [ 1,  2,  3,  4,  5]])</span><br></pre></td></tr></tbody></table></figure>

<h4 id="顺序分区"><a href="#顺序分区" class="headerlink" title="顺序分区"></a>顺序分区</h4><p>在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列</p>
<p>下面的代码每次可以从数据中随机生成一个小批量，参数<code>batch_size</code>指定了每个小批量中子序列样本的数目，参数<code>num_steps</code>是每次送入模型的时间步长度</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_sequential</span>(<span class="params">corpus, batch_size, num_steps</span>):</span><br><span class="line">    <span class="string">"""使用顺序分区生成一个小批量子序列"""</span></span><br><span class="line">    <span class="comment"># 从随机偏移量开始划分序列</span></span><br><span class="line">    offset = random.randint(<span class="number">0</span>,num_steps) <span class="comment"># 这里允许偏移等于一个完整的时间窗口</span></span><br><span class="line">    num_tokens = ((<span class="built_in">len</span>(corpus) - offset - <span class="number">1</span>) // batch_size) * batch_size</span><br><span class="line">    Xs = torch.tensor(corpus[offset: offset + num_tokens])</span><br><span class="line">    Ys = torch.tensor(corpus[offset + <span class="number">1</span>: offset + num_tokens + <span class="number">1</span>])</span><br><span class="line">    Xs, Ys = Xs.reshape(batch_size, -<span class="number">1</span>), Ys.reshape(batch_size, -<span class="number">1</span>)</span><br><span class="line">    num_batches = Xs.shape[<span class="number">1</span>] // num_steps  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_steps * num_batches, num_steps):</span><br><span class="line">        X = Xs[:, i: i + num_steps]</span><br><span class="line">        Y = Ys[:, i: i + num_steps]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></tbody></table></figure>

<p>基于相同的设置，通过顺序分区读取每个小批量的子序列的特征<code>X</code>和标签<code>Y</code></p>
<p>将它们打印出来可以发现：迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_sequential(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'X: '</span>, X, <span class="string">'\nY:'</span>, Y)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X:  tensor([[ 4,  5,  6,  7,  8],</span><br><span class="line">        [19, 20, 21, 22, 23]]) </span><br><span class="line">Y: tensor([[ 5,  6,  7,  8,  9],</span><br><span class="line">        [20, 21, 22, 23, 24]])</span><br><span class="line">X:  tensor([[ 9, 10, 11, 12, 13],</span><br><span class="line">        [24, 25, 26, 27, 28]]) </span><br><span class="line">Y: tensor([[10, 11, 12, 13, 14],</span><br><span class="line">        [25, 26, 27, 28, 29]])</span><br><span class="line">X:  tensor([[14, 15, 16, 17, 18],</span><br><span class="line">        [29, 30, 31, 32, 33]]) </span><br><span class="line">Y: tensor([[15, 16, 17, 18, 19],</span><br><span class="line">        [30, 31, 32, 33, 34]])</span><br></pre></td></tr></tbody></table></figure>

<p>将上面的两个采样函数包装到一个类中， 以便稍后可以将其用作数据迭代器</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SeqDataLoader</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""加载序列数据的迭代器"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, num_steps, use_random_iter, max_tokens</span>):</span><br><span class="line">        <span class="keyword">if</span> use_random_iter:</span><br><span class="line">            <span class="variable language_">self</span>.data_iter_fn = seq_data_iter_random</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.data_iter_fn = seq_data_iter_sequential</span><br><span class="line">        <span class="variable language_">self</span>.corpus, <span class="variable language_">self</span>.vocab = load_corpus_time_machine(max_tokens)</span><br><span class="line">        <span class="variable language_">self</span>.batch_size, <span class="variable language_">self</span>.num_steps = batch_size, num_steps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data_iter_fn(<span class="variable language_">self</span>.corpus, <span class="variable language_">self</span>.batch_size, <span class="variable language_">self</span>.num_steps)</span><br></pre></td></tr></tbody></table></figure>

<p>定义了一个函数<code>load_data_time_machine</code>，它同时返回数据迭代器和词表，因此可以与其他带有<code>load_data</code>前缀的函数类似地使用</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_time_machine</span>(<span class="params">batch_size, num_steps,  <span class="comment">#@save</span></span></span><br><span class="line"><span class="params">                           use_random_iter=<span class="literal">False</span>, max_tokens=<span class="number">10000</span></span>):</span><br><span class="line">    <span class="string">"""返回时光机器数据集的迭代器和词表"""</span></span><br><span class="line">    data_iter = SeqDataLoader(</span><br><span class="line">        batch_size, num_steps, use_random_iter, max_tokens)</span><br><span class="line">    <span class="keyword">return</span> data_iter, data_iter.vocab</span><br></pre></td></tr></tbody></table></figure>

<h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p>在n元语法模型中,假设单词$x_t$的出现仅依赖于前面$n-1$个单词<br>$$<br>P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid x_{t-1},\cdots ,x_{t-n+1}),<br>$$<br>如果希望模型考虑更长的上下文，就必须增大$n$</p>
<p>这样虽然能捕捉更复杂的语言结构，但模型的参数量会急剧增加，因为词表需要存储$\mid \mathcal{V}\mid ^n$个概率值，当词表很大时，这几乎无法计算与存储</p>
<p>为了解决这个问题，可以用一个<strong>隐变量模型</strong>进行近似<br>$$<br>P(x_t \mid x_{t-1}, \ldots, x_1) \approx P(x_t \mid h_{t-1})<br>$$<br>其中$h_{t-1}$是<strong>隐状态(hidden state)</strong>，也称为<strong>隐藏变量(hidden variable)</strong>，用于总结截至时间步$t-1$的全部上下文信息</p>
<p>在每一步，模型都会更新这个隐藏状态$h_t = f(x_{t}, h_{t-1})$</p>
<p>从而以固定大小的参数捕捉潜在的语言依赖关系</p>
<p><font color="DarkViolet">隐藏层和隐状态指的是两个截然不同的概念</font>，隐藏层是在从输入到输出的路径上(以观测角度来理解)的隐藏的层，而隐状态则是在给定步骤所做的任何事情的输入，并且这些状态只能通过先前时间步的数据来计算</p>
<p>**循环神经网络(recurrent neural networks，RNNs)**是具有隐状态的神经网络</p>
<h3 id="隐状态"><a href="#隐状态" class="headerlink" title="隐状态"></a>隐状态</h3><p>假设在时间步$t$有小批量输入$\mathbf{X}_t \in \mathbb{R}^{n \times d}$，用$\mathbf{H}_t \in \mathbb{R}^{n \times h}$示时间步的隐藏变量</p>
<p>与多层感知机不同的是，在这里保存了前一个时间步的隐藏变量$\mathbf H_{t-1}$，并引入了一个新的权重参数$\mathbf W_{hh} \in \mathbb R^{h \times h}$，来描述如何在当前时间步中使用前一个时间步的隐藏变量</p>
<p>具体地说，当前时间步隐藏变量由当前时间步的输入与前一个时间步的隐藏变量一起计算得出：<br>$$<br>\mathbf H_t = \phi(\mathbf X_t \mathbf W_{xh} + \mathbf H_{t-1} \mathbf W_{hh}  + \mathbf b_h)<br>$$<br>多添加了一项$\mathbf H_{t-1} \mathbf W_{hh}$，这些变量捕获并保留了序列直到其当前时间步的历史信息，这样的隐藏变量被称为<strong>隐状态(hidden state)</strong></p>
<p>隐状态使用的定义与前一个时间步中使用的定义相同，因此计算是循环的，在循环神经网络中执行循环计算的层称为<strong>循环层(recurrent layer)</strong></p>
<p>输出层的输出类似于多层感知机中的计算<br>$$<br>\mathbf O_t = \mathbf H_t \mathbf W_{hq} + \mathbf b_q.<br>$$<br>即使在不同的时间步，循环神经网络也总是使用同样的模型参数，因此循环神经网络的参数开销不会随着时间步的增加而增加</p>
<p>下图展示了循环神经网络在三个相邻时间步的计算逻辑</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/rnn.jpg" alt="rnn" style="zoom:80%;">

<p>在任意时间步隐状态的计算可以被视为：</p>
<ol>
<li>拼接当前时间步$t$的输入$\mathbf X_t$和前一时间步$t-1$的隐状态$\mathbf H_{t-1}$</li>
<li>将拼接的结果送入带有激活函数$\phi$的全连接层，全连接层的输出是当前时间步$t$的隐状态$\mathbf H_{t}$</li>
</ol>
<p>隐状态中$\mathbf X_t \mathbf W_{xh} + \mathbf H_{t-1} \mathbf W_{hh}$的计算，相当于$\mathbf X_t$和$\mathbf H_{t-1}$的拼接与$\mathbf W_{xh}$和$\mathbf W_{hh}$的拼接的矩阵乘法</p>
<p>用一段简单代码示意</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X, W_xh = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">1</span>)), torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">H, W_hh = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">4</span>)), torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">torch.matmul(X, W_xh) + torch.matmul(H, W_hh)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-1.1493,  6.7741, -5.4517,  0.2577],</span><br><span class="line">        [ 2.3512,  1.1124,  0.5166,  0.8442],</span><br><span class="line">        [ 2.5654,  1.5250, -0.3778,  1.3976]])</span><br></pre></td></tr></tbody></table></figure>

<p>沿列(轴1)拼接矩阵<code>X</code>和<code>H</code>，沿行(轴0)拼接矩阵<code>W_xh</code>和<code>W_hh</code></p>
<p>这两个拼接分别产生形状(3,5)和形状(5,4)的矩阵，将这两个拼接的矩阵相乘，得到与上面相同形状(3,4)的输出矩阵</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-1.1493,  6.7741, -5.4517,  0.2577],</span><br><span class="line">        [ 2.3512,  1.1124,  0.5166,  0.8442],</span><br><span class="line">        [ 2.5654,  1.5250, -0.3778,  1.3976]])</span><br></pre></td></tr></tbody></table></figure>

<h3 id="字符级语言模型"><a href="#字符级语言模型" class="headerlink" title="字符级语言模型"></a>字符级语言模型</h3><p>目标是根据过去的和当前的词元预测下一个词元，因此将原始序列移位一个词元作为标签</p>
<p>Bengio等人首先提出使用神经网络进行语言建模 (Bengio <em>et al.</em>, 2003)</p>
<p>设小批量大小为1，批量中的文本序列为“machine”，为了简化后续部分的训练，考虑使用<strong>字符级语言模型</strong>， 将文本词元化为字符而不是单词</p>
<p>下图演示了如何通过基于字符级语言建模的循环神经网络，使用当前的和先前的字符预测下一个字符</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/rnn-train.jpg" alt="rnn-train" style="zoom:80%;">

<p>输入序列和标签序列分别为“machin”和“achine”</p>
<p>在训练过程中，对每个时间步的输出层的输出进行softmax操作，然后利用交叉熵损失计算模型输出和标签之间的误差</p>
<p>在实践中使用的批量大小$n&gt;1$，每个词元都由一个$d$维向量表示，在时间步$t$输入$X_t$将是一个$n\times d$矩阵</p>
<h3 id="困惑度-Perplexity"><a href="#困惑度-Perplexity" class="headerlink" title="困惑度(Perplexity)"></a>困惑度(Perplexity)</h3><p>可以通过计算序列的似然概率来度量模型的质量，然而这是一个难以理解、难以比较的数字，因为较短的序列比较长的序列更有可能出现</p>
<p>一个更好的语言模型应该能更准确地预测下一个词元，因此它应该允许压缩序列时花费更少的比特</p>
<p>可以通过一个序列中所有的$n$个词元的交叉熵损失的平均值来衡量<br>$$<br>\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1)<br>$$<br>这使得不同长度的文档的性能具有了可比性</p>
<p>但是自然语言处理的科学家更喜欢使用一个叫做**困惑度(perplexity)**的量，是交叉熵损失的指数<br>$$<br>\exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right).<br>$$<br>困惑度可以理解为模型在预测下一个词元时，实际有效选择数量的调和平均数，值越小，模型越自信</p>
<ul>
<li>理想情况：模型总能完美预测正确词元，模型的困惑度为1，没有任何困惑</li>
<li>最坏情况：模型总是把正确词元的概率估计为0，困惑度趋于无穷，模型失败</li>
<li>基线(均匀分布)：如果模型认为所有词元的概率相同，困惑度等于词表大小，相当于盲猜，因此这种方式提供了一个重要的上限$\mid \mathcal{V}\mid$，而任何实际模型都必须超越这个上限</li>
</ul>
<h3 id="思考题-1"><a href="#思考题-1" class="headerlink" title="思考题"></a>思考题</h3><ol>
<li><p>如果使用循环神经网络来预测文本序列中的下一个字符，那么任意输出所需的维度是多少？</p>
<p>模型的输出是对所有可能字符的概率分布，所以输出的维度等于词表中字符的数量</p>
</li>
<li><p>如果基于一个长序列进行反向传播，梯度会发生什么状况？</p>
<p>当基于一个很长的序列进行反向传播时，梯度会在时间维度上反复被权重矩阵和激活函数的导数相乘，很可能会出现梯度消失或梯度爆炸，因为如果权重稍小，连乘就容易湮灭，如果权重稍大，容易爆炸，需要 LSTM、GRU 等结构来稳定训练</p>
</li>
</ol>
<h2 id="循环神经网络底层实现"><a href="#循环神经网络底层实现" class="headerlink" title="循环神经网络底层实现"></a>循环神经网络底层实现</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h3><p>将每个索引映射为相互不同的单位向量，假如词元的索引是整数$i$，创建长度为N的全0向量，并在$i$处设为1，获得独热向量，在0和2处创建独热向量举例：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F.one_hot(torch.tensor([<span class="number">0</span>, <span class="number">2</span>]), <span class="built_in">len</span>(vocab))</span><br></pre></td></tr></tbody></table></figure>

<p>每次采样的小批量数据形状为(batch_size，num_steps)，<code>one_hot</code>函数将小批量数据转换成三维张量，张量的最后一个维度等于词表大小(vocab_size)</p>
<p>经常转换输入的维度，获得形状为<code>(num_steps，batch_size，vocab_size)</code>的输出，将能够更方便地通过最外层的维度，一步一步地更新小批量数据的隐状态</p>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>隐藏单元数<code>num_hiddens</code>是一个可调的超参数，当训练语言模型时，输入和输出来自相同的词表，具有相同的维度，即词表的大小</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = normal((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = normal((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = torch.zeros(num_hiddens, device=device)</span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></tbody></table></figure>

<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>为了定义循环神经网络模型，首先需要一个<code>init_rnn_state</code>函数在初始化时返回隐状态</p>
<p>函数的返回值是一个张量，全0填充，形状为<code>(batch_size，num_hiddens)</code></p>
<p>隐状态可能包含多个变量，而使用元组可以更容易地处理些</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_rnn_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br><span class="line">	<span class="comment"># 加了一个逗号，返回值就变成了一个单元素元组</span></span><br></pre></td></tr></tbody></table></figure>

<p>下面的<code>rnn</code>函数定义了如何在一个时间步内计算隐状态和输出</p>
<p>循环神经网络模型通过<code>inputs</code>最外层的维度实现循环，以便逐时间步更新小批量数据的隐状态<code>H</code></p>
<p>这里使用<code>tanh</code>函数作为激活函数，平均值为0</p>
<blockquote>
<p>如果用ReLU，虽然缓解了梯度消失问题，却让梯度爆炸更容易出现，更需要梯度裁剪</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rnn</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    <span class="comment"># inputs: (时间步数, 批量大小, 词表大小)</span></span><br><span class="line">    <span class="comment"># state: 隐藏状态 (H,)</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="comment"># X：(batch_size, vocab_size)</span></span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.mm(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br><span class="line">	<span class="comment"># torch.cat(outputs, dim=0) 把小张量在时间维拼起来</span></span><br><span class="line">	<span class="comment"># (H,)返回最终隐藏状态，作为下一个序列的初始状态</span></span><br></pre></td></tr></tbody></table></figure>

<p>对应<br>$$<br>\mathbf H_t = \phi(\mathbf X_t \mathbf W_{xh} + \mathbf H_{t-1} \mathbf W_{hh}  + \mathbf b_h)\\<br>\mathbf O_t = \mathbf H_t \mathbf W_{hq} + \mathbf b_q.<br>$$<br>定义了所有需要的函数之后，接下来创建一个类来包装这些函数，并存储从零开始实现的循环神经网络模型的参数</p>
<figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModelScratch</span>: <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""从零开始实现的循环神经网络模型"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, device,</span></span><br><span class="line"><span class="params">                 get_params, init_state, forward_fn</span>):</span><br><span class="line">        <span class="variable language_">self</span>.vocab_size, <span class="variable language_">self</span>.num_hiddens = vocab_size, num_hiddens</span><br><span class="line">        <span class="variable language_">self</span>.params = get_params(vocab_size, num_hiddens, device)</span><br><span class="line">        <span class="variable language_">self</span>.init_state, <span class="variable language_">self</span>.forward_fn = init_state, forward_fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X, state</span>): <span class="comment"># 前向传播方法</span></span><br><span class="line">        <span class="comment"># 先转置变成(num_steps, batch_size)，方便按时间步迭代</span></span><br><span class="line">        X = F.one_hot(X.T, <span class="variable language_">self</span>.vocab_size).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">        <span class="comment"># 传入forward_fn函数</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.forward_fn(X, state, <span class="variable language_">self</span>.params)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, batch_size, device</span>):</span><br><span class="line">        <span class="comment"># 初始化隐藏状态</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.init_state(batch_size, <span class="variable language_">self</span>.num_hiddens, device)</span><br></pre></td></tr></tbody></table></figure>

<p>检查输出是否具有正确的形状，例如，隐状态的维数是否保持不变</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">512</span></span><br><span class="line">X = torch.arange(<span class="number">10</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">net = RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, try_gpu(), get_params,</span><br><span class="line">                      init_rnn_state, rnn)</span><br><span class="line">state = net.begin_state(X.shape[<span class="number">0</span>], try_gpu())</span><br><span class="line">Y, new_state = net(X.to(try_gpu()), state)</span><br><span class="line">Y.shape, <span class="built_in">len</span>(new_state), new_state[<span class="number">0</span>].shape</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([10, 28]), 1, torch.Size([2, 512]))</span><br></pre></td></tr></tbody></table></figure>

<p><font color="DarkViolet">输出形状是(时间步数×批量大小，词表大小)，隐状态形状保持不变(批量大小，隐藏单元数)</font></p>
<h3 id="预测-1"><a href="#预测-1" class="headerlink" title="预测"></a>预测</h3><p>首先定义预测函数来生成<code>prefix</code>之后的新字符，其中的<code>prefix</code>是一个用户提供的包含多个字符的字符串，在循环遍历<code>prefix</code>中的开始字符时，不断地将隐状态传递到下一个时间步，但是不生成任何输出</p>
<p>这被称为**预热(warm-up)**期，在此期间模型会自我更新，但不会进行预测</p>
<p>预热期结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch8</span>(<span class="params">prefix, num_preds, net, vocab, device</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""在prefix后面生成新字符"""</span></span><br><span class="line">    state = net.begin_state(batch_size=<span class="number">1</span>, device=device) <span class="comment"># 一次只生成一个序列</span></span><br><span class="line">    outputs = [vocab[prefix[<span class="number">0</span>]]] <span class="comment"># 把前缀的第一个字符转成索引</span></span><br><span class="line">    get_input = <span class="keyword">lambda</span>: torch.tensor([outputs[-<span class="number">1</span>]], device=device).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 让模型根据整个前缀更新隐藏状态，使其“理解上下文”</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> prefix[<span class="number">1</span>:]:  <span class="comment"># 预热期</span></span><br><span class="line">        _, state = net(get_input(), state)</span><br><span class="line">        outputs.append(vocab[y])</span><br><span class="line">    <span class="comment"># 正式预测阶段</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_preds):  <span class="comment"># 预测num_preds步</span></span><br><span class="line">        y, state = net(get_input(), state)</span><br><span class="line">        outputs.append(<span class="built_in">int</span>(y.argmax(dim=<span class="number">1</span>).reshape(<span class="number">1</span>)))</span><br><span class="line">    <span class="comment"># 遍历 outputs 里的所有索引用 vocab.idx_to_token 查回对应字符拼接</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join([vocab.idx_to_token[i] <span class="keyword">for</span> i <span class="keyword">in</span> outputs])</span><br></pre></td></tr></tbody></table></figure>

<p>将前缀指定为<code>time traveller</code>，并基于这个前缀生成10个后续字符</p>
<p>还没有训练网络，它会生成荒谬的预测结果</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_ch8(<span class="string">'time traveller '</span>, <span class="number">10</span>, net, vocab, try_gpu())</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">'time traveller pycscscscs'</span><br></pre></td></tr></tbody></table></figure>

<h3 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h3><p>对于长度为$T$序列，在迭代中计算这$T$个时间步上的梯度，将会在反向传播过程中产生长度为$\mathcal{O}(T)$的矩阵乘法链，为了避免$T$较大时导致不稳定，需要额外的方式来支持稳定训练</p>
<p>如果假设目标函数$f$表现良好，在常数下是<strong>利普希茨连续的(Lipschitz continuous)</strong>，对于任意$x,y$有：<br>$$<br>|f(\mathbf{x}) - f(\mathbf{y})| \leq L \mid\mid\mathbf{x} - \mathbf{y}\mid\mid<br>$$<br>这时可以安全假设<br>$$<br>|f(\mathbf{x}) - f(\mathbf{x} - \eta\mathbf{g})| \leq L \eta||\mathbf{g}||<br>$$<br>这意味着不会观察到超过$L \eta ||\mathbf{g}||$的变化，这限制了变化大小</p>
<p>有时梯度可能很大，从而优化算法可能无法收敛，可以通过降低学习率来解决这个问题，但这种情况很少发生，一直使用较小的学习率就会让训练速度变得很慢</p>
<p>一个流行的替代方案是通过将梯度$\mathbf{g}$投影回给定半径$\theta$的球来裁剪梯度<br>$$<br>\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{||\mathbf{g}||}\right) \mathbf{g}.<br>$$<br>这样做梯度的范数永远不会大于$\theta$，更新后的梯度方向与原始梯度保持一致，并且通过限制每个小批量对参数更新的影响，模型的训练过程会更加稳定</p>
<p>梯度裁剪提供了一个快速修复梯度爆炸的方法，虽然它并不能完全解决问题，但它是众多有效的技术之一</p>
<p>定义一个函数来裁剪模型的梯度</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">grad_clipping</span>(<span class="params">net, theta</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""裁剪梯度"""</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        params = net.params</span><br><span class="line">    norm = torch.sqrt(<span class="built_in">sum</span>(torch.<span class="built_in">sum</span>((p.grad ** <span class="number">2</span>)) <span class="keyword">for</span> p <span class="keyword">in</span> params))</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><p>与之前的训练方式有所不同</p>
<ol>
<li>序列数据的不同采样方法(随机采样和顺序分区)将导致隐状态初始化的差异</li>
<li>在更新模型参数之前裁剪梯度，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散</li>
<li>用困惑度来评价模型</li>
</ol>
<p>在顺序分区采样中，只在每个迭代周期开始时初始化隐藏状态，因为下一个小批量与上一个在时间上是连续的，所以上一个小批量最后一个时间步的隐藏状态会被用作下一个小批量的初始状态</p>
<p>但这样在任何一点隐状态的计算都依赖于同一迭代周期中前面所有的小批量数据，这使得梯度计算变得复杂</p>
<p>为了降低计算难度，通常在处理每个小批量数据前切断梯度的反向传播，让隐藏状态的梯度计算仅限于当前小批量的时间范围内</p>
<p>而当使用随机采样时，由于每个小批量样本之间没有时间连续性，必须在每次迭代开始时重新初始化隐藏状态</p>
<p><code>updater</code>是更新模型参数的常用函数，既可以是自定义函数，也可以是深度学习框架中内置的优化函数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch8</span>(<span class="params">net, train_iter, loss, updater, device, use_random_iter</span>):</span><br><span class="line">    <span class="string">"""训练网络一个迭代周期(定义见第8章)"""</span></span><br><span class="line">    state, timer = <span class="literal">None</span>, Timer()</span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)  <span class="comment"># 训练损失之和,词元数量</span></span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> use_random_iter:</span><br><span class="line">            <span class="comment"># 在第一次迭代或使用随机抽样时初始化state</span></span><br><span class="line">            state = net.begin_state(batch_size=X.shape[<span class="number">0</span>], device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(state, <span class="built_in">tuple</span>):</span><br><span class="line">                <span class="comment"># state对于nn.GRU是个张量</span></span><br><span class="line">                state.detach_()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">        y = Y.T.reshape(-<span class="number">1</span>)</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat, state = net(X, state)</span><br><span class="line">        l = loss(y_hat, y.long()).mean()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 因为已经调用了mean函数</span></span><br><span class="line">            updater(batch_size=<span class="number">1</span>)</span><br><span class="line">        metric.add(l * y.numel(), y.numel())</span><br><span class="line">    <span class="keyword">return</span> math.exp(metric[<span class="number">0</span>] / metric[<span class="number">1</span>]), metric[<span class="number">1</span>] / timer.stop()</span><br></pre></td></tr></tbody></table></figure>

<p>循环神经网络模型的训练函数既支持从零开始实现</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch8</span>(<span class="params">net, train_iter, vocab, lr, num_epochs, device,</span></span><br><span class="line"><span class="params">              use_random_iter=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">"""训练模型(定义见第8章)"""</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = Animator(xlabel=<span class="string">'epoch'</span>, ylabel=<span class="string">'perplexity'</span>,</span><br><span class="line">                            legend=[<span class="string">'train'</span>], xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        updater = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        updater = <span class="keyword">lambda</span> batch_size: sgd(net.params, lr, batch_size)</span><br><span class="line">    predict = <span class="keyword">lambda</span> prefix: predict_ch8(prefix, <span class="number">50</span>, net, vocab, device)</span><br><span class="line">    <span class="comment"># 训练和预测</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        ppl, speed = train_epoch_ch8(</span><br><span class="line">            net, train_iter, loss, updater, device, use_random_iter)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(predict(<span class="string">'time traveller'</span>))</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, [ppl])</span><br><span class="line">    animator.show()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'困惑度 <span class="subst">{ppl:<span class="number">.1</span>f}</span>, <span class="subst">{speed:<span class="number">.1</span>f}</span> 词元/秒 <span class="subst">{<span class="built_in">str</span>(device)}</span>'</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">'time traveller'</span>))</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">'traveller'</span>))</span><br></pre></td></tr></tbody></table></figure>

<p>训练循环神经网络模型，因为在数据集中只使用了10000个词元，所以模型需要更多的迭代周期来更好地收敛</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510302311.png" alt="202510302311" style="zoom:80%;">

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">困惑度 1.0, 81491.4 词元/秒 cpu</span><br><span class="line">time traveller with a slight accession ofcheerfulness really thi</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></tbody></table></figure>

<p>检查一下使用随机抽样方法的结果</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/image-20251030231642428.png" alt="image-20251030231642428" style="zoom:80%;">

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">困惑度 1.4, 81358.9 词元/秒 cpu</span><br><span class="line">time traveller smiled abe y ut to she ot soee pincandescendlight</span><br><span class="line">travellerit s against reason said filbywhat for thishing to</span><br></pre></td></tr></tbody></table></figure>

<p>输出结果都很奇怪</p>
<h2 id="循环神经网络的简洁实现"><a href="#循环神经网络的简洁实现" class="headerlink" title="循环神经网络的简洁实现"></a>循环神经网络的简洁实现</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>高级API提供了循环神经网络的实现，构造一个具有256个隐藏单元的单隐藏层的循环神经网络层<code>rnn_layer</code></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">rnn_layer = nn.RNN(<span class="built_in">len</span>(vocab), num_hiddens)</span><br></pre></td></tr></tbody></table></figure>

<p>使用张量来初始化隐状态，它的形状是(隐藏层数，批量大小，隐藏单元数)</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">state = torch.zeros((<span class="number">1</span>, batch_size, num_hiddens))</span><br><span class="line">state.shape  <span class="comment"># torch.Size([1, 32, 256])</span></span><br></pre></td></tr></tbody></table></figure>

<p>通过一个隐状态和一个输入，就可以用更新后的隐状态计算输出</p>
<p><code>rnn_layer</code>的“输出”(<code>Y</code>)不涉及输出层的计算：它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入</p>
<table>
<thead>
<tr>
<th>输出</th>
<th>含义</th>
<th>形状</th>
</tr>
</thead>
<tbody><tr>
<td><code>Y</code></td>
<td>每个时间步的输出序列</td>
<td><code>(num_steps, batch_size, num_hiddens)</code></td>
</tr>
<tr>
<td><code>state_new</code></td>
<td>最后一个时间步的隐藏状态</td>
<td><code>(num_layers, batch_size, num_hiddens)</code></td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(num_steps, batch_size, <span class="built_in">len</span>(vocab))) <span class="comment"># 时间步数，样本数，词表大小</span></span><br><span class="line">Y, state_new = rnn_layer(X, state)</span><br><span class="line">Y.shape, state_new.shape</span><br><span class="line"><span class="comment"># (torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))</span></span><br></pre></td></tr></tbody></table></figure>

<p>为一个完整的循环神经网络模型定义了一个<code>RNNModel</code>类，注意<code>rnn_layer</code>只包含隐藏的循环层，还需要创建一个单独的输出层</p>
<table>
<thead>
<tr>
<th>模块</th>
<th>功能</th>
<th>输入形状</th>
<th>输出形状</th>
</tr>
</thead>
<tbody><tr>
<td>One-hot 编码</td>
<td>把索引变成向量</td>
<td><code>(num_steps, batch_size)</code></td>
<td><code>(num_steps, batch_size, vocab_size)</code></td>
</tr>
<tr>
<td>RNN 层</td>
<td>提取时序特征</td>
<td>上一步输出</td>
<td><code>(num_steps, batch_size, num_hiddens)</code></td>
</tr>
<tr>
<td>Linear 层</td>
<td>预测下一个词</td>
<td><code>(num_steps * batch_size, num_hiddens)</code></td>
<td><code>(num_steps * batch_size, vocab_size)</code></td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModel</span>(nn.Module):</span><br><span class="line">    <span class="string">"""循环神经网络模型"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs) <span class="comment"># 允许传入额外参数，灵活扩展</span></span><br><span class="line">        <span class="variable language_">self</span>.rnn = rnn_layer <span class="comment"># 传入已经定义好的循环层</span></span><br><span class="line">        <span class="variable language_">self</span>.vocab_size = vocab_size <span class="comment"># 词表大小(用于输出层维度)</span></span><br><span class="line">        <span class="variable language_">self</span>.num_hiddens = <span class="variable language_">self</span>.rnn.hidden_size <span class="comment"># 隐藏层的维度</span></span><br><span class="line">        <span class="comment"># 判断是否双向，与输出维度有关</span></span><br><span class="line">        <span class="comment"># RNN 负责“记忆”，Linear 负责“说出预测结果”</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.rnn.bidirectional:</span><br><span class="line">            <span class="variable language_">self</span>.num_directions = <span class="number">1</span></span><br><span class="line">            <span class="variable language_">self</span>.linear = nn.Linear(<span class="variable language_">self</span>.num_hiddens, <span class="variable language_">self</span>.vocab_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.num_directions = <span class="number">2</span></span><br><span class="line">            <span class="variable language_">self</span>.linear = nn.Linear(<span class="variable language_">self</span>.num_hiddens * <span class="number">2</span>, <span class="variable language_">self</span>.vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, state</span>):</span><br><span class="line">        <span class="comment"># 输入 inputs 的形状是 (batch_size, num_steps)</span></span><br><span class="line">        X = F.one_hot(inputs.T.long(), <span class="variable language_">self</span>.vocab_size)</span><br><span class="line">        X = X.to(torch.float32)</span><br><span class="line">        Y, state = <span class="variable language_">self</span>.rnn(X, state)</span><br><span class="line">        <span class="comment"># Y：每个时间步的输出(num_steps, batch_size, num_hiddens * num_directions)</span></span><br><span class="line">        <span class="comment"># 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span></span><br><span class="line">        <span class="comment"># 输出形状是(时间步数*批量大小,词表大小)。</span></span><br><span class="line">        output = <span class="variable language_">self</span>.linear(Y.reshape((-<span class="number">1</span>, Y.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, device, batch_size=<span class="number">1</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(<span class="variable language_">self</span>.rnn, nn.LSTM):</span><br><span class="line">            <span class="comment"># nn.GRU以张量作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span>  torch.zeros((<span class="variable language_">self</span>.num_directions * <span class="variable language_">self</span>.rnn.num_layers,</span><br><span class="line">                                 batch_size, <span class="variable language_">self</span>.num_hiddens),</span><br><span class="line">                                device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># nn.LSTM以元组作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span> (torch.zeros((</span><br><span class="line">                <span class="variable language_">self</span>.num_directions * <span class="variable language_">self</span>.rnn.num_layers,</span><br><span class="line">                batch_size, <span class="variable language_">self</span>.num_hiddens), device=device),</span><br><span class="line">                    torch.zeros((</span><br><span class="line">                        <span class="variable language_">self</span>.num_directions * <span class="variable language_">self</span>.rnn.num_layers,</span><br><span class="line">                        batch_size, <span class="variable language_">self</span>.num_hiddens), device=device))</span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练与预测"><a href="#训练与预测" class="headerlink" title="训练与预测"></a>训练与预测</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = try_gpu()</span><br><span class="line">net = RNNModel(rnn_layer, vocab_size=<span class="built_in">len</span>(vocab))</span><br><span class="line">net = net.to(device)</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">train_ch8(net, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/image-20251030233533524.png" alt="image-20251030233533524" style="zoom:80%;">

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">困惑度 1.3, 196616.9 词元/秒 cpu</span><br><span class="line">time traveller but now you begin to seethe object of my investig</span><br><span class="line">traveller pores ththo dimensions are pussing ain time ascon</span><br></pre></td></tr></tbody></table></figure>

<p>与刚刚自定义的随机抽样方法相比，由于深度学习框架的高级API对代码进行了更多的优化，该模型在较短的时间内达到了较低的困惑度</p>
<h2 id="通过时间反向传播"><a href="#通过时间反向传播" class="headerlink" title="通过时间反向传播"></a>通过时间反向传播</h2><p>RNN的训练基于<strong>时间反向传播(backpropagation through time，BPTT)</strong>(Werbos, 1990)，利用链式法则计算序列中每个时间步的梯度</p>
<p>要求将循环神经网络的计算图一次展开一个时间步，以获得模型变量和参数之间的依赖关系</p>
<p>但在长序列中，梯度会出现爆炸或消失，因此在实践中通过截断传播与梯度裁剪来保证模型稳定收敛</p>
<h3 id="梯度分析"><a href="#梯度分析" class="headerlink" title="梯度分析"></a>梯度分析</h3><p>输入和隐状态可以拼接后与隐藏层中的一个权重变量相乘，分别使用$w_h$和$w_o$来表示隐藏层和输出层的权重，每个时间步的隐状态和输出可以写为：<br>$$<br>\begin{split}\begin{aligned}h_t &amp;= f(x_t, h_{t-1}, w_h),\\o_t &amp;= g(h_t, w_o),\end{aligned}\end{split}<br>$$<br>有一个链${\ldots, (x_{t-1}, h_{t-1}, o_{t-1}), (x_{t}, h_{t}, o_t), \ldots}$通过循环计算彼此依赖，前向传播相当简单，一次一个时间步的遍历三元组，然后通过一个目标函数在所有$T$个时间步内评估输出和对应的标签之间的差异<br>$$<br>L(x_1, \ldots, x_T, y_1, \ldots, y_T, w_h, w_o) = \frac{1}{T}\sum_{t=1}^T l(y_t, o_t).<br>$$<br>对于反向传播需要根据链式法则：<br>$$<br>\begin{split}\begin{aligned}\frac{\partial L}{\partial w_h}  &amp; = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial w_h}  \\&amp; = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial o_t} \frac{\partial g(h_t, w_o)}{\partial h_t}  \frac{\partial h_t}{\partial w_h}.\end{aligned}\end{split}<br>$$<br>乘积的第一项和第二项很容易计算，而第三项是困难所在，需要循环地计算参数$w_h$对$h_t$的影响</p>
<p>$h_t$既依赖于$h_{t-1}$又依赖于$w_h$，$h_{t-1}$的计算也依赖于$w_h$，使用链式法则产生：<br>$$<br>\frac{\partial h_t}{\partial w_h}= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h} +\frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h}.<br>$$<br>假设有三个序列${a_{t}},{b_{t}},{c_{t}}$，当$t=1,2,\ldots$时，序列满足$a_{0}=0$且$a_{t}=b_{t}+c_{t}a_{t-1}$，对于$t\geq 1$就很容易得出：<br>$$<br>a_{t}=b_{t}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t}c_{j}\right)b_{i}.<br>$$<br>基于下列公式替换<br>$$<br>\begin{split}\begin{aligned}a_t &amp;= \frac{\partial h_t}{\partial w_h},\\<br>b_t &amp;= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h}, \\<br>c_t &amp;= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}},\end{aligned}\end{split}<br>$$<br>所以刚刚那个复杂的链式法则可以转换为<br>$$<br>\frac{\partial h_t}{\partial w_h}=\frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t} \frac{\partial f(x_{j},h_{j-1},w_h)}{\partial h_{j-1}} \right) \frac{\partial f(x_{i},h_{i-1},w_h)}{\partial w_h}.<br>$$<br>虽然这样可以计算了，但是$t$很大时这个链会很长</p>
<h4 id="截断时间步"><a href="#截断时间步" class="headerlink" title="截断时间步"></a>截断时间步</h4><p>在$\tau$步后截断求和计算，在实践中这种方式工作得很好，通常被称为截断的通过时间反向传播 (Jaeger, 2002)</p>
<p>这样做导致该模型主要侧重于短期影响，而不是长期影响，这在现实中是可取的，因为它会将估计值偏向更简单和更稳定的模型</p>
<h4 id="随机截断"><a href="#随机截断" class="headerlink" title="随机截断"></a>随机截断</h4><p>可以用一个随机变量替换$\partial h_t/\partial w_h$，该随机变量在预期中是正确的，但是会截断序列</p>
<p>通过使用序列$\xi_t$来实现，$0 \leq \pi_t \leq 1$，$P(\xi_t = 0) = 1-\pi_t$，期望$E[\xi_t] = 1$，保证整个过程在期望意义上仍是无偏估计</p>
<p>使用它来替换$\partial h_t/\partial w_h$<br>$$<br>z_t= \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial w_h} +\xi_t \frac{\partial f(x_{t},h_{t-1},w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h}.<br>$$<br>每当$\xi_t = 0$时，递归计算终止在这个时间步，这导致了不同长度序列的加权和，长序列的梯度被截断得更频繁，短序列则更多被完整传播，因此这种方法相当于自适应加权不同长度的梯度贡献</p>
<p>这个想法是由塔莱克和奥利维尔(Tallec and Ollivier, 2017)提出的</p>
<h4 id="比较策略"><a href="#比较策略" class="headerlink" title="比较策略"></a>比较策略</h4><p>比较RNN中计算梯度的策略，3行自上而下分别为：随机截断、常规截断、完整计算</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/truncated-bptt.jpg" alt="truncated-bptt" style="zoom:80%;">

<ul>
<li>随机截断：将文本划分为不同长度的片断</li>
<li>常规截断：将文本分解为相同长度的子序列，这也是在循环神经网络实验中一直在做的</li>
<li>通过时间的完全反向传播：产生了在计算上不可行的表达式</li>
</ul>
<p>虽然随机截断在理论上具有吸引力，但很可能是由于多种因素在实践中并不比常规截断更好</p>
<h2 id="门控循环单元-GRU"><a href="#门控循环单元-GRU" class="headerlink" title="门控循环单元(GRU)"></a>门控循环单元(GRU)</h2><p>普通 RNN 缺乏对“重要信息的记忆”和“无关信息的屏蔽”能力</p>
<p>三类经典问题：</p>
<ul>
<li>需要长期记忆：早期观测(如第一个词元)对后续预测至关重要，普通 RNN 难以让这种早期信息在数百步后仍然“保留”，这会导致梯度必须极大才能维持影响，从而引发梯度爆炸</li>
<li>需要遗忘无关信息：某些输入与任务无关，例如网页文本中的 HTML 标签，RNN没法“跳过”这些噪声，它会无差别地更新隐状态，让无意义的信息污染记忆，降低模型性能，需要选择性屏蔽</li>
<li>需要重置记忆：序列中存在逻辑中断或上下文变化，旧的隐藏状态可能对新片段产生负面干扰，需要自适应地清空或重置内部状态</li>
</ul>
<p>在学术界已经提出了许多方法来解决这类问题，其中最早的方法是“长短期记忆”(long-short-term memory，LSTM) (Hochreiter and Schmidhuber, 1997)，门控循环单元(gated recurrent unit，GRU)(Cho <em>et al.</em>, 2014) 是一个稍微简化的变体，通常能够提供同等的效果，并且计算(Chung <em>et al.</em>, 2014)的速度明显更快</p>
<h3 id="门控隐状态"><a href="#门控隐状态" class="headerlink" title="门控隐状态"></a>门控隐状态</h3><p>门控循环单元(GRU)与普通循环神经网络的主要区别在于：它增加了门控机制，能自动学习在什么时候更新或重置隐藏状态，从而更好地控制信息的保留与遗忘</p>
<h4 id="重置门和更新门"><a href="#重置门和更新门" class="headerlink" title="重置门和更新门"></a>重置门和更新门</h4><p>**重置门(reset gate)<strong>和</strong>更新门(update gate)**是$(0,1)$区间的向量，可以对旧状态和新状态进行加权融合(凸组合)</p>
<p>重置门控制要保留多少来自过去的记忆；更新门控制当前状态中有多少直接来自旧状态</p>
<p>下图描述了门控循环单元中的重置门和更新门的输入，输入是当前输入和前一时刻的隐藏状态，输出由带sigmoid激活函数的全连接层计算得到</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/gru-1.jpg" alt="gru-1" style="zoom:80%;">

<p>假设输入是一个小批量$\mathbf X_t \in \mathbb R^{n \times d}$，样本个数为$n$，输入特征维度$d$，上一个时间步的隐状态是$\mathbf H_{t-1} \in \mathbb R^{n \times h}$</p>
<p>那么重置门$\mathbf R_t \in \mathbb R^{n \times h}$和更新门$\mathbf Z_t \in \mathbb{R}^{n \times h}$的计算如下所示<br>$$<br>\begin{split}\begin{aligned}<br>\mathbf R_t = \sigma(\mathbf X_t \mathbf W_{xr} + \mathbf H_{t-1} \mathbf W_{hr} + \mathbf b_r)\\<br>\mathbf Z_t = \sigma(\mathbf X_t \mathbf W_{xz} + \mathbf H_{t-1} \mathbf W_{hz} + \mathbf b_z)<br>\end{aligned}\end{split}<br>$$<br>其中$\mathbf W_{xr}, \mathbf W_{xz} \in \mathbb{R}^{d \times h}$和$\mathbf W_{hr}, \mathbf W_{hz} \in \mathbb{R}^{h \times h}$是权重参数，$\mathbf b_r, \mathbf b_z \in \mathbb{R}^{1 \times h}$是偏置参数，使用sigmoid函数将输出值转换到区间$(0,1)$</p>
<h4 id="候选隐状态"><a href="#候选隐状态" class="headerlink" title="候选隐状态"></a>候选隐状态</h4><p>将重置门$\mathbf R_t$与常规隐状态更新机制集成，得到在时间步$t$的<strong>候选隐状态(candidate hidden state)</strong>$\tilde H_t \in \mathbb R^{n \times h}$<br>$$<br>\tilde H_t = \tanh(\mathbf X_t \mathbf W_{xh} + \left(\mathbf R_t \odot \mathbf H_{t-1}\right) \mathbf W_{hh} + \mathbf b_h),<br>$$<br>符号$\odot$是Hadamard积(按元素乘积)运算符，使用tanh非线性激活函数来确保候选隐状态中的值保持在区间$(-1,1)$</p>
<p>$\mathbf R_t$和$\mathbf H_{t-1}$的元素相乘可以减少以往状态的影响</p>
<ul>
<li>重置门$\mathbf R_t$中的项接近1时，恢复普通的循环神经网络</li>
<li>重置门$\mathbf R_t$中的项接近0时，候选隐状态是以$\mathbf X_t$作为输入的多层感知机的结果，因此任何预先存在的隐状态都会被重置为默认值</li>
</ul>
<p>下图说明了应用重置门之后的计算流程</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/gru-2.jpg" alt="gru-2" style="zoom:80%;">

<h4 id="隐状态-1"><a href="#隐状态-1" class="headerlink" title="隐状态"></a>隐状态</h4><p>上述的计算结果只是候选隐状态，仍然需要结合更新门$\mathbf Z_t$的效果，这一步确定新的隐状态$\mathbf H_t \in \mathbb{R}^{n \times h}$在多大程度上来自旧的状态$\mathbf H_{t-1}$和新的候选状态$\tilde H_t $，更新门仅实现凸组合，得出了门控循环单元的最终更新公式：<br>$$<br>\mathbf H_t = \mathbf Z_t \odot \mathbf H_{t-1}  + (1 - \mathbf Z_t) \odot \tilde{\mathbf H}_t.<br>$$</p>
<ul>
<li>更新门$\mathbf Z_t$接近1时，模型就倾向只保留旧状态，来自$\mathbf X_t$的信息被忽略，从而有效地跳过了依赖链条中的时间步</li>
<li>更新门$\mathbf Z_t$接近0时，新的隐状态就会接近候选隐状态</li>
</ul>
<p>这些设计可以帮助处理循环神经网络中的梯度消失问题，并更好地捕获时间步距离很长的序列的依赖关系</p>
<p>如果整个子序列的所有时间步的更新门都接近于1，无论序列的长度如何，在序列起始时间步的隐状态将很容易保留并传递到序列结束</p>
<p>下图说明了更新门起作用后的计算流</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/gru-3.jpg" alt="gru-3" style="zoom:80%;">

<p>总结：</p>
<p>门控循环单元具有以下两个显著特征</p>
<ul>
<li>重置门有助于捕获序列中的短期依赖关系</li>
<li>更新门有助于捕获序列中的长期依赖关系</li>
</ul>
<h3 id="底层实现"><a href="#底层实现" class="headerlink" title="底层实现"></a>底层实现</h3><p>读取之前使用的时间机器数据集：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="初始化模型参数-1"><a href="#初始化模型参数-1" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h4><p>从<strong>标准差为0.01</strong>的高斯分布中提取权重，并将偏置项设为0，超参数<code>num_hiddens</code>定义隐藏单元的数量， 实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="comment"># 输入和输出的维度都等于词表大小(因为这是语言模型)</span></span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="comment"># 从标准正态分布采样，并缩放为较小的随机值(标准差=0.01)</span></span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">three</span>():</span><br><span class="line">        <span class="comment"># 返回一组循环层参数：(W_x, W_h, b)</span></span><br><span class="line">        <span class="comment"># 分别对应：输入权重矩阵、隐藏状态权重矩阵、偏置项</span></span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),   <span class="comment"># W_x* — 输入到隐藏层</span></span><br><span class="line">                normal((num_hiddens, num_hiddens)),  <span class="comment"># W_h* — 隐藏层到隐藏层</span></span><br><span class="line">                torch.zeros(num_hiddens, device=device))  <span class="comment"># b* — 偏置项</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ---- 门控循环单元的参数 ----</span></span><br><span class="line">    <span class="comment"># 更新门 (update gate) 参数</span></span><br><span class="line">    W_xz, W_hz, b_z = three()</span><br><span class="line">    <span class="comment"># 重置门 (reset gate) 参数</span></span><br><span class="line">    W_xr, W_hr, b_r = three()</span><br><span class="line">    <span class="comment"># 候选隐状态 (candidate hidden state) 参数</span></span><br><span class="line">    W_xh, W_hh, b_h = three()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出层参数(将隐藏状态映射到词表维度)</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs)) </span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ---- 收集全部可训练参数 ----</span></span><br><span class="line">    params = [W_xz, W_hz, b_z,</span><br><span class="line">              W_xr, W_hr, b_r,</span><br><span class="line">              W_xh, W_hh, b_h,</span><br><span class="line">              W_hq, b_q]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 启用自动梯度跟踪</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></tbody></table></figure>

<h4 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h4><p>定义隐状态的初始化函数<code>init_gru_state</code>，与之前定义的<code>init_rnn_state</code>函数一样，此函数返回一个形状为(批量大小，隐藏单元个数)的张量，张量的值全部为零</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_gru_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></tbody></table></figure>

<p>准备定义门控循环单元模型，模型的架构与基本的循环神经网络单元是相同的，只是权重更新公式更为复杂</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gru</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = H @ W_hq + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,) <span class="comment"># 习惯写法，因为LSTM(H,C)</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="训练与预测-1"><a href="#训练与预测-1" class="headerlink" title="训练与预测"></a>训练与预测</h4><p>训练和预测的工作方式与普通RNN完全相同</p>
<p>训练结束后，分别打印输出训练集的困惑度，以及前缀“time traveler”和“traveler”的预测序列上的困惑度</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_params,</span><br><span class="line">                            init_gru_state, gru)</span><br><span class="line">train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202510311655.png" alt="202510311655" style="zoom:80%;">

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">困惑度 1.1, 57687.6 词元/秒 cpu</span><br><span class="line">time traveller for so it will be convenient to speak of himwas e</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></tbody></table></figure>

<h3 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h3><p>高级API包含了前文介绍的所有配置细节，可以直接实例化门控循环单元模型</p>
<p>这段代码的运行速度要快得多，因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">gru_layer = nn.GRU(num_inputs, num_hiddens)</span><br><span class="line">model = RNNModel(gru_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/image-20251101151204768.png" alt="image-20251101151204768" style="zoom:80%;">

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">困惑度 1.0, 85331.8 词元/秒 cpu</span><br><span class="line">time travelleryou can show black is white by argument said filby</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></tbody></table></figure>

<p>虽然困惑度降到1.0了，但是训练样本少，模型还没泛化，输出句子只是记忆，不是真正语言建模的泛化效果</p>
<h2 id="长短期记忆网络-LSTM"><a href="#长短期记忆网络-LSTM" class="headerlink" title="长短期记忆网络(LSTM)"></a>长短期记忆网络(LSTM)</h2><p>隐变量模型存在着长期信息保存和短期输入缺失的问题，解决这一问题的最早方法之一是长短期存储器**(long short-term memory，LSTM)**(Hochreiter and Schmidhuber, 1997)</p>
<p>它有许多与门控循环单元一样的属性，但长短期记忆网络的设计比门控循环单元稍微复杂一些，却比门控循环单元早诞生了近20年</p>
<h3 id="门控记忆元"><a href="#门控记忆元" class="headerlink" title="门控记忆元"></a>门控记忆元</h3><p>LSTM的设计灵感源自计算机中的逻辑门结构，它在传统循环神经网络的基础上，引入了一个用于保存信息的<strong>记忆元(memory cell)</strong>，简称为<strong>单元(cell)</strong></p>
<p>有些文献认为，记忆元是一种特殊形式的隐状态，与隐状态具有相同的形状，但专门用于长期信息的保存</p>
<p>为了有效地控制记忆元的信息流动，LSTM 设计了多种门机制</p>
<ul>
<li><strong>输入门(input gate)</strong>：决定何时将新的信息写入单元；</li>
<li><strong>遗忘门(forget gate)</strong>：决定何时清除旧的信息；</li>
<li><strong>输出门(output gate)</strong>：控制何时从单元中输出信息</li>
</ul>
<h4 id="门机制"><a href="#门机制" class="headerlink" title="门机制"></a>门机制</h4><p>就如在门控循环单元中一样，当前时间步的输入和前一个时间步的隐状态作为数据送入长短期记忆网络的门中，它们由三个具有sigmoid激活函数的全连接层处理，以计算输入门、遗忘门和输出门的值</p>
<p>这三个门的值都在(0,1)范围内</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/lstm-0.jpg" alt="lstm-0" style="zoom:80%;">

<p>假设有$h$个隐藏单元，批量大小为$n$，输入数为$d$，输入为$\mathbf X_t \in \mathbb{R}^{n \times d}$，前一时间步的隐状态为$\mathbf H_{t-1} \in \mathbb{R}^{n \times h}$，相应的输入门是$\mathbf I_t \in \mathbb{R}^{n \times h}$，遗忘门是$\mathbf F_t \in \mathbb{R}^{n \times h}$，输出门是$\mathbf O_t \in \mathbb{R}^{n \times h}$</p>
<p>它们的计算方法如下：<br>$$<br>\begin{split}\begin{aligned}<br>\mathbf I_t &amp;= \sigma(\mathbf X_t \mathbf W_{xi} + \mathbf H_{t-1} \mathbf W_{hi} + \mathbf b_i),\\<br>\mathbf F_t &amp;= \sigma(\mathbf X_t \mathbf W_{xf} + \mathbf H_{t-1} \mathbf W_{hf} + \mathbf b_f),\\<br>\mathbf O_t &amp;= \sigma(\mathbf X_t \mathbf W_{xo} + \mathbf H_{t-1} \mathbf W_{ho} + \mathbf b_o),<br>\end{aligned}\end{split}<br>$$<br>其中$\mathbf W_{xi}, \mathbf W_{xf}, \mathbf W_{xo} \in \mathbb{R}^{d \times h}$，$\mathbf W_{hi}, \mathbf W_{hf}, \mathbf W_{ho} \in \mathbb{R}^{h \times h}$</p>
<h4 id="候选记忆元"><a href="#候选记忆元" class="headerlink" title="候选记忆元"></a>候选记忆元</h4><p><strong>候选记忆元(candidate memory cell)</strong>$\tilde{\mathbf C}_t \in \mathbb{R}^{n \times h}$</p>
<p>它的计算与上面描述的三个门的计算类似，但是使用$\tanh$函数作为激活函数，函数的值范围为(-1,1)<br>$$<br>\bf\tilde{C_t} = \text{tanh}(\mathbf X_t \mathbf W_{xc} + \mathbf H_{t-1} \mathbf W_{hc} + \mathbf b_c),<br>$$<br>候选记忆元如图所示</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/lstm-1.jpg" alt="lstm-1" style="zoom:80%;">

<h4 id="记忆元"><a href="#记忆元" class="headerlink" title="记忆元"></a>记忆元</h4><p>在门控循环单元中，有一种机制来控制输入和遗忘(或跳过)，在长短期记忆网络中，也有两个门用于这样的目的<br>$$<br>\mathbf C_t = \mathbf F_t \odot \mathbf C_{t-1} + \mathbf I_t \odot \bf\tilde{C_t}<br>$$<br>如果遗忘门始终为1且输入门始终为0，则过去的记忆元$\mathbf C_{t-1}$将随时间被保存并传递到当前时间步</p>
<p>引入这种设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系</p>
<p>这样就得到了计算记忆元的流程图</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/lstm-2.jpg" alt="lstm-2" style="zoom:80%;">

<h4 id="隐状态-2"><a href="#隐状态-2" class="headerlink" title="隐状态"></a>隐状态</h4><p>需要定义如何计算隐状态$\mathbf H_t \in \mathbb{R}^{n \times h}$，这就是输出门发挥作用的地方</p>
<p>在长短期记忆网络中，隐状态是“经过输出门调制的记忆元快照”，是记忆元的$\tanh$的门控版本，确保$\mathbf H_t$的值始终在区间(-1,1)内<br>$$<br>\mathbf H_t = \mathbf O_t \odot \tanh(\mathbf C_t).<br>$$<br>只要输出门接近1，就能够有效地将所有记忆信息传递给预测部分，而对于输出门接近0，只保留记忆元内的所有信息，而不需要更新隐状态</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/lstm-3.jpg" alt="lstm-3" style="zoom:80%;">

<p>输出表示为：<br>$$<br>\mathbf Y_t = \mathbf H_t \mathbf W_{hq} +\mathbf b_q<br>$$</p>
<h4 id="总结与对比"><a href="#总结与对比" class="headerlink" title="总结与对比"></a>总结与对比</h4><table>
<thead>
<tr>
<th>组件</th>
<th>功能</th>
<th>类比</th>
</tr>
</thead>
<tbody><tr>
<td>遗忘门</td>
<td>决定丢弃多少旧记忆</td>
<td>清空一部分硬盘内容</td>
</tr>
<tr>
<td>输入门</td>
<td>决定写入多少新信息</td>
<td>把新数据写入硬盘</td>
</tr>
<tr>
<td>候选记忆</td>
<td>新的候选内容</td>
<td>新文件内容</td>
</tr>
<tr>
<td>记忆元</td>
<td>存储长期信息</td>
<td>硬盘本体</td>
</tr>
<tr>
<td>输出门</td>
<td>控制输出多少记忆</td>
<td>决定要不要从硬盘读出来</td>
</tr>
<tr>
<td>隐状态</td>
<td>当前对外可见的输出</td>
<td>屏幕上显示的内容</td>
</tr>
</tbody></table>
<p>与 GRU 的对比</p>
<table>
<thead>
<tr>
<th>特征</th>
<th>LSTM</th>
<th>GRU</th>
</tr>
</thead>
<tbody><tr>
<td>门数量</td>
<td>3 个(输入、遗忘、输出)</td>
<td>2 个(重置、更新)</td>
</tr>
<tr>
<td>记忆元</td>
<td>独立</td>
<td>与隐状态合一</td>
</tr>
<tr>
<td>结构复杂度</td>
<td>稍高，性能更强</td>
<td>简洁高效，参数更少</td>
</tr>
<tr>
<td>学习能力</td>
<td>适合复杂依赖</td>
<td>适合中短期依赖</td>
</tr>
</tbody></table>
<p>输出门的存在是为了控制信息暴露：</p>
<ul>
<li>当网络认为当前时刻的信息还不成熟或不重要时，关上输出门；</li>
<li>这样隐状态不会被下游层使用；</li>
<li>但记忆元仍然积累经验，为未来的时间步准备</li>
</ul>
<p>这种设计是 LSTM 相比 GRU 更“细腻”的地方：它能明确地区分“内部记忆”和“外部输出”</p>
<h3 id="底层实现-1"><a href="#底层实现-1" class="headerlink" title="底层实现"></a>底层实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="初始化模型参数-2"><a href="#初始化模型参数-2" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h4><p>超参数<code>num_hiddens</code>定义隐藏单元的数量，按照标准差0.01的高斯分布初始化权重，并将偏置项设为0</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化网络参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_lstm_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size = shape, device=device)*<span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">three</span>():</span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xi, W_hi, b_i = three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = three()  <span class="comment"># 候选记忆元参数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 加上梯度</span></span><br><span class="line">    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></tbody></table></figure>

<h4 id="定义模型-2"><a href="#定义模型-2" class="headerlink" title="定义模型"></a>定义模型</h4><p>在初始化函数中，长短期记忆网络需要初始化隐藏状态和记忆元，单元的值为0，形状均为(batch_size, num_hiddens)</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化隐藏状态和记忆元</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_lstm_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device),</span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></tbody></table></figure>

<p>实际模型的定义与前面讨论的一样：提供三个门和一个额外的记忆元，只有隐状态才会传递到输出层，而记忆元不直接参与输出计算</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lstm</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        inputs: 输入序列，形状为 (num_steps, batch_size, input_size)</span></span><br><span class="line"><span class="string">        state:  初始状态 (H, C)</span></span><br><span class="line"><span class="string">        params: 所有模型参数(权重与偏置)，包含14个张量</span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        outputs: 所有时间步的输出拼接结果</span></span><br><span class="line"><span class="string">        (H, C): 当前时间步的隐状态和记忆元，用于下一批次</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 解包参数，对应四个门 + 输出层</span></span><br><span class="line">    [W_xi, W_hi, b_i,      <span class="comment"># 输入门参数</span></span><br><span class="line">     W_xf, W_hf, b_f,      <span class="comment"># 遗忘门参数</span></span><br><span class="line">     W_xo, W_ho, b_o,      <span class="comment"># 输出门参数</span></span><br><span class="line">     W_xc, W_hc, b_c,      <span class="comment"># 候选记忆参数</span></span><br><span class="line">     W_hq, b_q] = params   <span class="comment"># 输出层参数(hidden → output)</span></span><br><span class="line">    </span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * torch.tanh(C)</span><br><span class="line">        Y = (H @ W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="comment"># outputs: 所有时间步拼接后的输出 (num_steps * batch_size, output_size)</span></span><br><span class="line">    <span class="comment"># (H, C): 当前时间步的最终状态</span></span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, C)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="训练和预测"><a href="#训练和预测" class="headerlink" title="训练和预测"></a>训练和预测</h4><p>引入的<code>RNNModelScratch</code>类来训练一个长短期记忆网络</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_lstm_params, init_lstm_state, lstm)</span><br><span class="line">train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511011607.webp" alt="202511011607" style="zoom:80%;">

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">困惑度 1.1, 45695.4 词元/秒 cpu</span><br><span class="line">time traveller of the samm thack is of space and thating haid th</span><br><span class="line">traveller curiessions of spaceing we care the gotither so i</span><br></pre></td></tr></tbody></table></figure>

<h3 id="简洁实现-1"><a href="#简洁实现-1" class="headerlink" title="简洁实现"></a>简洁实现</h3><p>使用高级API，可以直接实例化<code>LSTM</code>模型</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens)</span><br><span class="line">model = RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/image-20251101161336227.png" alt="image-20251101161336227" style="zoom: 80%;">

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">困惑度 1.0, 125343.0 词元/秒 cpu</span><br><span class="line">time traveller with a slight accession ofcheerfulness really thi</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></tbody></table></figure>

<p>长短期记忆网络是典型的具有重要状态控制的隐变量自回归模型，多年来已经提出了其许多变体，例如，多层、残差连接、不同类型的正则化</p>
<p>然而，由于序列的长距离依赖性，训练长短期记忆网络和其他序列模型(例如门控循环单元)的成本是相当高的，将使用更高级的替代模型，比如Transformer</p>
<h2 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h2><p>到目前为止，只讨论了具有一个单向隐藏层的循环神经网络</p>
<p>隐变量和观测值与具体的函数形式的交互方式是相当随意的，对一个单层来说，这可能具有相当的挑战性，之前在线性模型中，通过添加更多的层来解决这个问题</p>
<p>可以将多层循环神经网络堆叠在一起，通过对几个简单层的组合，产生了一个灵活的机制</p>
<p>下图描述了一个具有$L$个隐藏层的深度循环神经网络，每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/deep-rnn.jpg" alt="deep-rnn" style="zoom:80%;">

<h3 id="函数依赖关系"><a href="#函数依赖关系" class="headerlink" title="函数依赖关系"></a>函数依赖关系</h3><p>将$l^\mathrm{th}$隐藏层($l=1,\ldots,L$)的隐状态设为$\mathbf H_t^{(l)} \in \mathbb{R}^{n \times h}$，设$\mathbf H_t^{(0)} = \mathbf X_t$，第$l$个隐藏层的隐状态使用激活函数$\phi_l$<br>$$<br>\mathbf H_t^{(l)} = \phi_l(\mathbf H_t^{(l-1)} \mathbf W_{xh}^{(l)} + \mathbf H_{t-1}^{(l)} \mathbf W_{hh}^{(l)}  + \mathbf b_h^{(l)}),<br>$$<br>输出层的计算仅基于第$l$个隐藏层最终的隐状态<br>$$<br>\mathbf O_t = \mathbf H_t^{(L)} \mathbf W_{hq} + \mathbf b_q<br>$$<br>与多层感知机一样隐藏层数目$L$和隐藏单元数目$h$都是超参数</p>
<h3 id="简洁实现-2"><a href="#简洁实现-2" class="headerlink" title="简洁实现"></a>简洁实现</h3><p>实现多层循环神经网络所需的许多逻辑细节在高级API中都是现成的，以长短期记忆网络模型为例，与之前的模型代码相似，唯一的区别是指定了层的数量，而不是使用单一层这个默认值</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = load_data_time_machine(batch_size, num_steps)</span><br><span class="line">vocab_size, num_hiddens, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">device = try_gpu()</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers) <span class="comment"># 新增参数</span></span><br><span class="line">model = RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练与预测-2"><a href="#训练与预测-2" class="headerlink" title="训练与预测"></a>训练与预测</h3><p>由于使用了长短期记忆网络模型来实例化两个层，使用GPU能加快训练速度</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">2</span></span><br><span class="line">train_ch8(model, train_iter, vocab, lr*<span class="number">1.0</span>, num_epochs, device)</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/image-20251101171722334.png" alt="image-20251101171722334" style="zoom: 80%;">

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">困惑度 1.0, 60333.7 词元/秒 cpu</span><br><span class="line">time traveller for so it will be convenient to speak of himwas e</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></tbody></table></figure>

<h2 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h2><p>在序列学习中，以往假设的目标是在给定观测的情况下对下一个输出进行建模，但还可能出现填空的任务</p>
<h3 id="隐马尔可夫的动态规划"><a href="#隐马尔可夫的动态规划" class="headerlink" title="隐马尔可夫的动态规划"></a>隐马尔可夫的动态规划</h3><p>设计一个隐变量模型：在任意时间步假设存在某个隐变量$h_t$，通过概率$P(x_t \mid h_t)$控制观测到的$x_t$，任何$h_t \to h_{t+1}$转移都是由一些状态转移概率$P(h_{t+1} \mid h_{t})$给出</p>
<p>下图为<strong>隐马尔可夫模型(hidden Markov model，HMM)</strong></p>
<p><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/hmm.jpg" alt="hmm"></p>
<p>在观测状态和隐状态上具有以下联合概率分布：<br>$$<br>P(x_1, \ldots, x_T, h_1, \ldots, h_T) = \prod_{t=1}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t), \text{ where } P(h_1 \mid h_0) = P(h_1).<br>$$<br>假设观测到所有的$x_i$，除了$x_j$，目标是计算$P(x_j \mid x_{-j})$，其中$x_{-j} = (x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_{T})$，即$x_{-j}$代表除了$x_j$以外的所有观测值</p>
<p>如果任何$h_i$可以接受$k$个不同的值(有限的状态数)，这意味着需要对$k^T$个项求和，要直接计算上面的和，计算量爆炸！</p>
<p>有个巧妙的解决方案：<strong>动态规划(dynamic programming)</strong></p>
<p>结合**前向递归(forward recursion)<strong>和</strong>后向递归(backward recursion)**能够计算<br>$$<br>P(x_j \mid x_{-j}) \propto \sum_{h_j} \pi_j(h_j) \rho_j(h_j) P(x_j \mid h_j).<br>$$<br>用前向量$\pi$和后向量$\rho$就能高效地计算任意时刻观测的概率分布</p>
<h3 id="双向模型"><a href="#双向模型" class="headerlink" title="双向模型"></a>双向模型</h3><p>希望在循环神经网络中拥有一种机制，使之能够提供与隐马尔可夫模型类似的前瞻能力，需要修改循环神经网络的设计</p>
<p>只需要增加一个“从最后一个词元开始从后向前运行”的循环神经网络，而不是只有一个在前向模式下“从第一个词元开始运行”的循环神经网络</p>
<p>**双向循环神经网络(bidirectional RNNs)**添加了反向传递信息的隐藏层，以便更灵活地处理此类信息，下图描述具有单个隐藏层的双向循环神经网络的架构</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/birnn.jpg" alt="birnn" style="zoom: 80%;">

<p>这与隐马尔可夫模型中的动态规划的前向和后向递归没有太大区别</p>
<p>相当于两个RNN，正向 RNN 从头看，反向 RNN 从尾看</p>
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>双向循环神经网络是由(Schuster and Paliwal, 1997)提出的，关于各种架构的详细讨论请参阅 (Graves and Schmidhuber, 2005)</p>
<p>对于任意时间步，在双向架构中，设该时间步的前向和反向隐状态分别为$\overrightarrow{\bf{H}}_t \in \mathbb{R}^{n \times h}$和$\overleftarrow{\bf{H}}_t \in \mathbb{R}^{n \times h}$</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/Snipaste_2025-11-01_17-47-37.webp" alt="Snipaste_2025-11-01_17-47-37" style="zoom:80%;">

<p>将前向隐状态和反向隐状态连接起来，获得需要送入输出层的隐状态$\bf{H}_t \in \mathbb{R}^{n \times 2h}$</p>
<p>在具有多个隐藏层的深度双向循环神经网络中，该信息作为输入传递到下一个双向层</p>
<p>输出层计算得到的输出为<br>$$<br>\mathbf O_t = \mathbf H_t \mathbf W_{hq} + \mathbf b_q.<br>$$<br>权重矩阵$\bf{W}_{hq} \in \mathbb{R}^{2h \times q}$和偏置$\bf{b}_q \in \mathbb{R}^{1 \times q}$是输出层的模型参数</p>
<h4 id="模型的问题"><a href="#模型的问题" class="headerlink" title="模型的问题"></a>模型的问题</h4><p>双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出</p>
<p>在训练期间，能够利用过去和未来的数据来估计现在空缺的词；而在测试期间，只有过去的数据，因此预测精度将会很差</p>
<p>另一个严重问题是，双向循环神经网络的计算速度非常慢。其主要原因是网络的前向传播需要在双向层中进行前向和后向递归，并且网络的反向传播还依赖于前向传播的结果</p>
<p>双向层的使用在实践中非常少，并且仅仅应用于部分场合</p>
<h3 id="错误应用"><a href="#错误应用" class="headerlink" title="错误应用"></a>错误应用</h3><p>由于双向循环神经网络使用了过去的和未来的数据，所以不能盲目地将这一语言模型应用于任何预测任务，尽管模型产出的困惑度是合理的，该模型预测未来词元的能力却可能存在严重缺陷</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_steps, device = <span class="number">32</span>, <span class="number">35</span>, try_gpu()</span><br><span class="line">train_iter, vocab = load_data_time_machine(batch_size, num_steps)</span><br><span class="line"><span class="comment"># 通过设置“bidirective=True”来定义双向LSTM模型</span></span><br><span class="line">vocab_size, num_hiddens, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=<span class="literal">True</span>)</span><br><span class="line">model = RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></tbody></table></figure>

<p>会出现很抽象的结果</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">困惑度 1.1, 23421.1 词元/秒 cpu</span><br><span class="line">time travellerererererererererererererererererererererererererer</span><br><span class="line">travellerererererererererererererererererererererererererer</span><br></pre></td></tr></tbody></table></figure>

<h2 id="机器翻译与数据集"><a href="#机器翻译与数据集" class="headerlink" title="机器翻译与数据集"></a>机器翻译与数据集</h2><p>语言模型是自然语言处理的关键，而机器翻译是语言模型最成功的基准测试</p>
<p><strong>机器翻译(machine translation)<strong>指的是将序列从一种语言自动翻译成另一种语言，正是将输入序列转换成输出序列的</strong>序列转换模型(sequence transduction)</strong></p>
<h3 id="下载和预处理数据集"><a href="#下载和预处理数据集" class="headerlink" title="下载和预处理数据集"></a>下载和预处理数据集</h3><p>下载一个由<a target="_blank" rel="noopener" href="http://www.manythings.org/anki/">Tatoeba项目的双语句子对</a> 组成的“英－法”数据集，数据集中的每一行都是制表符分隔的文本序列对，序列对由英文文本序列和翻译后的法语文本序列组成</p>
<p>每个文本序列可以是一个句子，也可以是包含多个句子的一个段落</p>
<p>在这个问题中，英语是<strong>源语言(source language)</strong>，法语是<strong>目标语言(target language)</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DATA_HUB[<span class="string">'fra-eng'</span>] = (  <span class="comment">#@save</span></span><br><span class="line">    DATA_URL + <span class="string">'fra-eng.zip'</span>,</span><br><span class="line">    <span class="string">'94646ad1522d915e7b0f9296181140edcf86a4f5'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_data_nmt</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""载入“英语－法语”数据集"""</span></span><br><span class="line">    data_dir = download_extract(<span class="string">'fra-eng'</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(data_dir, <span class="string">'fra.txt'</span>), <span class="string">'r'</span>,</span><br><span class="line">             encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> f.read()</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">raw_text = read_data_nmt()</span><br><span class="line"><span class="built_in">print</span>(raw_text[:<span class="number">75</span>])</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">正在从http://d2l-data.s3-accelerate.amazonaws.com/fra-eng.zip下载../data\fra-eng.zip...</span><br><span class="line">Go.	Va !</span><br><span class="line">Hi.	Salut !</span><br><span class="line">Run!	Cours !</span><br><span class="line">Run!	Courez !</span><br><span class="line">Who?	Qui ?</span><br><span class="line">Wow!	Ça alors !</span><br></pre></td></tr></tbody></table></figure>

<p>下载数据集后，原始文本数据需要经过几个预处理步骤</p>
<p>需要用空格代替<strong>不间断空格(non-breaking space)</strong>，使用小写字母替换大写字母，并在单词和标点符号之间插入空格</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_nmt</span>(<span class="params">text</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""预处理“英语－法语”数据集"""</span></span><br><span class="line">    <span class="comment"># 判断是否应该在标点前加空格</span></span><br><span class="line">    <span class="comment"># 如果标点前面没空格，就给它补上空格</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">no_space</span>(<span class="params">char, prev_char</span>):</span><br><span class="line">        <span class="keyword">return</span> char <span class="keyword">in</span> <span class="built_in">set</span>(<span class="string">',.!?'</span>) <span class="keyword">and</span> prev_char != <span class="string">' '</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把不间断空格(\u202f, \xa0)替换成普通空格</span></span><br><span class="line">    <span class="comment"># 使用小写字母替换大写字母</span></span><br><span class="line">    text = text.replace(<span class="string">'\u202f'</span>, <span class="string">' '</span>).replace(<span class="string">'\xa0'</span>, <span class="string">' '</span>).lower()</span><br><span class="line">    <span class="comment"># 在单词和标点符号之间插入空格</span></span><br><span class="line">    out = [<span class="string">' '</span> + char <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> no_space(char, text[i - <span class="number">1</span>]) <span class="keyword">else</span> char</span><br><span class="line">           <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(text)]</span><br><span class="line">    <span class="comment"># out 是列表，用 join() 把它们拼回去</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(out)</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">text = preprocess_nmt(raw_text)</span><br><span class="line"><span class="built_in">print</span>(text[:<span class="number">80</span>])</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">go .	va !</span><br><span class="line">hi .	salut !</span><br><span class="line">run !	cours !</span><br><span class="line">run !	courez !</span><br><span class="line">who ?	qui ?</span><br><span class="line">wow !	ça alors !</span><br></pre></td></tr></tbody></table></figure>

<h3 id="词元化-1"><a href="#词元化-1" class="headerlink" title="词元化"></a>词元化</h3><p>与之前字符级词元化不同，在机器翻译中更喜欢单词级词元化</p>
<p>下面的<code>tokenize_nmt</code>函数对前<code>num_examples</code>个文本序列对进行词元，每个词元要么是一个词，要么是一个标点符号</p>
<p>此函数返回两个词元列表：<code>source</code>和<code>target</code></p>
<p>``source[i]<code>是源语言第$i$个文本序列的词元列表，</code>target[i]`是目标语言第$i$个文本序列的词元列表</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_nmt</span>(<span class="params">text, num_examples=<span class="literal">None</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""词元化“英语－法语”数据数据集"""</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="comment"># 把整个文本按换行符拆成行</span></span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(text.split(<span class="string">'\n'</span>)):</span><br><span class="line">        <span class="comment"># i为行号，line为内容</span></span><br><span class="line">        <span class="keyword">if</span> num_examples <span class="keyword">and</span> i &gt; num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">'\t'</span>) <span class="comment"># 把一行用制表符分成两部分</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts) == <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">' '</span>)) <span class="comment"># 按空格切成单词</span></span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">' '</span>))</span><br><span class="line">    <span class="keyword">return</span> source, target</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">source, target = tokenize_nmt(text)</span><br><span class="line">source[:<span class="number">6</span>], target[:<span class="number">6</span>]</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">([['go', '.'],</span><br><span class="line">  ['hi', '.'],</span><br><span class="line">  ['run', '!'],</span><br><span class="line">  ['run', '!'],</span><br><span class="line">  ['who', '?'],</span><br><span class="line">  ['wow', '!']],</span><br><span class="line">  </span><br><span class="line"> [['va', '!'],</span><br><span class="line">  ['salut', '!'],</span><br><span class="line">  ['cours', '!'],</span><br><span class="line">  ['courez', '!'],</span><br><span class="line">  ['qui', '?'],</span><br><span class="line">  ['ça', 'alors', '!']])</span><br></pre></td></tr></tbody></table></figure>

<p>绘制每个文本序列所包含的词元数量的直方图，在这个数据集中，大多数文本序列的词元数量少于20个</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_list_len_pair_hist</span>(<span class="params">legend, xlabel, ylabel, xlist, ylist</span>):</span><br><span class="line">    <span class="string">"""绘制列表长度对的直方图"""</span></span><br><span class="line">    <span class="comment"># 计算每个句子的长度(词元数)</span></span><br><span class="line">    x_lengths = [<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> xlist]</span><br><span class="line">    y_lengths = [<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> ylist]</span><br><span class="line">    <span class="comment"># 设置图形大小</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">6</span>, <span class="number">4</span>))</span><br><span class="line">     <span class="comment"># 绘制两个直方图</span></span><br><span class="line">    counts, bins, patches = plt.hist(</span><br><span class="line">        [x_lengths, y_lengths],</span><br><span class="line">        bins=<span class="number">20</span>,   <span class="comment"># 直方图柱子数量(可调整)</span></span><br><span class="line">        color=[<span class="string">'skyblue'</span>, <span class="string">'salmon'</span>],</span><br><span class="line">        label=legend,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 给第二组加上斜线纹理(</span></span><br><span class="line">    <span class="keyword">for</span> patch <span class="keyword">in</span> patches[<span class="number">1</span>]:</span><br><span class="line">        patch.set_hatch(<span class="string">'/'</span>)</span><br><span class="line"></span><br><span class="line">    plt.xlabel(xlabel)</span><br><span class="line">    plt.ylabel(ylabel)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_list_len_pair_hist([<span class="string">'source'</span>, <span class="string">'target'</span>], <span class="string">'# tokens per sequence'</span>, <span class="string">'count'</span>, source, target);</span><br></pre></td></tr></tbody></table></figure>

<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/202511011950.png" alt="202511011950" style="zoom:80%;">

<h3 id="词表-1"><a href="#词表-1" class="headerlink" title="词表"></a>词表</h3><p>可以分别为源语言和目标语言构建两个词表，使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小</p>
<p>为减少词表规模，把出现次数少于2次的词都当作同一个未知词<code>&lt;unk&gt;</code>，还添加了几个特殊符号：</p>
<ul>
<li><code>&lt;pad&gt;</code>：在小批量训练时，用于把句子填充到相同长度；</li>
<li><code>&lt;bos&gt;</code>：表示句子开始；</li>
<li><code>&lt;eos&gt;</code>：表示句子结束</li>
</ul>
<p>这些特殊词元在自然语言处理任务中比较常用</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">src_vocab = Vocab(source, min_freq=<span class="number">2</span>, </span><br><span class="line">                  reserved_tokens=[<span class="string">'&lt;pad&gt;'</span>, <span class="string">'&lt;bos&gt;'</span>, <span class="string">'&lt;eos&gt;'</span>])</span><br><span class="line"><span class="built_in">len</span>(src_vocab)  <span class="comment"># 10012</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h3><p>语言模型中的序列样本都有一个固定的长度，这个固定长度是由<code>num_steps</code>(时间步数或词元数量)参数指定的</p>
<p>在机器翻译中，每个样本都是由源和目标组成的文本序列对，其中的每个文本序列可能具有不同的长度</p>
<p>为了提高计算效率，仍然可以通过**截断(truncation)<strong>和</strong>填充(padding)**方式实现一次只处理一个小批量的文本序列</p>
<p>在一个小批量中，所有序列的长度都设为相同的<code>num_steps</code>，如果太短在末尾补上 <code>&lt;pad&gt;</code> 直到其长度达到<code>num_steps</code>；如果太长只保留前 <code>num_steps</code> 个词元</p>
<p>下面的<code>truncate_pad</code>函数将截断或填充文本序列</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">truncate_pad</span>(<span class="params">line, num_steps, padding_token</span>): <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""截断或填充文本序列"""</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; num_steps:</span><br><span class="line">        <span class="keyword">return</span> line[:num_steps]  <span class="comment"># 截断</span></span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (num_steps - <span class="built_in">len</span>(line))  <span class="comment"># 填充</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truncate_pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab[<span class="string">'&lt;pad&gt;'</span>])</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[47, 4, 1, 1, 1, 1, 1, 1, 1, 1]</span><br></pre></td></tr></tbody></table></figure>

<p>现在定义一个函数，可以将文本序列转换成小批量数据集用于训练</p>
<p>将特定的<code>&lt;eos&gt;</code>词元添加到所有序列的末尾，用于表示序列的结束</p>
<p>当模型通过一个词元接一个词元地生成序列进行预测时，生成的<code>&lt;eos&gt;</code>词元说明完成了序列输出工作，此外还记录了每个文本序列的长度，统计长度时排除了填充词元，在稍后将要介绍的一些模型会需要这个长度信息</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_array_nmt</span>(<span class="params">lines, vocab, num_steps</span>): <span class="comment"># #@save</span></span><br><span class="line">    <span class="string">"""将机器翻译的文本序列转换成小批量"""</span></span><br><span class="line">    lines = [vocab[l] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="comment"># 在每个句子末尾加上 &lt;eos&gt;</span></span><br><span class="line">    lines = [l + [vocab[<span class="string">'&lt;eos&gt;'</span>]] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    array = torch.tensor([truncate_pad(</span><br><span class="line">        l, num_steps, vocab[<span class="string">'&lt;pad&gt;'</span>]) <span class="keyword">for</span> l <span class="keyword">in</span> lines])</span><br><span class="line">    <span class="comment"># 用bool值转为int累加即为长度</span></span><br><span class="line">    valid_len = (array != vocab[<span class="string">'&lt;pad&gt;'</span>]).<span class="built_in">type</span>(torch.int32).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br></pre></td></tr></tbody></table></figure>

<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>定义<code>load_data_nmt</code>函数来返回数据迭代器，以及源语言和目标语言的两种词表</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_nmt</span>(<span class="params">batch_size, num_steps, num_examples=<span class="number">600</span></span>):</span><br><span class="line">    <span class="string">"""返回翻译数据集的迭代器和词表"""</span></span><br><span class="line">    <span class="comment"># 1. 读取原始英法语料并清洗文本</span></span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    <span class="comment"># 2. 将文本拆分为词元列表</span></span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    <span class="comment"># 3. 分别构建源语言和目标语言词表</span></span><br><span class="line">    src_vocab = Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">'&lt;pad&gt;'</span>, <span class="string">'&lt;bos&gt;'</span>, <span class="string">'&lt;eos&gt;'</span>])</span><br><span class="line">    tgt_vocab = Vocab(target, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">'&lt;pad&gt;'</span>, <span class="string">'&lt;bos&gt;'</span>, <span class="string">'&lt;eos&gt;'</span>])</span><br><span class="line">    <span class="comment"># 4. 把句子转为统一长度的索引张量</span></span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)</span><br><span class="line">    <span class="comment"># 5. 打包所有张量并生成可迭代数据加载器</span></span><br><span class="line">    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    data_iter = load_array(data_arrays, batch_size)</span><br><span class="line">    <span class="comment"># 6. 返回训练数据迭代器及词表</span></span><br><span class="line">    <span class="keyword">return</span> data_iter, src_vocab, tgt_vocab</span><br></pre></td></tr></tbody></table></figure>

<p>读出“英语－法语”数据集中的第一个小批量数据</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=<span class="number">2</span>, num_steps=<span class="number">8</span>)</span><br><span class="line"><span class="keyword">for</span> X, X_valid_len, Y, Y_valid_len <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'X:'</span>, X.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'X的有效长度:'</span>, X_valid_len)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'Y:'</span>, Y.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'Y的有效长度:'</span>, Y_valid_len)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X: tensor([[31,  0,  4,  3,  1,  1,  1,  1],</span><br><span class="line">        [39, 19,  4,  3,  1,  1,  1,  1]], dtype=torch.int32)</span><br><span class="line">X的有效长度: tensor([4, 4])</span><br><span class="line">Y: tensor([[77, 23,  0,  4,  3,  1,  1,  1],</span><br><span class="line">        [92, 12,  5,  3,  1,  1,  1,  1]], dtype=torch.int32)</span><br><span class="line">Y的有效长度: tensor([5, 4])</span><br></pre></td></tr></tbody></table></figure>

<h2 id="编码器-解码器架构"><a href="#编码器-解码器架构" class="headerlink" title="编码器-解码器架构"></a>编码器-解码器架构</h2><p>机器翻译是典型的序列到序列转换问题，输入和输出的长度都可能不同</p>
<p>为了解决这个问题，使用一种由两个部分组成的结构：</p>
<ul>
<li>编码器(encoder)：把输入序列转化为一个固定长度的编码状态；</li>
<li>解码器(decoder)：根据这个表示生成输出序列</li>
</ul>
<p>这被称为**编码器-解码器(encoder-decoder)**架构</p>
<img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/YJS1/encoder-decoder.jpg" alt="encoder-decoder" style="zoom:80%;">

<p>由于“编码器－解码器”架构是形成不同序列转换模型的基础，将把这个架构转换为接口方便后面的代码实现</p>
<h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>在编码器接口中，只指定长度可变的序列作为编码器的输入<code>X</code>，任何继承这个<code>Encoder</code>基类的模型将完成代码实现</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):   <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""编码器-解码器架构的基本编码器接口"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        <span class="comment"># 只是定义接口，不做实现，如果继承却没写自己的 forward就报错</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></tbody></table></figure>

<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>在下面的解码器接口中，新增一个<code>init_state</code>函数，用于将编码器的输出(<code>enc_outputs</code>)转换为编码后的状态</p>
<p>此步骤可能需要额外的输入，例如输入序列的有效长度</p>
<p>为了逐个地生成长度可变的词元序列，解码器在每个时间步都会将输入(例如：在前一时间步生成的词元)和编码后的状态映射成当前时间步的输出词元</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""编码器-解码器架构的基本解码器接口"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></tbody></table></figure>

<h3 id="合并编码器和解码器"><a href="#合并编码器和解码器" class="headerlink" title="合并编码器和解码器"></a>合并编码器和解码器</h3><p>“编码器-解码器”架构包含了一个编码器和一个解码器，并且还拥有可选的额外的参数</p>
<p>在前向传播中，编码器的输出用于生成编码状态，这个状态又被解码器作为其输入的一部分</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">"""编码器-解码器架构的基类"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.encoder = encoder</span><br><span class="line">        <span class="variable language_">self</span>.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span><br><span class="line">        enc_outputs = <span class="variable language_">self</span>.encoder(enc_X, *args)</span><br><span class="line">        dec_state = <span class="variable language_">self</span>.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decoder(dec_X, dec_state)</span><br></pre></td></tr></tbody></table></figure>

<p>“编码器－解码器”体系架构中的术语状态会启发人们使用具有状态的神经网络来实现该架构</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://yhblogs.cn">今天睡够了吗</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://yhblogs.cn/posts/7224.html">http://yhblogs.cn/posts/7224.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yhblogs.cn" target="_blank">がんばろう</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E2%8C%A8%EF%B8%8Fpython/">⌨️python</a></div><div class="post_share"><div class="social-share" data-image="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-d8633m_1280x720.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer=""></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/62963.html" title="计算机视觉"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-po97l3_1280x720.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">计算机视觉</div></div></a></div><div class="next-post pull-right"><a href="/posts/21309.html" title="卷积神经网络"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7jpjzv_1280x720.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">卷积神经网络</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/30698.html" title="BERT_Pytorch"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7jjyd9_2560x1440.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-09</div><div class="title">BERT_Pytorch</div></div></a></div><div><a href="/posts/31208.html" title="FunRec 推荐系统_精排模型"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-7j931e_1280x720_(1) (1).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-18</div><div class="title">FunRec 推荐系统_精排模型</div></div></a></div><div><a href="/posts/24333.html" title="FunRec推荐系统_召回模型"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-vpp725_1280x720_(1).webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-14</div><div class="title">FunRec推荐系统_召回模型</div></div></a></div><div><a href="/posts/58676.html" title="Leetcode100记录"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/wallhaven-9ozdyx_1280x720.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-26</div><div class="title">Leetcode100记录</div></div></a></div><div><a href="/posts/22642.html" title="windows安装ROCm"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/ROCm_logo.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-10</div><div class="title">windows安装ROCm</div></div></a></div><div><a href="/posts/3865533702.html" title="pyqt5简单实践"><img class="cover" src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071521231.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-28</div><div class="title">pyqt5简单实践</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/b_2a1aef95f351a5f7ef72eb81e6838fd6.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">今天睡够了吗</div><div class="author-info__description">相遇是最小单位的奇迹</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071549233.webp" target="_blank" title="QQ"><i class="iconfont icon-QQ"></i></a><a class="social-icon" href="https://wuyaohui06022.oss-cn-chengdu.aliyuncs.com/blogwebp/202206071549234.webp" target="_blank" title="微信"><i class="iconfont icon-weixin"></i></a><a class="social-icon" href="https://space.bilibili.com/277953459?spm_id_from=333.1007.0.0" target="_blank" title="bilibili"><i class="iconfont icon-bilibili"></i></a><a class="social-icon" href="https://github.com/YaoHui-Wu06022" target="_blank" title="Github"><i class="iconfont icon-GitHub"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">保持理智，相信明天</div><div class="twopeople"><div class="twopeople"><div class="container" style="height:200px;"><canvas class="illo" width="800" height="800" style="max-width: 200px; max-height: 200px; touch-action: none; width: 640px; height: 640px;"></canvas></div> <script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople1.js"></script> <script src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/zdog.dist.js"></script> <script id="rendered-js" src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/js/twopeople.js"></script> <style>.twopeople{margin:0;align-items:center;justify-content:center;text-align:center}canvas{display:block;margin:0 auto;cursor:move}</style></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">序列模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E5%B7%A5%E5%85%B7"><span class="toc-number">1.1.</span> <span class="toc-text">统计工具</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.1.</span> <span class="toc-text">自回归模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.2.</span> <span class="toc-text">马尔可夫模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.2.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B"><span class="toc-number">1.3.</span> <span class="toc-text">预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.4.</span> <span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">文本预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.1.</span> <span class="toc-text">读取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E5%85%83%E5%8C%96"><span class="toc-number">2.2.</span> <span class="toc-text">词元化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E8%A1%A8"><span class="toc-number">2.3.</span> <span class="toc-text">词表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E5%90%88"><span class="toc-number">2.4.</span> <span class="toc-text">整合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%9D%E8%80%83%E9%A2%98"><span class="toc-number">2.5.</span> <span class="toc-text">思考题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.</span> <span class="toc-text">语言模型和数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">学习语言模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E4%B8%8En%E5%85%83%E8%AF%AD%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">马尔可夫模型与n元语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%BB%9F%E8%AE%A1"><span class="toc-number">3.3.</span> <span class="toc-text">自然语言统计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E9%95%BF%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE"><span class="toc-number">3.4.</span> <span class="toc-text">读取长序列数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7"><span class="toc-number">3.4.1.</span> <span class="toc-text">随机采样</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A1%BA%E5%BA%8F%E5%88%86%E5%8C%BA"><span class="toc-number">3.4.2.</span> <span class="toc-text">顺序分区</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">4.</span> <span class="toc-text">循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%90%E7%8A%B6%E6%80%81"><span class="toc-number">4.1.</span> <span class="toc-text">隐状态</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%97%E7%AC%A6%E7%BA%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.2.</span> <span class="toc-text">字符级语言模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%B0%E6%83%91%E5%BA%A6-Perplexity"><span class="toc-number">4.3.</span> <span class="toc-text">困惑度(Perplexity)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%9D%E8%80%83%E9%A2%98-1"><span class="toc-number">4.4.</span> <span class="toc-text">思考题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.</span> <span class="toc-text">循环神经网络底层实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81"><span class="toc-number">5.1.</span> <span class="toc-text">独热编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">5.2.</span> <span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">5.3.</span> <span class="toc-text">网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B-1"><span class="toc-number">5.4.</span> <span class="toc-text">预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA"><span class="toc-number">5.5.</span> <span class="toc-text">梯度裁剪</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="toc-number">5.6.</span> <span class="toc-text">训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.</span> <span class="toc-text">循环神经网络的简洁实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.1.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%A2%84%E6%B5%8B"><span class="toc-number">6.2.</span> <span class="toc-text">训练与预测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E6%97%B6%E9%97%B4%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">7.</span> <span class="toc-text">通过时间反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E5%88%86%E6%9E%90"><span class="toc-number">7.1.</span> <span class="toc-text">梯度分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%88%AA%E6%96%AD%E6%97%B6%E9%97%B4%E6%AD%A5"><span class="toc-number">7.1.1.</span> <span class="toc-text">截断时间步</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%88%AA%E6%96%AD"><span class="toc-number">7.1.2.</span> <span class="toc-text">随机截断</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AF%94%E8%BE%83%E7%AD%96%E7%95%A5"><span class="toc-number">7.1.3.</span> <span class="toc-text">比较策略</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83-GRU"><span class="toc-number">8.</span> <span class="toc-text">门控循环单元(GRU)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%A8%E6%8E%A7%E9%9A%90%E7%8A%B6%E6%80%81"><span class="toc-number">8.1.</span> <span class="toc-text">门控隐状态</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%8D%E7%BD%AE%E9%97%A8%E5%92%8C%E6%9B%B4%E6%96%B0%E9%97%A8"><span class="toc-number">8.1.1.</span> <span class="toc-text">重置门和更新门</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%80%99%E9%80%89%E9%9A%90%E7%8A%B6%E6%80%81"><span class="toc-number">8.1.2.</span> <span class="toc-text">候选隐状态</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%90%E7%8A%B6%E6%80%81-1"><span class="toc-number">8.1.3.</span> <span class="toc-text">隐状态</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0"><span class="toc-number">8.2.</span> <span class="toc-text">底层实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0-1"><span class="toc-number">8.2.1.</span> <span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">8.2.2.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%A2%84%E6%B5%8B-1"><span class="toc-number">8.2.3.</span> <span class="toc-text">训练与预测</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">8.3.</span> <span class="toc-text">简洁实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C-LSTM"><span class="toc-number">9.</span> <span class="toc-text">长短期记忆网络(LSTM)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%A8%E6%8E%A7%E8%AE%B0%E5%BF%86%E5%85%83"><span class="toc-number">9.1.</span> <span class="toc-text">门控记忆元</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%A8%E6%9C%BA%E5%88%B6"><span class="toc-number">9.1.1.</span> <span class="toc-text">门机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%80%99%E9%80%89%E8%AE%B0%E5%BF%86%E5%85%83"><span class="toc-number">9.1.2.</span> <span class="toc-text">候选记忆元</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%B0%E5%BF%86%E5%85%83"><span class="toc-number">9.1.3.</span> <span class="toc-text">记忆元</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%90%E7%8A%B6%E6%80%81-2"><span class="toc-number">9.1.4.</span> <span class="toc-text">隐状态</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E4%B8%8E%E5%AF%B9%E6%AF%94"><span class="toc-number">9.1.5.</span> <span class="toc-text">总结与对比</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">9.2.</span> <span class="toc-text">底层实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0-2"><span class="toc-number">9.2.1.</span> <span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B-2"><span class="toc-number">9.2.2.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E9%A2%84%E6%B5%8B"><span class="toc-number">9.2.3.</span> <span class="toc-text">训练和预测</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">9.3.</span> <span class="toc-text">简洁实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">10.</span> <span class="toc-text">深度循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="toc-number">10.1.</span> <span class="toc-text">函数依赖关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">10.2.</span> <span class="toc-text">简洁实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%A2%84%E6%B5%8B-2"><span class="toc-number">10.3.</span> <span class="toc-text">训练与预测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">11.</span> <span class="toc-text">双向循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E7%9A%84%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="toc-number">11.1.</span> <span class="toc-text">隐马尔可夫的动态规划</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8C%E5%90%91%E6%A8%A1%E5%9E%8B"><span class="toc-number">11.2.</span> <span class="toc-text">双向模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">11.2.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">11.2.2.</span> <span class="toc-text">模型的问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%94%99%E8%AF%AF%E5%BA%94%E7%94%A8"><span class="toc-number">11.3.</span> <span class="toc-text">错误应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">12.</span> <span class="toc-text">机器翻译与数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">12.1.</span> <span class="toc-text">下载和预处理数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E5%85%83%E5%8C%96-1"><span class="toc-number">12.2.</span> <span class="toc-text">词元化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E8%A1%A8-1"><span class="toc-number">12.3.</span> <span class="toc-text">词表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">12.4.</span> <span class="toc-text">加载数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">12.5.</span> <span class="toc-text">训练模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84"><span class="toc-number">13.</span> <span class="toc-text">编码器-解码器架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">13.1.</span> <span class="toc-text">编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">13.2.</span> <span class="toc-text">解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E7%BC%96%E7%A0%81%E5%99%A8%E5%92%8C%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">13.3.</span> <span class="toc-text">合并编码器和解码器</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">©2022 - 2026 By 今天睡够了吗</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">You must always have faith in who you are！</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"></div><div id="rightside-config-show"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="false"></script><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>